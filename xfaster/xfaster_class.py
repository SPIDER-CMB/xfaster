from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
import datetime
import os
import glob
import warnings
import copy
import logging
from collections import OrderedDict
from . import spec_tools as st
from . import parse_tools as pt
from configparser import RawConfigParser

__all__ = ["XFasterConfig", "XFaster", "XFasterWarning"]


class XFasterWarning(Warning):
    """
    Warning generated by the XFaster algorithm.
    """

    pass


class XFasterConfig(RawConfigParser):
    """
    ConfigParser subclass for storing command line options and config.
    """

    def __init__(self, defaults=None, default_sec="Uncategorized"):
        """
        Class that tracks command-line options for storage to disk.

        Arguments
        ---------
        defaults : dict
            Dictionary of overall configuration values.
            Eg: locals() at beginning of function, or vars(args) from argparse
        default_sec : string, optional
            The name of the default section in the configuration file.
        """
        super(XFasterConfig, self).__init__(dict_type=OrderedDict)
        self.default_sec = default_sec
        self.add_section(default_sec)
        if defaults is not None:
            self.update(defaults)

    def update(self, options, section=None):
        """
        Update configuration options with a dictionary. Behaves like
        dict.update() for specified section but also clears options of the same
        name from the default section.

        Arguments
        ---------
        options : dict
            The options to update
        section : string, optional
            Name of section to update. Default: self.default_sec
        """
        if section is None:
            section = self.default_sec
        if not self.has_section(section):
            self.add_section(section)
        # change kwargs to be like any other options
        kw = options.pop("kwargs", None)
        if isinstance(kw, dict):
            options.update(kw)
        for k, v in sorted(options.items()):
            self.remove_option(self.default_sec, k)
            self.set(section, k, str(v))

    def sort(self):
        """
        Sort the items in each section of the configuration alphabetically.
        """
        for section, section_items in self.items():
            if sorted(section_items) == list(section_items):
                continue

            section_dict = {k: v for k, v in section_items.items()}

            for k in list(section_items):
                self.remove_option(section, k)

            for k, v in sorted(section_dict.items()):
                self.set(section, k, v)

    def write(self, fp=None, sort=True):
        """
        Write an .ini-format representation of the configuration state.
        Keys are stored alphabetically if `sort` is True.

        Arguments
        ---------
        fp : file object
            If None, write to `sys.stdout`.
        sort : bool
            If True, sort items in each section alphabetically.
        """
        if fp is None:
            import sys

            fp = sys.stdout

        if sort:
            self.sort()
        super(XFasterConfig, self).write(fp)


class XFaster(object):

    checkpoints = [
        "files",
        "masks",
        "kernels",
        "sims_transfer",
        "shape_transfer",
        "transfer",
        "sims",
        "beams",
        "data",
        "template_noise",
        "shape",
        "bandpowers",
        "beam_errors",
        "likelihood",
    ]

    # if starting from KEY, force rerun all steps in VALUES
    checkpoint_tree = {
        "files": ["masks"],
        "masks": ["kernels", "sims_transfer", "sims", "data", "template_noise"],
        "kernels": ["transfer"],
        "sims_transfer": ["transfer"],
        "shape_transfer": ["transfer"],
        "transfer": ["bandpowers"],
        "sims": ["bandpowers"],
        "beams": ["transfer"],
        "data": ["bandpowers"],
        "template_noise": ["bandpowers"],
        "shape": ["bandpowers"],
        "bandpowers": ["likelihood"],
        "beam_errors": ["likelihood"],
    }

    data_version = 2

    def __init__(
        self,
        config,
        output_root=None,
        output_tag=None,
        verbose="notice",
        debug=False,
        checkpoint=None,
        add_log=False,
    ):
        """
        Initialize an XFaster instance for computing binned power spectra
        using a set of data maps along with signal and noise simulations.

        Arguments
        ---------
        config : string
            Configuration file. If path doesn't exist, assumed
            to be in xfaster/config/<config>
        output_root : string
            Path to data output directory
        output_tag : string
            Tag to use for output data.  Results are typically stored in
            the form ``<output_root>/<output_tag>/<name>_<output_tag>.npz``
        verbose : string
            Verbosity level to use for log messages.  Can be one of
            ["critical", "error", "warning", "notice", "info", "debug", "all"].
        debug : bool
            Store extra data in output files for debugging.
        checkpoint : string
            If output data from this step forward exist on disk, they are
            are re-computed rather than loading from file.
            Options are {checkpoints}.
        add_log : bool
            If True, write log output to a file instead of to STDOUT.
            The log will be in ``<output_root>/run_<output_tag>.log``.
        """
        # verbosity
        self.init_log(
            level=verbose,
            filename=self.get_filename("run", ext=".log") if add_log else None,
        )

        self.debug = debug

        # map tag configuration
        self.load_map_config(config)

        # checkpointing
        if checkpoint is not None:
            if checkpoint not in self.checkpoints:
                raise ValueError(
                    "Invalid checkpoint {}, must be one of {}".format(
                        checkpoint, self.checkpoints
                    )
                )
        self.checkpoint = checkpoint
        self.force_rerun = {cp: False for cp in self.checkpoints}

        if output_root is None:
            output_root = os.getcwd()
            self.warn("No output root supplied, using {}".format(output_root))

        self.output_root = output_root
        self.output_tag = output_tag
        if self.output_tag is not None:
            self.output_root = os.path.join(self.output_root, self.output_tag)
        if not os.path.exists(self.output_root):
            os.makedirs(self.output_root)

    __init__.__doc__ = __init__.__doc__.format(checkpoints=checkpoints)

    def load_map_config(self, filename):
        """
        Load the input map configuration file.

        The configuration file should be a file that is readable using
        ``ConfigParser``.  It must contain at least a single section called
        "frequencies", with keys for each map tag that may be used by the
        algorithm.  If using the harmonic-domain foreground fitting portions of
        the algorith, the value of each key should be the observing frequency in
        GHz that is appropriate for each tag.  Otherwise, these frequencies can
        be any floating point value.

        Other optional sections include:

        **beam**:
        Beam window specifications for each of the tags in "frequencies".  The
        "beam_product" key should be a path to a .npz file containing a
        dictionary of beam windows keyed by tag.  The "beam_product_error" key
        should be a path to a similar dictionary containing fraction beam error
        envelopes, also keyed by tag.  See ``get_beams`` or ``get_beam_errors``
        for more details.

        **fwhm**:
        If using a Gaussian beam model, this section should contain a list of
        FWHM in arcmin for each such tag in "frequencies".  Keys missing here
        should be present in the beam product file.

        **fwhm_err**:
        If using a Gaussian beam model, this section should contain a list of
        fractional errors on the FWHM for each such tag in "frequencies".  Keys
        missing here should be present in the beam error product file.

        **transfer**:
        If present, this section should contain each of the keys in
        "frequencies", with the value set to "true" if a transfer function
        should be computed for the tag, and "false" otherwise (in which case the
        transfer function will be set to unity for all bins).  This option is
        useful for including, e.g. optimally weighted Planck maps with no
        transfer function in a joint analysis.  If not supplied, it is assumed
        that a transfer function should be computed for every tag in
        "frequencies".

        Arguments
        ----------
        filename : str
            Path to config file
        """

        # Load map configuration file
        assert os.path.exists(filename), "Missing config file {}".format(filename)
        self.config_root = os.path.dirname(os.path.abspath(filename))
        cfg = XFasterConfig()
        cfg.read(filename)

        # dictionary of map frequencies keyed by map tag
        assert "frequencies" in cfg, "Missing [frequencies] section"
        self.dict_freqs = {
            k: cfg.getfloat("frequencies", k) for k in cfg["frequencies"]
        }
        tagset = set(self.dict_freqs)
        assert len(self.dict_freqs) > 0, "At least one map tag is required"

        # beam fwhm for each tag, if not supplied in beam_product
        # converted from arcmin to radians
        if "fwhm" in cfg:
            tags = [k for k in self.dict_freqs if cfg.has_option("fwhm", k)]
            self.fwhm = {k: np.radians(cfg.getfloat("fwhm", k) / 60.0) for k in tags}
        else:
            self.fwhm = {}

        # beam fwhm error for each tag, if not supplied in beam_error_product
        if "fwhm_err" in cfg:
            tags = [k for k in self.dict_freqs if cfg.has_option("fwhm_err", k)]
            self.fwhm_err = {k: cfg.getfloat("fwhm_err", k) for k in tags}
        else:
            self.fwhm_err = {}

        # make sure beam product files exist
        if "beam" in cfg:
            v = cfg["beam"].get("beam_product", None)
            if str(v).lower() != "none":
                if not os.path.exists(v):
                    v = os.path.join(self.config_root, v)
                assert os.path.exists(v), "Missing beam product file {}".format(v)
                self.beam_product = pt.load_compat(v)
                # Only use the fields in the beam product we need
                for btag in set(self.beam_product) - tagset:
                    self.beam_product.pop(btag)
            else:
                self.beam_product = {}

            v = cfg["beam"].get("beam_error_product", None)
            if str(v).lower() != "none":
                if not os.path.exists(v):
                    v = os.path.join(self.config_root, v)
                assert os.path.exists(v), "Missing beam error product file {}".format(v)
                self.beam_error_product = pt.load_compat(v)
                # Only use the fields in the beam product we need
                for btag in set(self.beam_error_product) - tagset:
                    self.beam_error_product.pop(btag)
            else:
                self.beam_error_product = {}
        else:
            self.beam_product = {}
            self.beam_error_product = {}

        # make sure all tags are present in either beam products or fwhm tables
        fwhm_set = set(self.fwhm) | set(self.beam_product)
        assert fwhm_set == tagset, "Missing tags in [fwhm] or beam product"

        if len(self.fwhm_err) or len(self.beam_error_product):
            fwhm_set = set(self.fwhm_err) | set(self.beam_error_product)
            assert (
                fwhm_set == tagset
            ), "Missing tags in [fwhm_err] or beam error product"

        # fit for the transfer function for each tag?
        if "transfer" in cfg:
            tags = [k for k in self.dict_freqs if cfg.has_option("transfer", k)]
            self.fit_transfer = {k: cfg.getboolean("transfer", k) for k in tags}
            assert tagset == set(self.fit_transfer), "Missing tags in [transfer]"
        else:
            # assume true for all tags otherwise
            self.fit_transfer = {k: True for k in self.dict_freqs}

    def init_log(self, level="notice", filename=None):
        """
        Initialize the logger from the input keyword arguments.

        Arguments
        ---------
        level : string, optional, default: "notice"
            Verbosity level.
            Options are "critical", "error", "warning", "notice", "info", "debug", "all".
        filename : string, optional
            Logging output filename.  Default: None (print to sys.stdout)
        """

        # add NOTICE logging level
        logging.NOTICE = 25
        logging.addLevelName(logging.NOTICE, "NOTICE")

        def logger_notice(self, msg, *args, **kwargs):
            """
            Log a message with severity "NOTICE".

            Arguments
            ---------
            msg : str
                Log message
            """
            if self.isEnabledFor(logging.NOTICE):
                self._log(logging.NOTICE, msg, args, **kwargs)

        logging.Logger.notice = logger_notice

        def root_notice(msg, *args, **kwargs):
            """
            Log a message with severity "NOTICE" on the root logger.

            Arguments
            ---------
            msg : str
                Log message
            """
            if len(logging.root.handlers) == 0:
                logging.basicConfig()
            logging.root.notice(msg, *args, **kwargs)

        logging.notice = root_notice

        # create handler
        if filename is None:
            from .batch_tools import get_job_logfile

            filename = get_job_logfile()
        if filename is None:
            handler = logging.StreamHandler()
        else:
            handler = logging.FileHandler(filename)

        # create formatter
        fmt = logging.Formatter(
            fmt="[ %(asctime)s ] %(levelname)s: %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S%Z",
        )
        handler.setFormatter(fmt)

        # configure logger
        self.logger = logging.getLogger("xfaster")
        self.logger.addHandler(handler)

        # set logging level
        if level is None:
            level = "notice"
        elif level.lower() == "all":
            level = "notset"
        self.logger.setLevel(getattr(logging, level.upper()))

    def log(self, message, level=None):
        """
        Log a message with the given logging level.

        Arguments
        ---------
        message : str
            Log message
        level : string, default : None
            Logging level.  Must be one of "critical", "error", "warning",
            "notice", "info", "debug", "all".  If not supplied, "all" is assumed.
        """

        if level is None or level.lower() == "all":
            level = "notset"
        level = getattr(logging, level.upper())

        self.logger.log(level, message)

    def warn(self, message):
        """
        Log a warning message.

        Arguments
        ---------
        message : str
            Warning log message
        """

        warnings.warn(message, XFasterWarning, stacklevel=2)

    def __del__(self):
        """
        Make sure logger is shutdown properly when the object is destroyed.
        """

        # cleanup logging handlers
        for handler in self.logger.handlers[::-1]:
            try:
                handler.acquire()
                handler.flush()
                handler.close()
            except (OSError, ValueError):
                pass
            finally:
                handler.release()

    def _get_sim_files(
        self,
        name,
        root=None,
        subset="*",
        suffix="",
        data_suffix="",
        match_count=True,
        fs=None,
    ):
        """
        Convenience function for finding all matching sims per map.  Used
        internally in ``get_files`` for selecting appropriate subsets of
        simulation files.

        Arguments
        ---------
        name : str
            Type of simulation files to collect (signal, noise, foreground,
            template)
        root : str
            Root of the simulation file tree, relative to data_root.  If not
            supplied, the returned set of variables are set to null values.
        subset : str
            Data subset to search for.  See ``get_files`` for details.
        suffix : str
            Suffix to apply to the output variables, e.g. "_sim" or "_sim2".
        data_suffix : str
            Suffix to apply to corresponding data variables to search for
            matching files, e.g. "2".
        match_count : bool
            If True, make sure that the number of files found matches those
            counted previously for the same dataset.  If False, stores the
            minimum number of files found.
        fs : dict
            Dictionary of file options to search for the appropriate data
            variables to check for consistency.  If not supplied, checks
            for the corresponding attributes of the calling object.

        Returns
        -------
        opts : dict
            Dictionary of sim file options, with the following keys:
            <name>_root<suffix>, <name>_files<suffix>, num_<name><suffix>.
        """
        if root is None:
            return {
                "{}_root{}".format(name, suffix): None,
                "{}_files{}".format(name, suffix): None,
                "num_{}{}".format(name, suffix): 0,
            }

        if fs is None:
            data_root = getattr(self, "data_root{}".format(data_suffix))
            map_root = getattr(self, "map_root{}".format(data_suffix))
            map_files = getattr(self, "map_files{}".format(data_suffix))
        else:
            data_root = fs["data_root{}".format(data_suffix)]
            map_root = fs["map_root{}".format(data_suffix)]
            map_files = fs["map_files{}".format(data_suffix)]

        root = os.path.join(data_root, root)
        num_files = None
        all_files = []
        for f in map_files:
            files = sorted(
                glob.glob(
                    f.replace(map_root, root).replace(
                        ".fits", "_{}.fits".format(subset)
                    )
                )
            )
            nfiles = len(files)
            if not nfiles:
                raise OSError("Missing {} sims for {}".format(name, f))
            if num_files is None:
                num_files = nfiles
            elif num_files != nfiles:
                if match_count:
                    raise OSError(
                        "Found {} {} sims for map {}, expected {}".format(
                            nfiles, name, f, num_files
                        )
                    )
                elif nfiles < num_files:
                    num_files = nfiles

            all_files.append(files)

        all_files = np.asarray([f[:num_files] for f in all_files])
        self.log("Found {} {} sims in {}".format(num_files, name, root), "info")
        self.log(
            "First {} sim files: {}".format(name, all_files[:, 0].tolist()), "debug"
        )

        return {
            "{}_root{}".format(name, suffix): root,
            "{}_files{}".format(name, suffix): all_files,
            "num_{}{}".format(name, suffix): num_files,
        }

    def _get_files(
        self,
        data_root,
        data_subset="full/*0",
        data_type="raw",
        mask_type="hitsmask_tailored",
        signal_type="r0p03",
        signal_transfer_type=None,
        signal_subset="*",
        noise_type="stationary",
        noise_subset="*",
        signal_type_sim=None,
        noise_type_sim=None,
        foreground_type_sim=None,
        suffix="",
    ):
        """
        Find all files for the given data root.  Internal function, see
        ``get_files`` for a complete docstring.
        """

        # regularize data root
        if not os.path.exists(data_root):
            raise OSError("Missing data root {}".format(data_root))

        # find all map files
        map_root = os.path.join(data_root, "data_{}".format(data_type))
        map_files = []
        data_subset = data_subset.split(",")
        for f in np.atleast_1d(data_subset):
            files = glob.glob(os.path.join(map_root, "{}.fits".format(f)))
            if not len(files):
                raise OSError("Missing files in data subset {}".format(f))
            map_files.extend(files)
        data_subset = ",".join(data_subset)
        map_files = sorted(map_files)
        map_files = [f for f in map_files if os.path.basename(f).startswith("map_")]
        map_tags = [
            os.path.splitext(os.path.basename(f))[0].split("_", 1)[1] for f in map_files
        ]
        map_freqs = []
        for t in map_tags:
            # if map tag is not a plain frequency, extract plain frequency
            map_freqs.append(self.dict_freqs[t])
        self.log("Found {} map files in {}".format(len(map_files), map_root), "info")
        self.log("Map files: {}".format(map_files), "debug")
        self.log("Map freqs: {}".format(map_freqs), "debug")

        # Also need a list of unique map tags for populating dictionaries
        # in data structures
        map_tags_orig = list(map_tags)  # copy
        map_tags = pt.unique_tags(map_tags)

        # make a list of names corresponding to the order of the cross spectra
        map_pairs = pt.tag_pairs(map_tags)
        map_pairs_orig = pt.tag_pairs(map_tags, index=map_tags_orig)

        # make a dictionary of map freqs for each unique map tag
        map_freqs_dict = {}
        for im0, m0 in enumerate(map_tags):
            map_freqs_dict[m0] = map_freqs[im0]
        map_freqs = map_freqs_dict

        # find all corresponding masks
        if mask_type is None:
            raise ValueError("Argument mask_type required")
        # If mask is a fits file, use the same mask for all maps
        if os.path.splitext(mask_type)[1] == ".fits":
            if os.path.exists(mask_type):
                # it's an absolute path
                mask_files = np.tile(mask_type, len(map_tags))
                mask_root = os.path.dirname(mask_type)
            else:
                # it's relative to base directory structure
                mask_files = np.tile(os.path.join(data_root, mask_type), len(map_tags))
                mask_root = os.path.dirname(os.path.join(data_root, mask_type))
        else:
            mask_root = os.path.join(data_root, "masks_{}".format(mask_type))
            # XXX Do this smarter
            mask_files = [
                os.path.join(mask_root, "mask_map_{}.fits".format(tag))
                for tag in map_tags_orig
            ]
        for f in mask_files:
            if not os.path.exists(f):
                raise OSError("Missing mask file {}".format(f))
        self.log("Found {} masks in {}".format(len(mask_files), mask_root), "info")
        self.log("Mask files: {}".format(mask_files), "debug")

        fs = {
            "data_root": data_root,
            "data_subset": data_subset,
            "map_root": map_root,
            "map_files": map_files,
            "num_maps": len(map_files),
            "map_tags": map_tags,
            "map_pairs": map_pairs,
            "map_tags_orig": map_tags_orig,
            "map_pairs_orig": map_pairs_orig,
            "map_freqs": map_freqs,
            "mask_root": mask_root,
            "mask_files": mask_files,
        }

        # convenience function
        def find_sim_files(name, root=None, subset="*"):
            fs.update(self._get_sim_files(name, root, subset, fs=fs))

        # signal and noise ensembles for building covariance model
        find_sim_files("signal", "signal_{}".format(signal_type), signal_subset)
        if noise_type is not None:
            find_sim_files("noise", "noise_{}".format(noise_type), noise_subset)
        else:
            find_sim_files("noise", None)

        # signal ensembles for transfer function covariance model
        if signal_transfer_type is None:
            signal_transfer_type = signal_type
        find_sim_files(
            "signal_transfer", "signal_{}".format(signal_transfer_type), signal_subset
        )

        # apply suffix
        out = dict()
        for k, v in fs.items():
            out[k + suffix] = v
        return out

    def get_files(
        self,
        data_root,
        data_subset="full/*0",
        signal_subset="*",
        noise_subset="*",
        data_type="raw",
        noise_type="gaussian",
        noise_type_sim=None,
        mask_type="rectangle",
        signal_type="synfast",
        signal_type_sim=None,
        signal_transfer_type=None,
        data_root2=None,
        data_subset2=None,
        foreground_type_sim=None,
        template_type=None,
        template_noise_type=None,
        template_type_sim=None,
        subtract_reference_signal=False,
        subtract_template_noise=False,
    ):
        """
        Find all files for the given data root.  The data structure is::

            <data_root>
                -> data_<data_type>
                    -> full
                        -> map_<tag>.fits
                        ...
                    -> 1of4 (same filenames as full)
                    -> 2of4 ('')
                    -> 3of4 ('')
                    -> 4of4 ('')
                -> signal_<signal_type>
                   -> spec_signal_<signal_type>.dat
                   -> full
                      -> map_<tag>_####.fits
                      ...
                   -> 1of4 (same filenames as full)
                   -> 2of4 (same filenames as full)
                   -> 3of4 (same filenames as full)
                   -> 4of4 (same filenames as full)
                -> noise_<noise_type> (same filenames as signal_<signal_type>)
                -> masks_<mask_type>
                    -> mask_map_<tag>.fits
                    ...
                -> foreground_<foreground_type_sim>
                    (same filenames as signal_<signal_type>)
                -> templates_<template_type>
                   -> template1
                      (same filenames as data_<data_type>)
                   -> template2
                      (same filenames as data_<data_type>)
                -> reobs_reference (if subtract_reference_signal=True)
                   -> reference1
                      (same filenames as data_<data_type>)
                   -> reference2
                      (same filenames as data_<data_type>)
            <data_root2> (If provided, same structure as data_root)
                ...

        Arguments
        ---------
        data_root : string
            Top level path containing subdirectories for data, signal sims,
            noise sims, and masks.
        data_subset : string
            Subset of maps to use for spectrum estimation.  This should be
            a string that is parseable using ``glob`` on the path
            ``data_<data_type>/<data_subset>.fits``.  For example,
            ``'full/*0'`` will expand to read in the 150 GHz and 90GHz maps.
            Maps are then sorted in alphabetical order, and identified
            by their file tag, where each filename is ``map_<tag>.fits``.
        signal_subset : string
            Subset of map tags to use for spectrum estimation for signal
            sims.  This should be a string that is parseable using ``glob``
            that is added onto the data_subset path to indicate which sims
            to use. For example, for all, use ``'*'``. For the first 300 sims,
            use ``'0[0-2]*'``.
        noise_subset : string
            Subset of map tags to use for spectrum estimation for noise
            sims.  This should be a string that is parseable using ``glob``
            that is added onto the data_subset path to indicate which sims
            to use. For example, for all, use ``'*'``. For the first 300 sims,
            use ``'0[0-2]*'``.
        data_type : string
            The type of data to use, default: "raw"
        noise_type: string
            The variant of noise simulation to use, e.g. 'gaussian',
            'stationary', etc.  The directory should contain the same number
            of simulations for each map tag.
        noise_type_sim : string
            The variant of noise sims to use for sim_index simulated data map.
            This enables having a different noise sim ensemble to use for
            sim_index run than the ensemble from which the noise is computed.
        mask_type : string
            The variant of mask to use, e.g. 'rectangle', etc.
            We assume a mask per file tag in the mask_<mask_type> folder,
            corresponding to the files in data.
        signal_type : string
            The variant of signal simulation to use, typically identified
            by the input spectrum model used to generate it, e.g 'synfast'.
        signal_type_sim : string
            The variant of signal sims to use for sim_index simulated data map.
            This enables having a different signal sim ensemble to use for
            sim_index run than the ensemble from which the signal is computed.
            If this is set to 'r', then two signal types are searched:
            'signal_r0' for a scalar component, and 'signal_r1tens' for a tensor
            component.  The two maps are linearly combined with a scalar r value
            to construct a signal map for a simulated dataset.
        signal_transfer_type : string
            The variant of signal simulation to use for transfer function
            calculation, typically identified by the input spectrum model used
            to generate it, e.g 'synfast'. This directory may also contain a
            copy of the input spectrum, to make sure that the correct spectrum
            is used to compute the transfer function.
        data_root2, data_subset2 : string
            The root and subset for a second set of data.  If either of these is
            keywords is supplied, then the two data sets are treated as two
            halves of a null test.  In this case, XFaster computes the sum and
            difference spectra for each map tag in order to estimate a null
            spectrum.
        foreground_type_sim : string
            Tag for directory (foreground_<foreground_type_sim>) where
            foreground sims are that should be added to the signal and noise
            sims when running in sim_index mode.
        template_type : string
            Tag for directory (templates_<template_type>) containing templates
            (e.g. a foreground model) to be scaled by a scalar value per map tag
            and subtracted from the data. The directory contains one template
            per map tag.
        template_noise_type : string
            Tag for directory containing template noise sims to be averaged and
            scaled similarly to the templates themselves.  These averaged sims
            are used to debias template cross spectra due to correlations in the
            way the noise ensembles are constructed.
        template_type_sim : string
            Tag for directory containing foreground templates, to be scaled by a
            scalar value per map tag and added to the simulated data.  The
            directory contains one template per map tag.
        subtract_reference_signal : bool
            If True, subtract a reobserved reference signal from each data map.
            The reference signal maps should be two datasets with uncorrelated
            noise, such as Planck half-mission maps.  This option is used for
            removing expected signal residuals from null tests.
        subtract_template_noise : bool
            If True, subtract average of cross spectra of an ensemble of noise
            realizations corresponding to each template map, to debias
            template-cleaned spectra.  Typically, this would be a noise model
            based on the Planck FFP10 ensemble for each half-mission foreground
            template.

        Returns
        -------
        file_settings : dict
            A dictionary of file settings used throughout the run.
            These are stored in full as ``<output_root>/files_<output_tag>.npz``,
            and a subset are added to the run configuration file
            ``<output_root>/config_<output_tag>.txt``.
        """

        if signal_transfer_type is None:
            signal_transfer_type = signal_type

        # make sure sims get rerun correctly
        if signal_transfer_type == signal_type:
            # if signal types match, then sims are run before computing the
            # transfer function, so need to set the correct checkpoint to rerun
            if self.checkpoint == "sims":
                self.checkpoint = "sims_transfer"

        if signal_type_sim is None:
            signal_type_sim = signal_type

        if noise_type_sim is None:
            noise_type_sim = noise_type

        if template_noise_type is None:
            template_noise_type = template_type

        if template_type_sim is None:
            template_type_sim = template_type

        # one of these must be set to do a null test
        null_run = False
        if data_root2 is not None or data_subset2 is not None:
            if data_root2 is None:
                data_root2 = data_root
            if data_subset2 is None:
                data_subset2 = data_subset
            if (data_root, data_subset) == (data_root2, data_subset2):
                raise ValueError(
                    "Either data_root2 or data_subset2 must differ "
                    "from data_root/data_subset"
                )
            null_run = True

        if null_run:
            template_type = None

        opts = dict(
            data_type=data_type,
            noise_type=noise_type,
            mask_type=mask_type,
            signal_type=signal_type,
            signal_transfer_type=signal_transfer_type,
            signal_subset=signal_subset,
            noise_subset=noise_subset,
        )
        ref_opts = dict(data_subset=data_subset, **opts)
        if null_run:
            ref_opts.update(data_subset2=data_subset2)

        def get_sim_data_files(fs):
            """
            Update options for simulated data
            """
            types = ["signal_type_sim", "noise_type_sim", "foreground_type_sim"]
            g = locals()
            if all([g[k] == fs.get(k, None) for k in types]):
                return

            fs.update({k: g[k] for k in types})

            def find_sim_files(name, root=None):
                for suff in ["", "2"] if null_run else [""]:
                    fs.update(
                        self._get_sim_files(
                            name,
                            root,
                            suffix="_sim" + suff,
                            data_suffix=suff,
                            match_count=False,
                            fs=fs,
                        )
                    )

            if str(signal_type_sim) == "r":
                find_sim_files("signal", "signal_r0")
                find_sim_files("tensor", "signal_r1tens")
            else:
                find_sim_files("signal", "signal_{}".format(signal_type_sim))
            if noise_type_sim is not None:
                find_sim_files("noise", "noise_{}".format(noise_type_sim))
            else:
                find_sim_files("noise", None)
            if foreground_type_sim is not None:
                find_sim_files(
                    "foreground", "foreground_{}".format(foreground_type_sim)
                )
            else:
                find_sim_files("foreground", None)

            return True

        def get_template_files(fs):
            """
            Update options for template cleaning. Internal to get_files.

            Arguments
            ---------
            fs : dict
                Dictionary of file options
            """
            types = ["template_type", "template_noise_type", "template_type_sim"]
            g = locals()
            if all([k in fs and g[k] == fs[k] for k in types]):
                return

            if template_type is None:
                fs_temp = {}
                for name, suff in [
                    ("template", ""),
                    ("template_noise", ""),
                    ("template", "_sim"),
                ]:
                    fs_temp["{}_type{}".format(name, suff)] = None
                    fs_temp["num_{}{}".format(name, suff)] = 0
                    for suff2 in ["", "2"]:
                        fs_temp["{}_root{}".format(name, suff + suff2)] = None
                        fs_temp["{}_files{}".format(name, suff + suff2)] = None
                fs.update(**fs_temp)
                for k, v in fs_temp.items():
                    setattr(self, k, v)
                return

            def find_templates(tname, troot, grp, ensemble=False, suffix=""):
                tkey = "{}_type{}".format(tname, suffix)
                nkey = "num_{}{}".format(tname, suffix)
                ttype = g[tkey]
                if tkey not in fs:
                    fs[tkey] = ttype

                nfiles = None
                suffix = suffix + (grp if grp == "2" else "")
                root = os.path.join(fs["data_root"], troot, "template{}".format(grp))

                files = []
                for f in fs["map_files"]:
                    tf = f.replace(fs["map_root"], root)

                    if ensemble:
                        tf = sorted(glob.glob(tf.replace(".fits", "_*.fits")))
                        nfiles1 = len(tf)
                        if not nfiles1:
                            raise OSError(
                                "Missing temp{} {} files for {}".format(grp, tname, f)
                            )
                        if nfiles is None:
                            nfiles = nfiles1
                        elif nfiles1 != nfiles:
                            raise OSError(
                                "Wrong number of {} sims. Found {} files, expected {}.".format(
                                    tname, nfiles1, nfiles
                                )
                            )
                    elif not os.path.exists(tf):
                        raise OSError(
                            "Missing temp{} {} file for {}".format(grp, tname, f)
                        )

                    files.append(tf)

                files = np.asarray(files)
                if nfiles is None:
                    nfiles = len(files)

                if nkey in fs:
                    if fs[nkey] != nfiles:
                        raise OSError(
                            "Wrong number of {} sims. Found {} files, expected {}.".format(
                                tname, nfiles, fs[nkey]
                            )
                        )

                fs_temp = {
                    "{}_root{}".format(tname, suffix): root,
                    "{}_files{}".format(tname, suffix): files,
                    nkey: nfiles,
                }

                fs.update(**fs_temp)
                for k, v in fs_temp.items():
                    setattr(self, k, v)

                self.log("Found {} templates in {}".format(nfiles, root), "info")

            for grp in ["1", "2"]:
                find_templates("template", "templates_{}".format(template_type), grp)
                if subtract_template_noise:
                    find_templates(
                        "template_noise",
                        "templates_noise_{}".format(template_noise_type),
                        grp,
                        ensemble=True,
                    )
                else:
                    suff = "2" if hm == "2" else ""
                    fs.update(
                        {
                            "template_noise_root{}".format(suff): None,
                            "template_noise_files{}".format(suff): None,
                            "num_template_noise": None,
                        }
                    )
                find_templates(
                    "template",
                    "templates_{}".format(template_type_sim),
                    grp,
                    suffix="_sim",
                )

            return True

        def get_reference_files(fs):
            """
            Update options for reference signal subtraction. Internal to get_files.

            Arguments
            ---------
            fs : dict
                Dictionary of file options
            """
            if subtract_reference_signal and fs.get("reference_root", None) is not None:
                return

            if not subtract_reference_signal:
                for k in ["reference_root", "reference_files", "num_reference"]:
                    fs[k] = 0 if k.startswith("num_") else None
                    setattr(self, k, fs[k])
                return

            reference_root = {}
            reference_files = {}
            num_reference = 0
            for null_split in ["a", "b"]:
                if null_split == "a":
                    suff = ""
                else:
                    suff = 2
                for grp in ["1", "2"]:
                    pgrp = "ref{}{}".format(grp, null_split)
                    proot = os.path.join(
                        fs["data_root{}".format(suff)],
                        "reobs_reference",
                        "reference{}".format(hm),
                    )
                    pfiles = []
                    for f in fs["map_files{}".format(suff)]:
                        nfile = f.replace(fs["map_root{}".format(suff)], proot)
                        if not os.path.exists(nfile):
                            raise OSError("Missing ref{} map for {}".format(grp, f))
                        pfiles.append(nfile)
                    if num_reference == 0:
                        num_reference = len(pfiles)
                    elif len(pfiles) != num_reference:
                        raise OSError(
                            "Found {} files for reference group {}, expected {}".format(
                                len(pfiles), pgrp, num_reference
                            )
                        )
                    pfiles = np.asarray(pfiles)
                    reference_root[pgrp] = proot
                    reference_files[pgrp] = pfiles

                    self.log(
                        "Found {} reference maps in {}".format(
                            num_reference,
                            reference_root[pgrp],
                        ),
                        "info",
                    )
                    self.log("Reference files: {}".format(pfiles), "debug")

            loc = locals()
            for k in ["reference_root", "reference_files", "num_reference"]:
                fs[k] = loc[k]
                setattr(self, k, fs[k])

            return True

        def update_files(fs):
            ret1 = get_template_files(fs)
            ret2 = get_reference_files(fs)
            ret3 = get_sim_data_files(fs)
            return ret1 or ret2 or ret3

        save_name = "files"
        alt_name = None
        if data_type != "raw":
            alt_name = save_name
            save_name = "{}_{}".format(save_name, data_type)
        if template_type is not None:
            alt_name = save_name
            save_name = "{}_clean_{}".format(save_name, template_type)
        if subtract_reference_signal:
            alt_name = save_name
            save_name = "{}_ref_sub".format(save_name)
        # load file info from disk
        ret = self.load_data(
            save_name, "files", to_attrs=True, value_ref=ref_opts, alt_name=alt_name
        )
        if ret is not None:
            # fix data root
            ret_data_root = ret["data_root"]
            ret_data_root2 = ret.get("data_root2", data_root2)
            if data_root == ret_data_root and (
                not null_run or data_root2 == ret_data_root2
            ):
                if update_files(ret):
                    self.save_data(save_name, **ret)
                return ret

            def replace_root(k, v):
                if not isinstance(v, str):
                    return v
                if null_run and ret_data_root2 != ret_data_root:
                    if k.endswith("2") and v.startswith(ret_data_root2):
                        return v.replace(ret_data_root2, data_root2)
                if v.startswith(ret_data_root):
                    return v.replace(ret_data_root, data_root)
                return v

            for k, v in list(ret.items()):
                if isinstance(v, str):
                    ret[k] = replace_root(k, v)
                    setattr(self, k, ret[k])
                elif isinstance(v, np.ndarray) and isinstance(v[0], str):
                    ret[k] = np.array(
                        [replace_root(k, vv) for vv in v.ravel()]
                    ).reshape(v.shape)
                    setattr(self, k, ret[k])

            if update_files(ret):
                self.save_data(save_name, **ret)
            return ret

        # find all map files
        fs = self._get_files(data_root, data_subset, **opts)
        fs.update(null_run=null_run, **opts)

        if null_run:
            # find all map files for null tests
            fs2 = self._get_files(data_root2, data_subset2, suffix="2", **opts)

            # make sure map tags match
            if not fs["map_tags"] == fs2["map_tags2"]:
                raise RuntimeError(
                    "Found tags2 {}, expected {}".format(
                        fs2["map_tags2"], fs["map_tags"]
                    )
                )

            # make sure file counts match
            for k, v in fs.items():
                if not k.startswith("num_") or k + "2" not in fs2:
                    continue
                v2 = fs2[k + "2"]
                if v != v2:
                    ftype = k.split("_", 1)[1]
                    raise RuntimeError(
                        "Found {} {}2 files, expected {}".format(v2, ftype, v)
                    )

            # XXX make sure sim numbers match?

            # we're doing a null test
            fs.update(**fs2)

        # update templates, reference and simulated data files
        update_files(fs)

        # store and return settings dictionary
        self.save_data(save_name, **fs)
        for k, v in fs.items():
            setattr(self, k, v)
        return fs

    def get_map(self, filename, check_nside=True, cache=False, **kwargs):
        """
        Load an input map from file or from an internal cache.  Maps are
        checked to make sure they all have a consistent size, and optionally
        cached to limit disk I/O.

        Arguments
        ---------
        filename : string
            Path to file on disk.
        check_nside : bool
            If True (default), make sure that all maps have the same ``nside``,
            and that it satisfies ``lmax <= 4 * nside``.
        cache : bool
            If True, cache the map in memory to avoid rereading from disk.
            Use this for maps that are used multiple times by the algoritm
            (e.g. masks).

        Any remaining arguments are passed to ``healpy.read_map``.

        Returns
        -------
        map : array_like
            2D map array containing 1 (T) or 3 (T/Q/U) maps.
            If the XFaster class was initialized with ``pol = True``, this
            returns a 2D array of T/Q/U maps from the file. Otherwise a
            (1, npix) array is returned containing only the T map.
        """
        import healpy as hp

        # initialize map cache
        if not hasattr(self, "_map_cache"):
            self._map_cache = dict()
        if kwargs.pop("reset", False):
            self._map_cache.pop(filename, None)

        # return a copy from cache if found
        if cache and filename in self._map_cache:
            return np.copy(self._map_cache[filename])

        kwargs.setdefault("field", [0, 1, 2] if self.pol else [0])
        kwargs.setdefault("dtype", None)

        self.log("Reading map from {}".format(filename), "all")
        m = np.atleast_2d(hp.read_map(filename, verbose=False, **kwargs))
        m[hp.mask_bad(m)] = 0
        m[np.isnan(m)] = 0

        if check_nside:

            if not hasattr(self, "nside"):
                self.nside = None

            # check nside
            nside = hp.get_nside(m)
            if self.nside is None:
                self.nside = nside
            else:
                if nside != self.nside:
                    raise ValueError(
                        "Input map {} has nside {} expected {}".format(
                            filename, nside, self.nside
                        )
                    )

            # check npix
            npix = hp.nside2npix(nside)
            if getattr(self, "npix", None) is None:
                self.npix = npix
            else:
                if npix != self.npix:
                    raise ValueError(
                        "Input map {} has npix {} expected {}".format(
                            filename, npix, self.npix
                        )
                    )

            # check lmax
            if self.lmax is None:
                self.lmax = 4 * self.nside
            elif self.lmax > 4 * self.nside:
                self.warn(
                    "lmax {} may be too large for nside {}".format(
                        self.lmax, self.nside
                    )
                )

        if cache:
            self._map_cache[filename] = m
        return m

    def get_mask(self, filename, cache=True, check_lims=True, **kwargs):
        """
        Load an input mask from file or from an internal cache.
        See ``XFaster.get_map`` for details.

        Arguments
        ---------
        filename : string
            Path to mask file on disk.
        cache : bool
            This option defaults to True, since masks are typically used
            for all data and sims for a given map tag.
        check_lims : bool
            If True, values in the mask outside of [0,1] are fixed to
            these limits.

        Any remaining arguments are passed to ``XFaster.get_map``.

        Returns
        -------
        mask : array_like
            2D array containing 1 (T) or 2 (T/P) maps;  If the XFaster class
            was initialized with ``pol_mask = True``, this returns a 2D array
            containing both T and P masks.  Otherwise, a (1, npix) is
            returned containing only the T map.
        """

        fields_mask = [0, 1] if self.pol_mask else [0]
        kwargs.setdefault("field", fields_mask)
        m = self.get_map(filename, cache=cache, **kwargs)
        if check_lims:
            m[m < 0] = 0
            m[m > 1] = 1
        return m

    def get_filename(
        self,
        name,
        ext=".npz",
        map_tag=None,
        iter_index=None,
        extra_tag=None,
        data_opts=False,
        bp_opts=False,
    ):
        """
        Define a standard output file path to read or write.

        Arguments
        ---------
        name : string
            String name of output type.  E.g. 'data_xcorr' for data
            cross-correlation spectra.
            If an output tag is set, the name is appended with
            '_<output_tag>'.
        ext : string
            File extension.  The default ('.npz') is used for storing
            output data dictionaries.
        map_tag : string
            If supplied, the name is appended with '_map_<map_tag>'.
            Use this argument when storing output data in a loop over
            input maps.
        iter_index : int
            If supplied, the name is appended with '_iter<iter_index>'.
        extra_tag : string
            If supplied the extra tag is appended to the name as is.
        data_opts : bool
            If True, the output filename is constructed by checking the
            following list of options used in constructing data cross-spectra:
            ensemble_mean, ensemble_median, sim_index, sim_type, data_type,
            template_cleaned, reference_subtracted.
        bp_opts : bool
            If True, also check the following attributes (in addition to those
            checked if ``data_opts`` is True): weighted_bins, return_cls.

        Returns
        -------
        filename : string
            Output filename as ``<output_root>/<name><ext>``, where
            <name> is constructed from the above set of options.
        """
        if self.output_root is None:
            return None

        name = [name]
        if data_opts or bp_opts:
            if getattr(self, "ensemble_mean", False):
                name += ["sim_mean"]
            elif getattr(self, "ensemble_median", False):
                name += ["sim_median"]
            else:
                if getattr(self, "sim_type", None) is not None:
                    name += ["sim"]
                    for comp in ["signal", "noise", "foreground", "template"]:
                        if comp not in self.sim_type:
                            continue
                        name += [self.sim_type[comp]]
                        if comp not in self.sim_index:
                            continue
                        name += ["{}{:04d}".format(comp[0], self.sim_index[comp])]
                        if comp == "signal" and "tensor" in self.sim_index:
                            name += ["t{:04d}".format(sidx["tensor"])]
                elif self.data_type != "raw":
                    name += [self.data_type]
                if getattr(self, "template_cleaned", False):
                    name += ["clean", self.template_type]
                if getattr(self, "reference_subtracted", False):
                    name += ["ref_sub"]
        if bp_opts:
            if self.weighted_bins:
                name += ["wbins"]
            if getattr(self, "return_cls", False):
                name += ["cl"]

        if map_tag is not None:
            name += ["map", map_tag]
        if iter_index is not None:
            name += ["iter{:03d}".format(iter_index)]
        if extra_tag is not None:
            name += [extra_tag]
        if self.output_tag:
            name += [self.output_tag]

        name = "_".join(name)
        if not ext.startswith("."):
            ext = ".{}".format(ext)
        return os.path.join(self.output_root, "{}{}".format(name, ext))

    def load_data(
        self,
        name,
        checkpoint,
        fields=None,
        to_attrs=True,
        shape=None,
        shape_ref=None,
        alt_name=None,
        value_ref=None,
        optional=None,
        **file_opts
    ):
        """
        Load xfaster data from an output.npz file on disk.

        This method is called throughout the code at various checkpoints.  If
        the data exist on disk, they are loaded and returned.  If the data are
        missing or otherwise incompatible, they are recomputed by the corresponding
        calling method, and trigger all subsequent data to also be recomputed.  Data
        handling is described in the ``Notes`` section for methods that use this
        functionality.

        Arguments
        ---------
        name : string
            The name of the data set.  The filename is contructed from this
            as ``<output_root>/<name>_<output_tag>.npz``.  If the file is not
            found then the data are recomputed.
        checkpoint : string
            The name of the checkpoint to which this dataset applies.
            If XFaster is initialized at this checkpoint, or if any of the
            file checks enabled with the following options fails, all
            quantities from this point forward are recomputed.
        fields : list of strings
            List of fields that should be present in the data file.
            If any are not found, the entire dataset and all subsequent
            step are recomputed.
        to_attrs : bool or list of bools or strings
            If True, all items in ``fields`` are stored as attributes of the
            parent object. If A list of booleans, must have the same length
            as ``fields``; any field for which this list item is True is then
            stored as an attribute of the object.  If any list item is a string,
            then the corresponding field is stored as an attribute with this
            new name.
        shape : tuple of ints
            If set, the field specified by ``shape_ref`` is checked to have this
            shape.  If this check fails, then all data are recomputed.
        shape_ref : string
            The reference field whose shape is checked against ``shape``.
            If None and ``shape`` is set, use the first field in ``fields``.
        alt_name : string
            Alternative to ``name`` argument that will be read if file matching
            ``name`` doesn't exist.
        value_ref : dict
            Dictionary of reference values that is checked if simply loading a
            file from disk instead of recomputing-- forces rerun of checkpoints
            if loaded dictionary differs from value_ref.
        optional : list of strings
            Fields that, if missing from the data loaded from disk, will not
            trigger force rerunning of any checkpoints.


        Remaining options are passed to ``get_filename`` for constructing the
        output file path.

        Returns
        -------
        data : dict
            If all checks above succeed, the requested data are returned.
            If any tests fail, None is returned, and all subsequent calls
            to ``load_data`` also return None to trigger recomputing all data
            that may depend on this dataset.
            The output dictionary has the additional key 'output_file' which
            is set to the path to the data file on disk.
        """

        # checkpointing
        def force_rerun_children():
            """Trigger rerunning steps that depend on this checkpoint."""
            for step in self.checkpoint_tree.get(checkpoint, []):
                if step not in self.checkpoints:
                    raise ValueError(
                        "Invalid checkpoint {}, must be one of {}".format(
                            step, self.checkpoints
                        )
                    )
                self.force_rerun[step] = True
            return None

        if checkpoint not in self.checkpoints:
            raise ValueError(
                "Invalid checkpoint {}, must be one of {}".format(
                    checkpoint, self.checkpoints
                )
            )

        if self.checkpoint == checkpoint:
            self.force_rerun[checkpoint] = True
        if self.force_rerun[checkpoint]:
            return force_rerun_children()

        use_alt = False
        output_file = self.get_filename(name, ext=".npz", **file_opts)
        if not output_file:
            return force_rerun_children()
        errmsg = "Error loading {}".format(output_file)
        if not os.path.exists(output_file):
            self.warn("{}: Output file not found".format(errmsg))
            if alt_name is not None:
                output_file = self.get_filename(alt_name, ext=".npz", **file_opts)
                errmsg = "Error loading {}".format(output_file)
                if not os.path.exists(output_file):
                    self.warn("{}: Alternate output file not found".format(errmsg))
                    return force_rerun_children()
                else:
                    use_alt = True
            else:
                return force_rerun_children()

        try:
            data = pt.load_and_parse(output_file)
        except Exception as e:
            self.warn("{}: {}".format(errmsg, str(e)))
            return force_rerun_children()

        if fields is None:
            fields = list(data)

        if not isinstance(fields, list):
            fields = [fields]

        if shape_ref is not None and shape_ref not in fields:
            self.warn("{}: Field {} not found".format(errmsg, shape_ref))
            return force_rerun_children()

        if to_attrs is True or to_attrs is False:
            to_attrs = [to_attrs] * len(fields)

        if shape is not None and shape_ref is None:
            shape_ref = fields[0]

        if value_ref is not None:
            value_ref = copy.deepcopy(value_ref)

        ret = dict()
        for field, attr in zip(fields, to_attrs):
            if field not in data:
                if optional is not None and field in optional:
                    data[field] = None
                else:
                    self.warn("{}: Field {} not found".format(errmsg, field))
                    return force_rerun_children()
            v = pt.dict_to_arr(data[field])
            try:
                v.shape
            except AttributeError:
                v = np.asarray(v)
            if not v.shape:
                v = v.tolist()
            if shape_ref in [field, attr]:
                if v.shape != tuple(shape):
                    # check if it's just an extra dimension, ie (1,500) vs (500,)
                    passes = False
                    if tuple(shape)[0] == 1:
                        if v.shape == tuple(shape)[1:]:
                            passes = True
                    if not passes:
                        self.warn(
                            "{}: Field {} has shape {}, expected {}".format(
                                errmsg, shape_ref, v.shape, shape
                            )
                        )
                        return force_rerun_children()
            if value_ref is not None:
                for k in [field, attr]:
                    vref = value_ref.pop(k, "undef")
                    # turn it into an array if possible to compare
                    try:
                        vref = pt.dict_to_arr(vref)
                    except:
                        pass

                    if not vref == "undef" and np.any(v != vref):
                        self.warn(
                            "{}: Field {} has value {}, expected {}".format(
                                errmsg, k, v, vref
                            )
                        )
                        return force_rerun_children()
            ret[field] = data[field]
            if attr:
                key = field if attr is True else attr
                setattr(self, key, ret[field])

        if value_ref:
            self.warn("{}: Missing reference fields {}".format(errmsg, list(value_ref)))
            return force_rerun_children()

        self.log("Loaded input data from {}".format(output_file), "debug")
        if use_alt:
            # copy data to original file name
            ret.update(**file_opts)
            self.save_data(name, **ret)
            for k in file_opts:
                ret.pop(k)
        ret["output_file"] = output_file
        return ret

    def save_data(self, name, from_attrs=[], **data):
        """
        Save xfaster data to an output .npz file on disk.

        Arguments
        ---------
        name : string
            The name of the data set.  The filename is contructed from this as
            ``<output_root>/<name>_<output_tag>.npz``.  If the file is not found
            then the data are recomputed.
        from_attrs : list of strings
            A list of object attributes which should be stored in the data file.
        map_tag : str
            Load the dataset corresponding to this map.
            See ``get_filename`` for documentation.
        iter_index : int
            Load the dataset corresponding to this iteration index.
            See ``get_filename`` for documentation.
        bp_opts : bool
            Format output bandpowers file.  See ``get_filename`` for
            documentation.
        extra_tag : str
             Tag to add to file name.

        Any remaining keyword arguments are added to the output dictionary.

        Returns
        -------
        data : dict
            A copy of the data dictionary that was stored to disk.
            The output dictionary has the additional key 'output_file' which
            is set to the path to the data file on disk.
        """
        data["data_version"] = self.data_version

        file_opts = {}
        for opt in ["map_tag", "iter_index", "data_opts", "bp_opts", "extra_tag"]:
            if opt in data:
                file_opts[opt] = data.pop(opt)

        output_file = self.get_filename(name, ext=".npz", **file_opts)
        if not output_file:
            return

        for attr in from_attrs:
            if hasattr(self, attr):
                data[attr] = getattr(self, attr)

        np.savez_compressed(output_file, **data)
        self.log("Saved output data to {}".format(output_file), "debug")
        data["output_file"] = output_file
        return data

    def save_config(self, cfg):
        """
        Save a configuration file for the current run on disk.
        This method is used by ``xfaster_run`` to store the config
        in ``<output_root>/config_<output_tag>.txt``.

        Arguments
        ---------
        cfg : XFasterConfig or RawConfigParser object
            Config object containing all relevant arguments to save to disk.

        Returns
        -------
        filename : str
            Name of the config file saved to disk.
        """
        filename = self.get_filename("config", ext=".txt")
        if filename is None:
            return

        if not isinstance(cfg, XFasterConfig):
            cfg = XFasterConfig(cfg)

        try:
            creator = os.getlogin()
        except OSError:
            creator = "unknown"
        with open(filename, "w") as f:
            f.write(
                "# Created by {} on {:%Y-%m-%d %H:%M:%S}\n\n".format(
                    creator, datetime.datetime.now()
                )
            )
            cfg.write(f)

        return filename

    def apply_mask(self, m, mask):
        """
        Apply the input mask to the data map, in place.

        If the map is polarized, the appropriate mask is applied
        to the polarization data, depending on whether the mask
        is also polarized.

        Arguments
        ---------
        m : array_like
            Input map (T/Q/U if polarized, T-only if not)
            This array is modified in place.
        mask : array_like
            Mask to apply (T/P if polarized, T-only if not)

        Returns
        -------
        m : array_like
            Input map multiplied by mask.
        """

        m[0] *= mask[0]
        if self.pol:
            m[1:] *= (mask[1] if self.pol_mask else mask[0])[None, :]

        return m

    def map2alm(self, m, pol=None):
        """
        Wrapper for healpy.map2alm.

        Arguments
        ---------
        m : array_like
            Masked input map for which spherical harmonic alms are
            computed.
        pol : bool
            If None, this is set using the value with which the object
            was initialized.

        Returns
        -------
        alms : array_like
            Alms for the input map, computed using the equivalent of
            ``healpy.map2alm (m, lmax, pol=self.pol, use_weights=True)``.
        """
        import healpy as hp

        if pol is None:
            pol = self.pol
        return np.asarray(hp.map2alm(m, self.lmax, pol=pol, use_weights=True))

    def alm2cl(self, m1, m2=None, lmin=2, lmax=None, symmetric=True):
        """
        Wrapper for healpy.alm2cl.

        Arguments
        ---------
        m1 : array_like
            Masked alms for map1
        m2 : array_like
            Masked alms for map2
        lmin : int, default: 2
            The minimum ell bin to include in the output Cls.  All ell
            bins below this are nulled out.
        lmax : int
            The maximum ell bin to compute.  If None, this is set to the
            lmax value with which the class was initialized.
        symmetric : bool, default: True
            If True, the average cross spectrum of (m1-x-m2 + m2-x-m1) / 2.
            is computed.

        Returns
        -------
        cls : array_like
            Cross-spectrum of m1-x-m2.
        """
        import healpy as hp

        if lmax is None:
            lmax = self.lmax
        cls = np.asarray(hp.alm2cl(m1, alms2=m2, lmax=lmax))
        if symmetric:
            cls_T = np.asarray(hp.alm2cl(m2, alms2=m1, lmax=lmax))
            cls = (cls + cls_T) / 2.0
        if lmin:
            cls[..., :lmin] = 0
        return np.atleast_2d(cls)

    def get_mask_weights(self, apply_gcorr=False, reload_gcorr=False, gcorr_file=None):
        """
        Compute cross spectra of the masks for each data map.

        Mode counting matrices are also computed and stored for each mask.

        Arguments
        ---------
        apply_gcorr : bool
            If True, a correction factor is applied to the g (mode counting)
            matrix.  The correction factor should have been pre-computed for
            each map tag using independent scripts in the code package.
        reload_gcorr : bool
            If True, reload the gcorr file from the masks directory. Useful when
            iteratively solving for the correction terms.
        gcorr_file : str
            If not None, path to gcorr file. Otherwise, use file labeled
            mask_map_<tag>_gcorr.npz in mask directory for signal, or
            mask_map_<tag>_gcorr_null.npz for nulls.

        Notes
        -----
        This method is called at the 'masks' checkpoint, loads or saves a
        data dictionary with the following keys:

            wls:
                mask1-x-mask2 mask cross spectra for every mask pair
            fsky, w1, w2, w4:
                sky fraction and weighted modes per mask product
            gmat:
                mode-counting matrix, computed from ``g = fsky * w2 ** 2 / w4``
        """

        mask_files = self.mask_files
        num_maps = self.num_maps
        mask_shape = self.mask_shape
        save_attrs = ["wls", "fsky", "w1", "w2", "w4", "gmat", "nside", "npix", "gcorr"]
        save_name = "masks_xcorr"
        ret = self.load_data(
            save_name,
            "masks",
            fields=save_attrs,
            optional=["gcorr"],
            to_attrs=True,
            shape=mask_shape,
            shape_ref="wls",
            alt_name="data_xcorr",
        )

        def process_gcorr(gcorr_file_in):
            if not hasattr(self, "gcorr"):
                self.gcorr = None
            if apply_gcorr and self.gcorr is None:
                self.gcorr = OrderedDict()

            for tag, mfile in zip(self.map_tags, self.mask_files):
                if not apply_gcorr:
                    continue

                if not reload_gcorr and tag in self.gcorr:
                    continue

                if gcorr_file_in is None:
                    if self.null_run:
                        gcorr_file = mfile.replace(".fits", "_gcorr_null.npz")
                    else:
                        gcorr_file = mfile.replace(".fits", "_gcorr.npz")
                else:
                    gcorr_file = gcorr_file_in

                if not os.path.exists(gcorr_file):
                    self.warn("G correction file {} not found".format(gcorr_file))
                    continue

                gdata = pt.load_and_parse(gcorr_file)
                gcorr = gdata["gcorr"]
                for k, g in gcorr.items():
                    # check bins match g
                    bd0 = self.bin_def["cmb_{}".format(k)]
                    bd = gdata["bin_def"]["cmb_{}".format(k)]
                    if len(bd0) < len(bd):
                        bd = bd[: len(bd0)]
                        gcorr[k] = g[: len(bd)]
                    if not np.all(bd0 == bd):
                        self.warn(
                            "G correction for map {} has incompatible bin def".format(
                                tag
                            )
                        )
                        break
                else:
                    self.log(
                        "Found g correction for map {}: {}".format(tag, gcorr), "debug"
                    )
                    self.gcorr[tag] = gcorr

            # compute ell-by-ell mode counting factor
            gmat_ell = OrderedDict()
            ell = np.arange(self.lmax + 1)
            ellfac = 2.0 * ell + 1.0

            for xname, (m0, m1) in self.map_pairs.items():
                gmat_ell[xname] = OrderedDict()
                if apply_gcorr:
                    gcorr0 = self.gcorr[m0]
                    gcorr1 = self.gcorr[m1]

                for spec in self.specs:
                    gmat_ell[xname][spec] = self.gmat[xname][spec] * ellfac
                    if apply_gcorr:
                        gcorr = np.sqrt(gcorr0[spec] * gcorr1[spec])
                        bd = self.bin_def["cmb_{}".format(spec)]
                        for gc, (start, stop) in zip(gcorr, bd):
                            gmat_ell[xname][spec][start:stop] *= gc

            self.apply_gcorr = apply_gcorr
            self.gmat_ell = gmat_ell

        if ret is not None:
            process_gcorr(gcorr_file)
            if apply_gcorr and (reload_gcorr or ret.get("gcorr", None) is None):
                return self.save_data(save_name, from_attrs=save_attrs)
            ret["gcorr"] = self.gcorr
            return ret

        # mask spectra
        wls = OrderedDict()

        pol_dim = 3 if self.pol else 1

        # moments
        fsky = OrderedDict()
        fsky_eff = OrderedDict()
        w1 = OrderedDict()
        w2 = OrderedDict()
        w4 = OrderedDict()
        gmat = OrderedDict()

        cache = dict()

        def process_index(idx):
            """
            A internal convenience function computes/loads alms from/to cache for each mask idx.
            """
            if idx in cache:
                return cache[idx]

            self.log("Computing Alms for mask {}/{}".format(idx + 1, num_maps), "all")

            mask = self.get_mask(mask_files[idx])
            mask_alms = self.map2alm(mask, False)

            cache[idx] = (mask_alms, mask)
            return cache[idx]

        spec_inds = {
            "tt": (0, 0),
            "ee": (1, 1),
            "bb": (2, 2),
            "te": (0, 1),
            "eb": (1, 2),
            "tb": (0, 2),
        }

        for xname, (idx, jdx) in pt.tag_pairs(self.map_tags, index=True).items():
            imask_alms, imask = process_index(idx)
            jmask_alms, jmask = process_index(jdx)

            self.log("Computing mask spectra {}x{}".format(idx, jdx), "debug")
            wls[xname] = self.alm2cl(imask_alms, jmask_alms, lmin=0)

            if self.pol_mask:
                # If there is a pol mask in addition to I, copy it to
                # U so no I masks is [I, Q, U] instead of [I, pol]
                imask = np.vstack([imask, imask[1]])
                jmask = np.vstack([jmask, jmask[1]])

            # calculate moments of cross masks
            # this is an array of shape (pol_dim, pol_dim, npix)
            # and contains all combinations of mask products
            mask = np.sqrt(np.einsum("i...,j...->ij...", imask, jmask))
            counts = [np.count_nonzero(x) for x in mask.reshape(-1, mask.shape[-1])]
            counts = np.array(counts).reshape(self.pol_dim, self.pol_dim).astype(float)

            # fsky is the fraction of pixels that are nonzero, independent
            # of weight
            fsky[xname] = counts / self.npix
            w1[xname] = np.sum(mask, axis=-1) / counts
            w2[xname] = np.sum(mask ** 2, axis=-1) / counts
            w4[xname] = np.sum(mask ** 4, axis=-1) / counts
            # effective fsky takes into account weights between 0 and 1
            fsky_eff[xname] = (
                fsky[xname]
                * w2[xname] ** 2
                / w4[xname]
                * (1.0 + 4.0 * fsky[xname])  # second order correction
            )

            # compute gmat as the average fsky_eff assuming symmetrically
            # computed cross spectra, e.g. TE = (T1 * E2 + T2 * E1) / 2
            gmat[xname] = OrderedDict()
            for spec in self.specs:
                si, sj = spec_inds[spec]
                f = (fsky_eff[xname][si, sj] + fsky_eff[xname][sj, si]) / 2.0
                gmat[xname][spec] = f

        if np.any(np.asarray([f for f in fsky.values()]) > 0.1):
            self.warn(
                "Some fsky are larger than 10% - second order "
                "correction may break down here: {}".format(fsky)
            )

        # store and return
        self.wls = wls
        self.fsky = fsky
        self.w1 = w1
        self.w2 = w2
        self.w4 = w4
        self.gmat = gmat

        process_gcorr(gcorr_file)

        self.log("Fsky: {}".format(self.fsky), "debug")
        self.log("Effective fsky: {}".format(fsky_eff), "debug")
        self.log("Mask moments 1: {}".format(self.w1), "debug")
        self.log("Mask moments 2: {}".format(self.w2), "debug")
        self.log("Mask moments 4: {}".format(self.w4), "debug")
        self.log("G matrix: {}".format(self.gmat), "debug")

        return self.save_data(save_name, from_attrs=save_attrs)

    def get_noise_residuals(self, filename):
        """
        Return a dictionary of ell-by-ell noise residual spectra from an output
        bandpowers file, to be applied to noise Alm's using ``healpy.almxfl``.
        """
        rls = OrderedDict()

        if not os.path.exists(filename):
            filename = os.path.join(self.output_root, filename)
        data = pt.load_and_parse(filename)

        for k, bd in data["bin_def"].items():
            if not k.startswith("res_"):
                continue
            _, specs, map_tag = k.split("_", 2)
            specs = [x + x for x in np.unique(list(specs))]

            res_qb = np.sqrt(1.0 + data["qb"][k]) - 1.0
            (bad,) = np.where(np.isnan(res_qb))
            if len(bad):
                self.warn("Unphysical residuals fit, nulling {} bins {}".format(k, bad))
                res_qb[bad] = 0.0
            rl = pt.expand_qb(res_qb, bd)

            if map_tag not in rls:
                rls[map_tag] = OrderedDict()
            for spec in specs:
                rls[map_tag][spec] = rl

        return rls

    def get_masked_data(
        self,
        template_alpha=None,
        subtract_reference_signal=False,
        subtract_template_noise=True,
        template_specs=None,
        ensemble_mean=False,
        ensemble_median=False,
        sim=False,
        components=["signal", "noise", "foreground"],
        index=None,
        r=None,
        qb_file=None,
        template_alpha_sim=None,
        save_sim=False,
    ):
        """
        Compute cross spectra of the data maps.

        Map and mask files must have been loaded in by calling the ``get_files``
        method with the appropriate file selection options.

        If only one dataset is selected, spectra are computed for every
        combination of pairs of data maps. This results in N * (N + 1) / 2
        cross spectra for N maps. A unique mask is used for each input map.

        If two datasets are selected for a null test, then sum and difference cross-spectra are
        computed by summing and differencing the two datasets.  A unique mask is
        used for each map in the first dataset, and the same mask is applied to
        the corresponding map in the second dataset, so that both halves are
        masked identically.

        If ``template_alpha`` is supplied, the values given are applied to an
        appropriate template, and the result is subtracted from the data alms
        with map tags in the dictionary.  Map alms are cached to speed up
        processing, if this method is called repeatedly with different values.

        The remaining options handle subtraction of additional biases from the
        data, or constructing simulated datasets from sim ensembles.

        Arguments
        ---------
        template_alpha : dict
            Dictionary of template scaling factors to apply to foreground
            templates to be subtracted from the data.  Keys should match
            original map tags in the data set.
        subtract_reference_signal : bool
            If True, subtract a reobserved reference signal from each data map.
            The reference signal maps should be two datasets with uncorrelated
            noise, such as Planck half-mission maps.  This option is used for
            removing expected signal residuals from null tests.
        subtract_template_noise : bool
            If True, subtract average of cross spectra of an ensemble of noise
            realizations corresponding to each template map, to debias
            template-cleaned spectra.  Typically, this would be a noise model based
            on the Planck FFP10 ensemble for each half-mission foreground template.
        template_specs : list
            Which spectra to use for alpha in the likelihood.
        ensemble_mean : bool
            If True, use the mean of the ``signal_type`` and ``noise_type``
            ensembles, rather than using maps from the ``data_type`` directory
            or any other sim options.  This is useful for testing the behavior
            of the estimator and mapmaker independently of the data.
        ensemble_median : bool
            If True, use the median of the ``signal_type`` and ``noise_type``
            ensembles, rather than using maps from the ``data_type`` directory
            or any other sim options.  This is useful for testing the behavior
            of the estimator and mapmaker independently of the data.
        sim : bool
            If True, construct simulated data maps using the options below.
        components : list of strings
            List of components to include in the simulated data, of signal,
            noise, foreground or template.
        index : int or dict
            If supplied and ``sim`` is True, then simulated data maps are
            constructed from the appropriate index from the sim ensembles
            ``signal_type_sim``, ``noise_type_sim`` and/or
            ``foreground_type_sim``, rather than using maps from the
            ``data_type`` directory.  If an integer, then the same index is used
            for all ensembles.  Otherwise, should be a dictionary keyed by
            component (signal, tensor, noise, foreground).  Additionally, the key
            ``default`` can be used to indicate the index to use for components
            that are not explicitly enumerated in the dictionary.  If not
            supplied, and ``sim`` is True, index 0 is used for all ensembles.
        r : float
            If supplied, the simulated signal maps are constructed by combining
            ``signal_scalar + r * signal_tensor``, where the scalar maps are
            stored in the directory ``signal_root_sim``, and the tensor maps are
            stored in the directory ``tensor_root_sim``.  A separate sim index
            is assumed for the ``signal`` (scalar) ensemble and the ``tensor``
            ensemble.
        qb_file : str
            If supplied and noise is included in ``components``, correct the
            simulated noise spectra using the noise residuals stored in this
            file.  Typically, this is the output of a separate data run used to
            determine the appropriate noise correction.  See
            ``get_noise_residuals`` for more details.
        template_alpha_sim : dict
            Dictionary of template scaling factors to apply to foreground
            templates to be added to the simulated the data.  Keys should match
            original map tags in the data set.
        save_sim : bool
            If True and constructing a simulated dataset using any of the above
            sim options, write the simulated dataset to disk using an
            appropriate ``'data_xcorr.npz'`` filename.  If False, only
            non-simulated datasets are written to disk.

        Notes
        -----
        This method is called at the 'data' checkpoint, loads or saves a data
        dictionary with the following spectra:

            cls_data:
                map1-x-map2 cross spectra for every map pair. This contains the
                sum cross spectra if constructing a null test.
            cls_data_clean:
                template_subtracted spectra, if ``template_alpha`` is supplied.
            cls_template:
                template cross spectra necessary to rebuild the
                template-subtracted data when the ``template_alpha`` parameter
                is changed.
            cls_data_null:
                (map1a-map1b)-x-(map2a-map2b) difference cross spectra for every
                map pair, if computing a null test
            cls_ref, cls_ref_null:
                reference cross spectra, if ``subtract_reference_signal`` is
                True.
        """
        import healpy as hp

        num_maps = self.num_maps
        data_shape = self.data_shape
        null_run = self.null_run

        if template_specs is None:
            template_specs = self.specs

        # ensure dictionary
        if template_alpha is None or null_run or self.template_type is None:
            template_alpha = OrderedDict()
        else:
            # ensure tagged by original tags
            template_alpha = OrderedDict(
                [(k, v) for k, v in template_alpha.items() if k in self.map_tags_orig]
            )

        # set sim attributes
        if ensemble_mean or ensemble_median or not sim:
            if ensemble_mean:
                cls_ens = self.cls_sim
                cls_ens_null = self.cls_sim_null if null_run else None
            elif ensemble_median:
                cls_ens = self.cls_med
                cls_ens_null = self.cls_med_null if null_run else None
            sim_type = components = index = r = qb_file = template_alpha_sim = None
        elif sim:
            if index is None:
                index = {}
            else:
                index = index.copy()
            default_index = index.pop("default", 0)
            sim_type = {}
            if (
                null_run or self.template_type_sim is None
            ) and "template" in components:
                components.remove("template")
            for comp in ["signal", "noise", "foreground", "template"]:
                if comp not in components:
                    if comp == "signal":
                        r = None
                    elif comp == "noise":
                        qb_file = None
                    elif comp == "template":
                        template_alpha_sim = None
                    continue
                root = getattr(self, "{}_root_sim".format(comp), None)
                if root is None:
                    raise ValueError("Missing {} sim files".format(comp))
                if comp == "signal" and r is not None:
                    if self.tensor_root_sim is None:
                        raise ValueError("Missing tensor signal sim files")
                    sim_type[comp] = "rp{:03d}".format(int(r * 1000))
                    index.setdefault("tensor", default_index)
                else:
                    sim_type[comp] = getattr(self, "{}_type_sim".format(comp))
                if comp == "template":
                    if template_alpha_sim is None:
                        template_alpha_sim = template_alpha
                    else:
                        template_alpha_sim = OrderedDict(
                            [
                                (k, v)
                                for k, v in template_alpha_sim.items()
                                if k in self.map_tags_orig
                            ]
                        )
                else:
                    index.setdefault(comp, default_index)

        self.ensemble_mean = ensemble_mean
        self.ensemble_median = ensemble_median
        self.sim_type = sim_type
        self.sim_index = index

        # Check for output data on disk
        save_attrs = ["cls_data", "nside"]
        template_fit = False
        if null_run:
            save_attrs += ["cls_data_null"]
            if subtract_reference_signal:
                save_attrs += ["cls_data_sub_null", "cls_ref_null"]
        elif any([x is not None for x in template_alpha.values()]):
            template_fit = True
            self.template_cleaned = True
            save_attrs += ["cls_data_clean", "cls_template", "template_alpha"]
        else:
            subtract_template_noise = False
        if subtract_reference_signal:
            self.reference_subtracted = True
            save_attrs += ["cls_data_sub", "cls_ref"]
        if sim:
            if template_alpha_sim:
                self.template_alpha_sim = template_alpha_sim
                save_attrs += ["template_alpha_sim"]

        def apply_template():
            """
            Internal data processing function to have scaled foreground template subtracted from data map.
            """
            if subtract_template_noise:
                if getattr(self, "cls_template_noise", None) is None:
                    self.cls_template_noise = self.get_masked_template_noise()

            cls_clean = getattr(self, "cls_data_clean", OrderedDict())

            for spec in set(self.specs) & set(template_specs):
                cls_clean[spec] = copy.deepcopy(self.cls_data[spec])
                if spec not in self.cls_template:
                    continue
                for xname, d in cls_clean[spec].items():
                    if xname not in self.cls_template[spec]:
                        continue
                    m0, m1 = self.map_pairs_orig[xname]
                    alphas = [template_alpha.get(m, None) for m in (m0, m1)]

                    t1, t2, t3 = self.cls_template[spec][xname]

                    if alphas[0] is not None:
                        d -= alphas[0] * t1
                    if alphas[1] is not None:
                        d -= alphas[1] * t2
                        if alphas[0] is not None:
                            d += alphas[0] * alphas[1] * t3
                            # subtract average template noise spectrum to debias
                            if subtract_template_noise:
                                n = self.cls_template_noise["temp1:temp2"][spec][xname]
                                d -= alphas[0] * alphas[1] * n

            self.cls_data_clean = cls_clean
            self.template_alpha = template_alpha
            self.template_cleaned = True

        def subtract_reference_maps():
            """
            Internal data processing function to have reference maps subtracted from data map, useful for null tests
            """
            cls_data_sub = getattr(self, "cls_data_sub", OrderedDict())
            if null_run:
                cls_data_sub_null = getattr(self, "cls_data_sub_null", OrderedDict())

            for spec in self.specs:
                cls_data_sub[spec] = copy.deepcopy(self.cls_data[spec])
                for xname, d in cls_data_sub[spec].items():
                    t1, t2, t3 = self.cls_ref[spec][xname]
                    d += -t1 - t2 + t3

                # do null specs
                if null_run:
                    cls_data_sub_null[spec] = copy.deepcopy(self.cls_data_null[spec])
                    for xname, d in cls_data_sub_null[spec].items():
                        t1, t2, t3 = self.cls_ref_null[spec][xname]
                        d += -t1 - t2 + t3

            self.cls_data_sub = cls_data_sub
            if null_run:
                self.cls_data_sub_null = cls_data_sub_null
            self.reference_subtracted = True

        # change template subtraction coefficients for pre-loaded data
        if all([hasattr(self, attr) for attr in save_attrs]):
            if not template_fit and not getattr(self, "template_cleaned", False):
                return {k: getattr(self, k) for k in save_attrs}

            if template_fit and getattr(self, "template_cleaned", False):
                if not np.all(template_alpha == self.template_alpha):
                    apply_template()
                return {k: getattr(self, k) for k in save_attrs}

        save_name = None if sim and not save_sim else "data"
        if save_name is not None:
            ret = self.load_data(
                save_name,
                "data",
                fields=save_attrs,
                to_attrs=True,
                shape=data_shape,
                shape_ref="cls_data",
                data_opts=True,
                extra_tag="xcorr",
            )
            if ret is not None:
                self.reference_subtracted = False
                self.template_cleaned = False
                if null_run:
                    if subtract_reference_signal and not self.reference_subtracted:
                        subtract_reference_maps()
                    return ret
                if all([x is None for x in template_alpha.values()]):
                    self.template_cleaned = False
                    return ret
                if np.all(template_alpha == self.template_alpha):
                    self.template_cleaned = True
                    return ret
                apply_template()
                return ret

        # map spectra
        cls = OrderedDict()
        cls_null = OrderedDict() if null_run else None

        # set up template subtraction
        cls_tmp = None
        if template_fit:
            cls_tmp = OrderedDict()
            template_files = list(zip(self.template_files, self.template_files2))

        # set up reference subtraction
        cls_ref = None
        if subtract_reference_signal:
            cls_ref = OrderedDict()
            cls_ref_null = OrderedDict() if null_run else None
            reference_files_split = list(
                zip(
                    self.reference_files[x]
                    for x in ["ref1a", "ref2a", "ref1b", "ref2b"]
                )
            )

        # set up noise residuals
        rls = OrderedDict()
        if sim and qb_file is not None:
            rls = self.get_noise_residuals(qb_file)

        # convenience functions
        def get_map_file(attr, idx, suffix=""):
            files = getattr(self, "{}{}".format(attr, suffix))
            if isinstance(files[0], str):
                return files[idx]
            files = files[idx]
            # list of available file indices
            fidx = [int(os.path.splitext(x.rsplit("_")[-1])[0]) for x in files]
            # sim index for this component
            comp = attr.split("_")[0]
            sidx = index[comp]
            if sidx not in fidx:
                raise ValueError("Missing sim index {} in {}".format(sidx, attr))
            return files[fidx.index(sidx)]

        def get_map_alms(idx, suffix=""):
            m = mn = mr = None
            if sim and "noise" in sim_type:
                rls1 = rls.get(self.map_tags[idx], None)

            mask = self.get_mask(self.mask_files[idx])
            if not sim:
                # data from disk
                f = get_map_file("map_files", idx, suffix)
                m = self.get_map(f)

            else:
                # simulated data constructed from individual components
                if "signal" in sim_type:
                    f = get_map_file("signal_files_sim", idx, suffix)
                    m = self.get_map(f)
                    if r is not None:
                        f = get_map_file("tensor_files_sim", idx, suffix)
                        m += np.sqrt(r) * self.get_map(f)

                if "noise" in sim_type:
                    f = get_map_file("noise_files_sim", idx, suffix)
                    mn = self.get_map(f)
                    if m is None:
                        m = mn
                    else:
                        m += mn
                    if rls1 is None:
                        mn = None

                if "foreground" in sim_type:
                    f = get_map_file("foreground_files_sim", idx, suffix)
                    mf = self.get_map(f)
                    if m is None:
                        m = mf
                    else:
                        m += mf
                    del mf

                if "template" in sim_type:
                    alpha = template_alpha_sim.get(self.map_tags_orig[idx], None)
                    if alpha is not None:
                        f = get_map_file("template_files_sim", idx)
                        mt = alpha * self.get_map(f)
                        if m is None:
                            m = mt
                        else:
                            m += mt
                        del mt

            self.apply_mask(m, mask)
            m_alms = self.map2alm(m, self.pol)

            if sim and "noise" in sim_type:
                # add noise residuals
                if rls1 is not None:
                    self.apply_mask(mn, mask)
                    mn_alms = self.map2alm(mn, self.pol)
                    for s, spec in enumerate(self.specs):
                        if spec in rls1:
                            m_alms[s] += hp.almxfl(mn_alms[s], rls1[spec])

            return m_alms

        cache = dict()

        def process_index(idx):
            """
            A internal convenience function computes/loads alms from/to cache for each data map idx,
            or data map pair for null tests.
            """
            if idx in cache:
                return cache[idx]

            self.log("Computing Alms for map {}/{}".format(idx + 1, num_maps), "all")

            mask = self.get_mask(self.mask_files[idx])
            m_alms = get_map_alms(idx)
            mn_alms = None

            if null_run:
                # sum and diff spectra for null tests
                # XXX should not take average but sum here if we want to
                # compare power with sum...
                m2_alms = get_map_alms(idx, "2")
                m_alms, mn_alms = (m_alms + m2_alms) / 2.0, (m_alms - m2_alms) / 2.0
                del m2_alms

                if subtract_reference_signal:
                    # cache raw data alms and reference alms together
                    mp1ref1, mp1ref2, mp2ref1, mp2ref2 = (
                        self.map2alm(self.apply_mask(self.get_map(f), mask), self.pol)
                        for f in reference_files_split[idx]
                    )
                    m_alms_ref1 = (mp1ref1 + mp2ref1) / 2.0
                    m_alms_ref2 = (mp1ref2 + mp2ref2) / 2.0
                    mn_alms_ref1 = (mp1ref1 - mp2ref1) / 2.0
                    mn_alms_ref2 = (mp1ref2 - mp2ref2) / 2.0
                    m_alms = (m_alms, m_alms_ref1, m_alms_ref2)
                    mn_alms = (mn_alms, mn_alms_ref1, mn_alms_ref2)

            elif template_fit:
                # cache raw data alms and template alms together
                m_alms = [m_alms]
                for tf in template_files[idx]:
                    self.log("Loading template from {}".format(tf), "debug")
                    mt_alms = self.map2alm(
                        self.apply_mask(self.get_map(tf), mask), self.pol
                    )
                    # null out T template
                    if self.pol:
                        mt_alms[0] *= 0
                    m_alms.append(mt_alms)
                m_alms = tuple(m_alms)

            cache[idx] = (m_alms, mn_alms)
            return cache[idx]

        map_pairs = pt.tag_pairs(self.map_tags, index=True)

        for xname, (idx, jdx) in map_pairs.items():

            if self.ensemble_mean or self.ensemble_median:
                for spec in self.specs:
                    cls[spec][xname] = cls_ens[spec][xname]
                    if null_run:
                        cls_null[spec][xname] = cls_ens_null[spec][xname]
                continue

            imap_alms, inull_alms = process_index(idx)
            jmap_alms, jnull_alms = process_index(jdx)

            self.log("Computing spectra {}x{}".format(idx + 1, jdx + 1), "debug")

            # store cross spectra
            if isinstance(imap_alms, tuple) and len(imap_alms) == 3:
                # raw map spectrum component
                cls1 = self.alm2cl(imap_alms[0], jmap_alms[0])

                # average template alms
                imt = (imap_alms[1] + imap_alms[2]) / 2.0
                jmt = (jmap_alms[1] + jmap_alms[2]) / 2.0

                # compute maximally symmetric cross spectra
                t1 = self.alm2cl(imt, jmap_alms[0])  # multiplies alpha_i
                t2 = self.alm2cl(imap_alms[0], jmt)  # multiplies alpha_j
                t3 = (
                    self.alm2cl(imap_alms[1], jmap_alms[2])
                    + self.alm2cl(imap_alms[2], jmap_alms[1])
                ) / 2.0  # multiplies alpha_i * alpha_j

                for s, spec in enumerate(self.specs):
                    # apply template to TE/TB but not TT
                    if not subtract_reference_signal and spec == "tt":
                        continue
                    cls_dict = cls_ref if subtract_reference_signal else cls_tmp
                    cls_dict = cls_dict.setdefault(spec, OrderedDict())
                    cls_dict[xname] = (t1[s], t2[s], t3[s])

                if null_run:
                    # do this again for the null maps
                    cls_null1 = self.alm2cl(inull_alms[0], jnull_alms[0])

                    # average template alms
                    imt = (inull_alms[1] + inull_alms[2]) / 2.0
                    jmt = (jnull_alms[1] + jnull_alms[2]) / 2.0

                    # compute maximally symmetric cross spectra
                    t1 = self.alm2cl(imt, jnull_alms[0])
                    t2 = self.alm2cl(inull_alms[0], jmt)
                    t3 = (
                        self.alm2cl(inull_alms[1], jnull_alms[2])
                        + self.alm2cl(inull_alms[2], jnull_alms[1])
                    ) / 2.0

                    for s, spec in enumerate(self.specs):
                        cls_dict = cls_ref_null.setdefault(spec, OrderedDict())
                        cls_dict[xname] = (t1[s], t2[s], t3[s])

            else:
                cls1 = self.alm2cl(imap_alms, jmap_alms)
                if null_run:
                    cls_null1 = self.alm2cl(inull_alms, jnull_alms)

            for s, spec in enumerate(self.specs):
                cls.setdefault(spec, OrderedDict())[xname] = cls1[s]
                if null_run:
                    cls_null.setdefault(spec, OrderedDict())[xname] = cls_null1[s]

        # store and return
        self.cls_data = cls
        self.cls_data_null = cls_null

        if template_fit:
            self.cls_template = cls_tmp
            apply_template()
        else:
            self.template_cleaned = False

        if subtract_reference_signal:
            self.cls_ref = cls_ref
            self.cls_ref_null = cls_ref_null
            subtract_reference_maps()
        else:
            self.reference_subtracted = False

        if not sim or save_sim:
            return self.save_data(
                save_name, from_attrs=save_attrs, extra_tag="xcorr", data_opts=True
            )

    def get_masked_sims(self, transfer=False, do_noise=True, qb_file=None):
        """
        Compute average signal and noise spectra for a given ensemble of sim
        maps.  The same procedure that is used for computing data cross spectra
        is used for each realization in the sim ensemble, and only the average
        spectra for all realizations are stored.

        See ``get_masked_data`` for more details on how cross spectra are
        computed.

        Arguments
        ---------
        transfer : bool
            If True, use the signal ensemble corresponding to
            `signal_transfer_type`.  Otherwise, uses the standard `signal_type`
            ensemble.
        do_noise : bool
            If True, include noise spectra in the output, as long as
            `noise_type` is supplied.  Otherwise, only the signal component is
            computed.
        qb_file : string
            Pointer to a bandpowers.npz file in the output directory, used
            to correct the ensemble mean noise spectrum by the appropriate
            residual terms.  See ``get_noise_residuals`` for details.

        Notes
        -----
        This method is called at the 'sims' checkpoint, and loads or saves
        a data dictionary with the following keys:

            cls_signal, cls_signal_null:
                Mean signal spectra
            cls_noise, cls_noise_null:
                Mean noise spectra
            cls_sim, cls_sim_null:
                Mean signal+noise spectra
            cls_med, cls_med_null:
                Median signal+noise spectra
            cls_res, cls_res_null:
                NxN, SxN and NxS spectra for computing noise residuals

        For null tests, difference spectra of the two null halves are stored
        in the corresponding `*_null` keys, and summed spectra are stored
        in the normal keys.  Note that these differ from standard non-null
        spectra by a factor of 2.
        """
        import healpy as hp

        mask_files = self.mask_files
        map_tags = self.map_tags
        map_pairs = pt.tag_pairs(map_tags, index=True)
        num_maps = self.num_maps
        num_corr = self.num_corr
        data_shape = self.data_shape
        null_run = self.null_run

        if transfer:
            signal_files = self.signal_transfer_files
            signal_files2 = self.signal_transfer_files2 if null_run else None
            num_signal = self.num_signal_transfer
        else:
            signal_files = self.signal_files
            signal_files2 = self.signal_files2 if null_run else None
            num_signal = self.num_signal

        noise_files = self.noise_files
        noise_files2 = self.noise_files2 if null_run else None
        num_noise = self.num_noise

        if do_noise:
            do_noise = noise_files is not None

        save_attrs = [
            "cls_signal",
            "cls_noise",
            "cls_sim",
            "cls_med",
            "cls_res",
        ]
        if null_run:
            save_attrs += [
                "cls_signal_null",
                "cls_noise_null",
                "cls_sim_null",
                "cls_med_null",
                "cls_res_null",
            ]

        if transfer:
            save_name = "sims_xcorr_{}".format(self.signal_transfer_type)
            cp = "sims_transfer"
        else:
            save_name = "sims_xcorr_{}".format(self.signal_type)
            cp = "sims"

        ret = self.load_data(
            save_name,
            cp,
            fields=save_attrs,
            to_attrs=True,
            shape=data_shape,
            shape_ref="cls_signal",
        )
        if ret is not None:
            if do_noise:
                if self.cls_noise is not None:
                    return ret
            else:
                if self.cls_noise is not None:
                    self.cls_noise = None
                    self.cls_noise_null = None
                    ret["cls_noise"] = None
                    ret["cls_noise_null"] = None
                return ret
        else:
            # don't rerun sims a second time if they've already been run once
            # with the same signal type
            if transfer and self.signal_transfer_type == self.signal_type:
                self.force_rerun["sims"] = False

        # process signal, noise, and S+N
        cls_sig = OrderedDict()
        cls_null_sig = OrderedDict() if null_run else None
        cls_noise = OrderedDict() if do_noise else None
        cls_null_noise = OrderedDict() if null_run and do_noise else None
        cls_tot = OrderedDict()
        cls_null_tot = OrderedDict() if null_run else None
        cls_med = OrderedDict()
        cls_null_med = OrderedDict() if null_run else None

        ### Noise iteration from res fit fields
        cls_res = OrderedDict() if do_noise else None
        cls_null_res = OrderedDict() if null_run and do_noise else None
        if do_noise:
            for k in ["nxn0", "nxn1", "sxn0", "sxn1", "nxs0", "nxs1"]:
                cls_res[k] = OrderedDict()
                if null_run:
                    cls_null_res[k] = OrderedDict()

        if num_noise != 0:
            nsim_min = min([num_signal, num_noise])
        else:
            nsim_min = num_signal
        nsim_max = max([num_signal, num_noise])
        cls_all = np.zeros(
            [nsim_max, len(map_pairs.items()), len(self.specs), self.lmax + 1]
        )
        if null_run:
            cls_all_null = np.zeros_like(cls_all)

        rls = OrderedDict()
        if do_noise and qb_file is not None:
            rls = self.get_noise_residuals(qb_file)

        # convenience functions
        def get_map_alms(filename, mask, rls=None):
            m = self.get_map(filename)
            self.apply_mask(m, mask)
            m_alms = self.map2alm(m, self.pol)

            if rls is not None:
                for s, spec in enumerate(self.specs):
                    if spec in rls:
                        m_alms[s] += hp.almxfl(m_alms[s], rls[spec])

            return m_alms

        def process_index(files, files2, idx, idx2=None, cache=None, rls=None):
            """
            Compute alms of masked input map
            """
            if cache is None:
                cache = {}

            if idx in cache:
                return cache[idx]

            filename = files[idx]
            if idx2 is None:
                self.log(
                    "Computing Alms for map {}/{}".format(idx + 1, num_maps), "all"
                )
            else:
                self.log(
                    "Computing Alms for sim {} of map {}/{}".format(
                        idx2, idx + 1, num_maps
                    ),
                    "all",
                )
                filename = filename[idx2]

            if rls is not None:
                rls = rls.get(map_tags[idx], None)

            mask = self.get_mask(mask_files[idx])
            m_alms = get_map_alms(filename, mask, rls=rls)
            mn_alms = None

            if null_run:
                # second null half
                filename2 = files2[idx]
                if idx2 is not None:
                    filename2 = filename2[idx2]

                m2_alms = get_map_alms(filename2, mask, rls=rls)
                m_alms, mn_alms = (m_alms + m2_alms) / 2.0, (m_alms - m2_alms) / 2.0

            cache[idx] = (m_alms, mn_alms)
            return cache[idx]

        sig_cache = dict()
        noise_cache = dict()

        for isim in range(nsim_max):
            sig_cache.clear()
            noise_cache.clear()
            for xind, (xname, (idx, jdx)) in enumerate(map_pairs.items()):
                self.log(
                    "Computing spectra {} for signal{} sim {}".format(
                        xname, "+noise" if do_noise else "", isim
                    ),
                    "debug",
                )
                if isim < num_signal:
                    simap_alms, sinull_alms = process_index(
                        signal_files, signal_files2, idx, isim, sig_cache
                    )
                    sjmap_alms, sjnull_alms = process_index(
                        signal_files, signal_files2, jdx, isim, sig_cache
                    )

                    cls1_sig = self.alm2cl(simap_alms, sjmap_alms)
                    if null_run:
                        cls_null1_sig = self.alm2cl(sinull_alms, sjnull_alms)

                    if isim < nsim_min:
                        cls1t = np.copy(cls1_sig)
                        if null_run:
                            cls_null1t = np.copy(cls_null1_sig)

                if do_noise and isim < num_noise:
                    cls1_res = OrderedDict()
                    if null_run:
                        cls_null1_res = OrderedDict()

                    nimap_alms, ninull_alms = process_index(
                        noise_files,
                        noise_files2,
                        idx,
                        isim,
                        noise_cache,
                        rls=rls,
                    )
                    njmap_alms, njnull_alms = process_index(
                        noise_files,
                        noise_files2,
                        jdx,
                        isim,
                        noise_cache,
                        rls=rls,
                    )

                    # need non-symmetric since will potentially modify these
                    # with different residuals for T, E, B
                    for k, cx, cy, cnx, cny in [
                        ("nxn", nimap_alms, njmap_alms, ninull_alms, njnull_alms),
                        ("sxn", simap_alms, njmap_alms, sinull_alms, njnull_alms),
                        ("nxs", nimap_alms, sjmap_alms, ninull_alms, sjnull_alms),
                    ]:
                        k0, k1 = ["{}{}".format(k, i) for i in range(2)]
                        cls1_res[k0] = self.alm2cl(cx, cy, symmetric=False)
                        cls1_res[k1] = self.alm2cl(cy, cx, symmetric=False)
                        if null_run:
                            cls_null1_res[k0] = self.alm2cl(cnx, cny, symmetric=False)
                            cls_null1_res[k1] = self.alm2cl(cny, cnx, symmetric=False)

                    # construct noise model
                    cls1_noise = (cls1_res["nxn0"] + cls1_res["nxn1"]) / 2.0
                    if null_run:
                        cls_null1_noise = (
                            cls_null1_res["nxn0"] + cls_null1_res["nxn1"]
                        ) / 2.0

                    # construct total model
                    if isim < nsim_min:
                        for k in cls1_res:
                            cls1t += cls1_res[k] / 2.0
                            if null_run:
                                cls_null1t += cls_null1_res[k] / 2.0

                quants = []
                if isim < num_signal:
                    quants += [[cls_sig, cls1_sig]]
                    if null_run:
                        quants += [[cls_null_sig, cls_null1_sig]]

                if do_noise and isim < num_noise:
                    quants += [[cls_noise, cls1_noise]]
                    quants += [[cls_res[k], cls1_res[k]] for k in cls_res]
                    if null_run:
                        quants += [[cls_null_noise, cls_null1_noise]]
                        quants += [
                            [cls_null_res[k], cls_null1_res[k]] for k in cls_null_res
                        ]
                if isim < nsim_min:
                    quants += [[cls_tot, cls1t]]
                    if null_run:
                        quants += [[cls_null_tot, cls_null1t]]

                for s, spec in enumerate(self.specs):
                    # running average
                    for q0, q1 in quants:
                        if spec not in q0:
                            q0[spec] = OrderedDict()
                        if xname not in q0[spec]:
                            q0[spec][xname] = np.zeros_like(q1[s])
                        d = q0[spec][xname]
                        d[:] += (q1[s] - d) / float(isim + 1)  # in-place

                    # matrix form for efficient median
                    if isim < nsim_min:
                        cls_all[isim][xind][s] = cls_tot[spec][xname]
                        if null_run:
                            cls_all_null[isim][xind][s] = cls_null_tot[spec][xname]

        cls_med_arr = np.median(cls_all, axis=0)
        for s, spec in enumerate(self.specs):
            cls_med[spec] = OrderedDict()
            for xind, xname in enumerate(map_pairs):
                cls_med[spec][xname] = cls_med_arr[xind][s]

        if null_run:
            cls_null_med_arr = np.median(cls_all_null, axis=0)
            for s, spec in enumerate(self.specs):
                cls_null_med[spec] = OrderedDict()
                for xind, xname in enumerate(map_pairs):
                    cls_null_med[spec][xname] = cls_null_med_arr[xind][s]

        self.cls_signal = cls_sig
        self.cls_signal_null = cls_null_sig
        self.cls_noise = cls_noise
        self.cls_noise_null = cls_null_noise
        self.cls_sim = cls_tot
        self.cls_sim_null = cls_null_tot
        self.cls_med = cls_med
        self.cls_med_null = cls_null_med
        self.cls_res = cls_res
        self.cls_res_null = cls_null_res

        # save and return
        return self.save_data(save_name, from_attrs=save_attrs)

    def get_masked_template_noise(self):
        """
        Compute all combinations of template noise cross spectra from the ensemble
        of template noise sims.

        Returns
        -------
        cls_template_noise : OrderedDict
            Dictionary of template noise spectra averaged over all sims, containing
            the following keys: ["temp1:temp1", "temp2:temp2", "temp1:temp2"].  Each entry
            has the same shape structure as the ``cls_data`` attribute.

        Notes
        -----
        This method is called at the 'template_noise' checkpoint, and loads or
        saves a data dictionary containing the ``cls_template_noise`` attribute
        that is returned.
        """
        template_type = self.template_noise_type
        mask_files = self.mask_files
        map_files = [self.template_noise_files, self.template_noise_files2]
        map_tags = self.map_tags
        map_pairs = pt.tag_pairs(map_tags, index=True)
        num_maps = self.num_maps
        data_shape = (3 * self.data_shape[0], self.data_shape[1])

        save_attrs = ["cls_template_noise"]
        save_name = "template_noise_{}".format(template_type)

        ret = self.load_data(
            save_name,
            "template_noise",
            fields=save_attrs,
            to_attrs=True,
            shape=data_shape,
            shape_ref="cls_template_noise",
        )
        if ret is not None:
            return ret["cls_template_noise"]

        cls_template_noise = OrderedDict()
        for k in ["temp1:temp1", "temp2:temp2", "temp1:temp2"]:
            cls_template_noise[k] = OrderedDict()

        cache = dict()

        # convenience functions
        def process_index(idx, isim):
            """
            Compute alms of masked temp1 and temp2 template noise sims
            """
            if idx in cache:
                return cache[idx]

            self.log(
                "Computing Alms for sim {} of map {}/{}".format(
                    isim, idx + 1, num_maps
                ),
                "all",
            )

            mask = self.get_mask(mask_files[idx])

            m_alms = []
            for f in [x[idx][isim] for x in map_files]:
                m = self.get_map(f)
                self.apply_mask(m, mask)
                m_alms.append(self.map2alm(m, self.pol))

            cache[idx] = m_alms
            return cache[idx]

        for isim in range(self.num_template_noise):
            cache.clear()
            for xname, (idx, jdx) in map_pairs.items():
                self.log(
                    "Computing spectra {} for template noise sim {}".format(
                        xname, isim
                    ),
                    "debug",
                )
                temp1i, temp2i = process_index(idx, isim)
                temp1j, temp2j = process_index(jdx, isim)

                cls1 = OrderedDict()
                cls1["temp1:temp1"] = self.alm2cl(temp1i, temp1j)
                cls1["temp2:temp2"] = self.alm2cl(temp2i, temp2j)
                cls1["temp1:temp2"] = 0.5 * (
                    self.alm2cl(temp1i, temp2j) + self.alm2cl(temp2i, temp1j)
                )

                for q0, q1 in [(cls_template_noise[k], cls1[k]) for k in cls1]:
                    for s, spec in enumerate(self.specs):
                        dd = q0.setdefault(spec, OrderedDict())
                        if xname not in dd:
                            dd[xname] = np.zeros_like(q1[s])
                        d = dd[xname]
                        d[:] += (q1[s] - d) / float(isim + 1)  # in-place

        self.cls_template_noise = cls_template_noise
        self.save_data(save_name, from_attrs=save_attrs)
        return cls_template_noise

    def get_kernels(self, window_lmax=None):
        """
        Compute kernels using the mask cross-spectra.  This follows
        the polspice azimuthal approximation for the kernel computation.

        Arguments
        ---------
        window_lmax : int
            The window within which the kernel is computed about
            each ell bin.

        Notes
        -----
        This method is called at the 'kernels' checkpoint and loads or saves
        the following data keys to disk:

            kern:
                temperature kernel
            pkern, mkern:
                + and - polarization kernels
            xkern:
                temperature/polarization cross term kernel
        """

        if window_lmax is None:
            window_lmax = self.lmax

        save_name = "kernels"
        save_attrs = ["kern", "pkern", "mkern", "xkern", "window_lmax"]
        ret = self.load_data(
            save_name,
            "kernels",
            fields=save_attrs,
            to_attrs=True,
            shape=self.kern_shape,
            shape_ref="kern",
            value_ref={"window_lmax": window_lmax},
        )
        if ret is not None:
            return ret

        kern = OrderedDict()
        if self.pol:
            pkern = OrderedDict()
            mkern = OrderedDict()
            xkern = OrderedDict()
        else:
            pkern = None
            mkern = None
            xkern = None

        lmax = self.lmax
        pol = self.pol
        wls = self.wls

        all_ells = np.arange(2 * lmax + 1)
        for xname in self.map_pairs:
            kern[xname] = np.zeros((lmax + 1, 2 * lmax + 1))
            if pol:
                pkern[xname] = np.zeros((lmax + 1, 2 * lmax + 1))
                mkern[xname] = np.zeros((lmax + 1, 2 * lmax + 1))
                xkern[xname] = np.zeros((lmax + 1, 2 * lmax + 1))

        for l in all_ells[2 : lmax + 1]:
            if np.mod(l, 50) == 0:
                self.log("Computing kernels for ell {}/{}".format(l, lmax), "debug")
            l2 = np.min([2 * lmax + 1, l + lmax + 1])
            # populate upper triangle
            for ll in all_ells[l:l2]:
                j0, j0_lmin, j0_lmax = st.wigner3j(l, 0, ll, 0)
                if pol:
                    j2, j2_lmin, j2_lmax = st.wigner3j(l, 2, ll, -2)

                # only go up to window lmax
                j0_lmax = np.minimum(j0_lmax, window_lmax)

                # computed as in https://arxiv.org/abs/1909.09375
                # equations 128 - 136
                l3 = np.arange(j0_lmin, j0_lmax + 1)
                dl3 = 2.0 * l3 + 1.0
                vk = j0[l3] ** 2 * dl3
                if pol:
                    sign = ((-1.0) ** (l + ll + l3)).astype(int)
                    v = j2[l3] ** 2 * dl3
                    vp = v * (1.0 + sign) / 2.0
                    vm = v * (1.0 - sign) / 2.0
                    vx = j2[l3] * j0[l3] * dl3
                for xname in self.map_pairs:
                    wls1 = wls[xname][:, l3]
                    kern[xname][l, ll] += (vk * wls1[0]).sum(axis=-1)
                    if pol:
                        pkern[xname][l, ll] += (vp * wls1[1]).sum(axis=-1)
                        mkern[xname][l, ll] += (vm * wls1[1]).sum(axis=-1)
                        xkern[xname][l, ll] += (vx * wls1[2]).sum(axis=-1)

        # apply symmetry relation
        for l in all_ells[2 : lmax + 1]:
            ll = np.arange(2 * lmax + 1)
            dll = (2.0 * ll + 1.0) / 4.0 / np.pi
            sll = slice(l, lmax + 1)
            for xname in self.map_pairs:
                # populate lower triangle (wigners are symmetric in l and ll)
                kern[xname][sll, l] = kern[xname][l, sll]
                if pol:
                    pkern[xname][sll, l] = pkern[xname][l, sll]
                    mkern[xname][sll, l] = mkern[xname][l, sll]
                    xkern[xname][sll, l] = xkern[xname][l, sll]
                # apply ell scaling along the axis that we bin over
                kern[xname][l, :] *= dll
                if pol:
                    pkern[xname][l, :] *= dll
                    mkern[xname][l, :] *= dll
                    xkern[xname][l, :] *= dll

        # save and return
        self.kern = kern
        self.pkern = pkern
        self.mkern = mkern
        self.xkern = xkern
        self.window_lmax = window_lmax

        return self.save_data(save_name, from_attrs=save_attrs)

    def get_signal_shape(
        self,
        filename=None,
        r=None,
        component=None,
        flat=None,
        signal_mask=None,
        transfer=False,
        save=True,
    ):
        """
        Load a shape spectrum for input to the Fisher iteration algorithm.

        If the spectrum is used as input to ``get_transfer``, it must match
        the spectrum used to generate the simulations, in order to compute
        the correct transfer function.

        Alternatively, the spectrum can be computed using CAMB for arbitrary
        values of ``r``, typically used to compute the ``r`` likelihood once
        the bandpowers have been computed.

        Finally, the spectrum can be flat in ell^2 Cl.  This is typically
        used as the input shape for computing bandpowers for a null test.

        Arguments
        ---------
        filename : string
            Filename for a spectrum on disk.  If None, and ``r`` is None and
            ``flat`` is False, this will search for a spectrum stored in
            ``signal_<signal_type>/spec_signal_<signal_type>.dat``.
            Otherwise, if the filename is a relative path and not found,
            the config directory is searched.
        r : float
            If supplied and ``flat`` is False, a spectrum is computed using
            CAMB for the given ``r`` value.  Overrides ``filename``.
        component : 'scalar', 'tensor', 'fg'
            If 'scalar', and ``r`` is not None, return just the r=0 scalar terms
            in the signal model.  If 'tensor', return just the tensor component
            scaled by the input ``r`` value. If 'fg', return just fg term
        flat : float
            If given, a spectrum that is flat in ell^2 Cl is returned, with
            amplitude given by the supplied value. Overrides all other options.
        signal_mask: str array
            Include only these spectra, others set to zero.
            Options: TT, EE, BB, TE, EB, TB
        transfer : bool
            If True, this is a transfer function run. If ``filename`` is None
            and ``r`` is None and ``flat`` is False, will search for a spectrum
            stored in
            ``signal_<signal_transfer_type>/spec_signal_<signal_transfer_type>.dat``.
        save : bool
            If True, save signal shape dict to disk.

        Returns
        -------
        cls : OrderedDict
            Dictionary keyed by spectrum (cmb_tt, cmb_ee, ... , fg), each
            entry containing a vector of length 2 * lmax + 1
        """

        lmax_kern = 2 * self.lmax

        specs = list(self.specs)
        if transfer:
            if "eb" in specs:
                specs.remove("eb")
            if "tb" in specs:
                specs.remove("tb")
        tbeb = "eb" in specs and "tb" in specs and r is None
        specs = ["cmb_{}".format(spec) for spec in specs]

        if not self.null_run and "fg_tt" in self.bin_def:
            specs.append("fg")

        if component == "fg":
            specs = ["fg"]

        nspecs = len(specs)

        if save:
            shape = (nspecs, lmax_kern + 1)
            save_name = "shape_transfer" if transfer else "shape"

            opts = dict(
                filename=filename,
                r=r,
                flat=flat,
                signal_mask=signal_mask,
            )
            ret = self.load_data(
                save_name, save_name, shape_ref="cls_shape", shape=shape, value_ref=opts
            )
            if ret is not None:
                if r is not None:
                    self.r_model = ret["r_model"]
                self.cls_shape = ret["cls_shape"]
                return ret["cls_shape"]

        ell = np.arange(lmax_kern + 1)
        ellfac = ell * (ell + 1) / 2.0 / np.pi
        cls_shape = OrderedDict()

        if flat is not None and flat is not False:
            if flat is True:
                flat = 2e-5
            # flat spectrum for null tests
            for spec in specs:
                cls_shape[spec] = flat * np.ones_like(ell)

        elif r is not None:
            # cache model components
            if not hasattr(self, "r_model") or self.r_model is None:
                # scalar CAMB spectrum
                scal = st.get_camb_cl(r=0, lmax=lmax_kern)
                # tensor CAMB spectrum for r=1, scales linearly with r
                tens = st.get_camb_cl(r=1, lmax=lmax_kern, nt=0, spec="tensor")
                self.r_model = {"scalar": scal, "tensor": tens}
                if save:
                    opts["r_model"] = self.r_model
            # CAMB spectrum for given r value
            component = str(component).lower()
            if component == "scalar":
                cls_camb = self.r_model["scalar"]
            elif component == "tensor":
                cls_camb = r * self.r_model["tensor"]
            else:
                cls_camb = self.r_model["scalar"] + r * self.r_model["tensor"]
            ns, _ = cls_camb.shape
            for s, spec in enumerate(specs[:ns]):
                cls_shape[spec] = cls_camb[s]

        elif any([x.startswith("cmb") for x in specs]):
            # signal sim model or custom filename
            if filename is None:
                signal_root = (
                    self.signal_transfer_root if transfer else self.signal_root
                )
                filename = "spec_{}.dat".format(os.path.basename(signal_root))
                filename = os.path.join(signal_root, filename)
            elif not os.path.exists(filename):
                filename = os.path.join(self.config_root, filename)
            if not os.path.exists(filename):
                raise OSError("Missing model file {}".format(filename))

            tmp = np.loadtxt(filename, unpack=True)

            ltmp = tmp.shape[-1]
            if lmax_kern + 1 < ltmp:
                ltmp = lmax_kern + 1
            else:
                raise ValueError(
                    "Require at least lmax={} in model file, found {}".format(
                        lmax_kern, ltmp
                    )
                )

            # camb starts at l=2, so set first 2 ells to be 0
            cls_shape["cmb_tt"] = np.append([0, 0], tmp[1, : ltmp - 2])
            if self.pol:
                if np.any(tmp[2, : ltmp - 2] < 0):
                    # this is true if TE is the third index instead of EE
                    # (ell is 0th index for CAMB)
                    self.warn(
                        "Old CAMB format in model file {}. Re-indexing.".format(
                            filename
                        )
                    )
                    pol_specs = [3, 4, 2]
                else:
                    pol_specs = [2, 3, 4]
                for spec, d in zip(specs[1:4], tmp[pol_specs]):
                    cls_shape[spec] = np.append([0, 0], d[: ltmp - 2])

        # EB and TB flat l^2 * C_l
        if self.pol:
            if tbeb and (flat is None or flat is False):
                tbeb_flat = np.abs(cls_shape["cmb_bb"][100]) * ellfac[100] * 1e-4
                tbeb_flat = np.ones_like(cls_shape["cmb_bb"]) * tbeb_flat
                tbeb_flat[:2] = 0
                cls_shape["cmb_eb"] = np.copy(tbeb_flat)
                cls_shape["cmb_tb"] = np.copy(tbeb_flat)
            elif not tbeb and not transfer:
                cls_shape["cmb_eb"] = np.zeros_like(ell, dtype=float)
                cls_shape["cmb_tb"] = np.zeros_like(ell, dtype=float)

        if "fg" in specs:
            # From Planck LIV EE dust
            cls_dust = 34.0 * (ell[2:] / 80.0) ** (-2.28 + 2.0)
            cls_shape["fg"] = np.append([0, 0], cls_dust)
            self.log(
                "Added foreground to cls shape {}".format(list(cls_shape)), "debug"
            )

        # divide out l^2/2pi
        for spec in specs:
            cls_shape[spec][2:] /= ellfac[2:]
            cls_shape[spec][:2] = 0.0

        if signal_mask is not None:
            self.log("Masking {} spectra".format(signal_mask), "debug")
            for csk in cls_shape:
                masked = False
                for smk in signal_mask:
                    if smk.lower() in csk:
                        masked = True
                if not masked:
                    cls_shape[csk] *= 1.0e-12

        if save:
            self.cls_shape = cls_shape
            opts["cls_shape"] = cls_shape
            self.save_data(save_name, **opts)

        return cls_shape

    def get_beams(self, pixwin=True):
        """
        Return beam window functions for all input map tags.

        Arguments
        ---------
        pixwin : bool
            If True, the pixel window function for the map ``nside`` is
            applied to the Gaussian beams.

        Returns
        -------
        beam_windows : dict
            A dictionary of beam window function arrays
            (3 x lmax+1 if pol, 1 x lmax+1 if not) for each map tag

        Notes
        -----
        This method is called at the 'beams' checkpoint and loads or saves
        a dictionary containing just the ``beam_windows`` key to disk.
        """
        import healpy as hp

        lsize = 2 * self.lmax + 1
        nspec = 6 if self.pol else 1
        beam_shape = (self.num_maps * nspec, lsize)

        save_name = "beams"
        cp = "beams"
        ret = self.load_data(
            save_name, cp, fields=["beam_windows"], to_attrs=True, shape=beam_shape
        )
        if ret is not None:
            return ret["beam_windows"]

        beam_windows = OrderedDict()
        beam_windows["tt"] = OrderedDict()
        if self.pol:
            for s in ["ee", "bb", "te", "eb", "tb"]:
                beam_windows[s] = OrderedDict()

        pwl = np.ones((3 if self.pol else 1, lsize))
        if pixwin:
            pwl *= 0.0
            pix = hp.pixwin(self.nside, pol=self.pol)
            if self.pol:
                pixT = pix[0]
                pixP = pix[1]
                end = min(len(pixT), lsize)
                pwl[1, :end] = pixP[:end]
                pwl[2, :end] = np.sqrt(pixT[:end] * pixP[:end])
            else:
                pixT = pix
                end = min(len(pixT), lsize)
            pwl[0, :end] = pixT[:end]

        for tag, otag in zip(self.map_tags, self.map_tags_orig):
            if otag in self.beam_product:
                bl = np.atleast_2d(self.beam_product[otag])[:, :lsize]
            elif otag in self.fwhm:
                bl = hp.gauss_beam(self.fwhm[otag], lsize - 1, self.pol)
                if self.pol:
                    bl = bl.T[[0, 1, 3]]
            else:
                raise ValueError("No beam in config for {}".format(tag))

            blpw = np.atleast_2d(bl * pwl)[:, :lsize]
            beam_windows["tt"][tag] = np.copy(blpw[0])
            if self.pol:
                for s in ["ee", "bb", "eb"]:
                    beam_windows[s][tag] = np.copy(blpw[1])
                for s in ["te", "tb"]:
                    beam_windows[s][tag] = np.copy(blpw[2])

        # save and return
        self.beam_windows = beam_windows
        self.save_data(save_name, from_attrs=["beam_windows"])
        return self.beam_windows

    def get_beam_errors(self):
        """
        Get error envelope to multiply beam by (so, to get beam + 2 sigma error,
        do beam * (1 + 2 * beam_error))
        """

        lsize = 2 * self.lmax + 1
        nspec = 6 if self.pol else 1
        beam_shape = (self.num_maps * nspec, lsize)

        save_name = "beam_errors"
        cp = "beam_errors"

        if hasattr(self, "beam_errors") and not self.force_rerun[cp]:
            return self.beam_errors

        ret = self.load_data(
            save_name, cp, fields=["beam_errors"], to_attrs=True, shape=beam_shape
        )
        if ret is not None:
            return ret["beam_errors"]

        beam_errors = OrderedDict()
        beam_errors["tt"] = OrderedDict()
        if self.pol:
            for s in ["ee", "bb", "te", "eb", "tb"]:
                beam_errors[s] = OrderedDict()

        for tag, otag in zip(self.map_tags, self.map_tags_orig):
            if otag in self.beam_error_product:
                be = self.beam_error_product[otag]
            elif otag in self.fwhm_err:
                # convert error on the FWHM to an envelope error on the beam window
                fwhm = self.fwhm[otag]
                bl = self.beam_windows["tt"][tag]
                blp = hp.gauss_beam(
                    fwhm * (1 - self.fwhm_err[otag]), lsize - 1, self.pol
                )
                blm = hp.gauss_beam(
                    fwhm * (1 + self.fwhm_err[otag]), lsize - 1, self.pol
                )
                if self.pol:
                    bl = np.asarray(
                        [bl, self.beam_windows["ee"][tag], self.beam_windows["te"][tag]]
                    )
                    blp = blp.T[[0, 1, 3]]
                    blm = blm.T[[0, 1, 3]]
                be = (blp - blm) / 2.0 / bl
            else:
                raise ValueError("No beam in config for {}".format(otag))

            be = np.atleast_2d(be)[:, :lsize]
            beam_errors["tt"][tag] = np.copy(be[0])
            if self.pol:
                for s in ["ee", "bb", "eb"]:
                    beam_errors[s][tag] = np.copy(be[1])
                for s in ["te", "tb"]:
                    beam_errors[s][tag] = np.copy(be[2])

        # save and return
        self.beam_errors = beam_errors
        self.save_data(save_name, from_attrs=["beam_errors"])
        return self.beam_errors

    def get_bin_def(
        self,
        lmin=2,
        lmax=500,
        pol=True,
        pol_mask=True,
        tbeb=False,
        bin_width=25,
        weighted_bins=False,
        residual_fit=False,
        res_specs=None,
        bin_width_res=25,
        foreground_fit=False,
        bin_width_fg=25,
    ):
        """
        Construct the bin definition array that defines the bins for each output
        spectrum.

        Arguments
        ---------
        lmin : int
            Minimum ell for binned spectra
        lmax : int
            The maximum multipole for which spectra are computed
        pol : bool
            If True, polarized spectra are computed from the input maps
        pol_mask : bool
            If True, two independent masks are applied to every map:
            one for T maps and one for Q/U maps.
        tbeb : bool
            If True, EB and TB bins are constructed so that these spectra are
            computed by the estimator.  Otherwise, these spectra are fixed at
            zero.
        bin_width : int or list of ints
            Width of each spectrum bin.  If a scalar, the same width is applied
            to all cross spectra.  Otherwise, must be a list of up to six
            elements, listing bin widths for the spectra in the order (TT, EE,
            BB, TE, EB, TB).
        weighted_bins : bool
            If True, use an lfac-weighted binning operator to construct Cbls.
            By default, a flat binning operator is used.
        residual_fit : bool
            If True, fit for (compute bandpower amplitudes for) several wide
            bins of excess noise.
        res_specs : list of strings
            List of spectra which are to be included in the residual fit.  Can
            be individual spectra ('tt', 'ee', 'bb'), or 'eebb' to fit for EE
            and BB residuals simultaneously.  If not supplied, this defaults to
            ['eebb'] for polarized maps, and ['tt'] for unpolarized maps.
        bin_width_res : int or list of ints
            Width of each residual spectrum bin.  If a scalar, the same width
            is applied to all spectra for all cross spectra.  Otherwise, must
            be a list of up to nspec * nmaps elements, listing bin widths for
            each of the spectra in ``res_specs`` in order, then ordered by map.
        foreground_fit : bool
            If True, construct bin definitions for foreground components as
            well.
        bin_width_fg : int or list of ints
            Width of each foreground spectrum bin.  If a scalar, the same width
            is applied to all cross spectra.  Otherwise, must be a list of up to
            six elements, listing bin widths for the spectra in the order (TT,
            EE, BB, TE, EB, TB).

        Returns
        -------
        bin_def : dict
            The bin definition dictionary.  Each key contains a Nx2 array
            that defines the left and right edges for each bin of the
            corresponding spectrum.
        """
        self.lmax = lmax
        self.pol = pol
        self.pol_dim = 3 if self.pol else 1
        self.pol_mask = pol_mask if self.pol else False

        # count all the things
        self.num_corr = pt.num_corr(self.num_maps)
        self.num_spec = pt.num_corr(3 if self.pol else 1)
        self.num_spec_mask = pt.num_corr(2 if self.pol_mask else 1)
        self.num_kern = self.num_corr
        self.data_shape = (self.num_corr * self.num_spec, self.lmax + 1)
        self.mask_shape = (self.num_corr * self.num_spec_mask, self.lmax + 1)
        self.kern_shape = (self.num_kern * (self.lmax + 1), 2 * self.lmax + 1)

        npol = (6 if tbeb else 4) if self.pol else 1
        specs = ["tt", "ee", "bb", "te", "eb", "tb"][:npol]

        if np.isscalar(bin_width) or len(bin_width) == 1:
            bin_width = [bin_width] * npol
        bin_width = np.asarray(bin_width).ravel()[:npol]

        bwerr = "EE and BB must have the same bin width (for mixing)"
        if self.pol and bin_width[1] != bin_width[2]:
            raise ValueError(bwerr)

        # Define bins
        nbins_cmb = 0
        bin_def = OrderedDict()
        for spec, bw in zip(specs, bin_width):
            bins = np.arange(lmin, self.lmax, bw)
            bins = np.append(bins, self.lmax + 1)
            bin_def["cmb_{}".format(spec)] = np.column_stack((bins[:-1], bins[1:]))
            nbins_cmb += len(bins) - 1
        self.log("Added {} CMB bins to bin_def".format(nbins_cmb), "debug")

        # Do the same for foreground bins
        nbins_fg = 0
        if foreground_fit:
            if np.isscalar(bin_width_fg) or len(bin_width_fg) == 1:
                bin_width_fg = [bin_width_fg] * npol
            bin_width_fg = np.asarray(bin_width_fg).ravel()[:npol]

            if self.pol and bin_width_fg[1] != bin_width_fg[2]:
                raise ValueError("Foreground {}".format(bwerr))

            for spec, bw in zip(specs, bin_width_fg):
                bins = np.arange(lmin, self.lmax, bw)
                bins = np.append(bins, self.lmax + 1)
                bin_def["fg_{}".format(spec)] = np.column_stack((bins[:-1], bins[1:]))
                nbins_fg += len(bins) - 1
            bin_def["delta_beta"] = np.array([[0, 0]])
            self.log(
                "Added {} foreground bins to bin_def".format(nbins_fg + 1), "debug"
            )

        # Do the same for residual bins
        nbins_res = 0
        if residual_fit:
            if res_specs is None:
                res_specs = ["eebb"] if self.pol else ["tt"]
            res_specs = [s.lower() for s in res_specs]
            nmap = len(self.map_tags)
            nspecr = len(res_specs) * nmap
            if np.isscalar(bin_width_res) or len(bin_width_res) == 1:
                bin_width_res = [bin_width_res] * nspecr
            elif len(bin_width_res) == nspecr // nmap:
                bin_width_res = np.array([bin_width_res] * nmap).ravel()
            bin_width_res = np.asarray(bin_width_res).ravel()[:nspecr].reshape(nmap, -1)

            for tag, bws in zip(self.map_tags, bin_width_res):
                if self.pol and "ee" in res_specs and "bb" in res_specs:
                    if bws[res_specs.index("ee")] != bws[res_specs.index("bb")]:
                        raise ValueError("Residual {} {}".format(tag, bwerr))

                for spec, bw in zip(res_specs, bws):
                    bins = np.arange(lmin, self.lmax, bw)
                    bins = np.append(bins, self.lmax + 1)
                    btag = "res_{}_{}".format(spec, tag)
                    bin_def[btag] = np.column_stack((bins[:-1], bins[1:]))
                    nbins_res += len(bins) - 1

            self.log("Added {} residual bins to bin_def".format(nbins_res), "debug")

        ell = np.arange(self.lmax + 1)
        lfac = ell * (ell + 1) / 2.0 / np.pi

        bin_weights = OrderedDict()
        for k, bd in bin_def.items():
            bin_weights[k] = []
            for left, right in bd:
                if weighted_bins:
                    w = lfac[left:right] / np.mean(lfac[left:right])
                else:
                    w = 1.0
                bin_weights[k].append(w)

        self.lmin = lmin
        self.nbins_cmb = nbins_cmb
        self.nbins_fg = nbins_fg
        self.nbins_res = nbins_res
        self.bin_def = bin_def
        self.specs = specs
        self.weighted_bins = weighted_bins
        self.bin_weights = bin_weights

        return self.bin_def

    def kernel_precalc(self, map_tag=None, transfer_run=False):
        """
        Compute the mixing kernels M_ll' = K_ll' * F_l' * B_l'^2.  Called by
        ``bin_cl_template`` to pre-compute kernel terms.

        Arguments
        ---------
        map_tag : str
            If supplied, the kernels are computed only for the given map tag
            (or cross if map_tag is map_tag1:map_tag2).
            Otherwise, it is computed for all maps and crosses.
        transfer_run : bool
            If True, set transfer function to 1 to solve for transfer function
            qbs.

        Returns
        -------
        mll : OrderedDict
            Dictionary of M_ll' matrices, keyed by spec and xname.
        """

        map_pairs = None
        if map_tag is not None:
            if map_tag in self.map_pairs:
                map_pairs = {map_tag: self.map_pairs[map_tag]}
                map_tags = list(set(self.map_pairs[map_tag]))
            else:
                map_tags = [map_tag]
                map_pairs = pt.tag_pairs(map_tags)
        else:
            map_tags = self.map_tags
            map_pairs = self.map_pairs

        specs = list(self.specs)
        lmax = self.lmax  # 2 * lmax

        if not transfer_run:
            # collect transfer function terms
            transfer = OrderedDict()
            for spec in specs:
                transfer[spec] = OrderedDict()
                for tag in map_tags:
                    transfer[spec][tag] = self.transfer[spec][tag]

        lk = slice(0, lmax + 1)
        mll = OrderedDict()

        for spec in specs:
            mll[spec] = OrderedDict()
            if spec in ["ee", "bb"]:
                mspec = "{}_mix".format(spec)
                mll[mspec] = OrderedDict()

            for xname, (m0, m1) in map_pairs.items():
                # beams
                fb2 = self.beam_windows[spec][m0][lk] * self.beam_windows[spec][m1][lk]

                # transfer function
                if not transfer_run:
                    fb2 *= np.sqrt(transfer[spec][m0][lk] * transfer[spec][m1][lk])

                # kernels
                if spec == "tt":
                    k = self.kern[xname][:, lk]
                elif spec in ["ee", "bb"]:
                    k = self.pkern[xname][:, lk]
                    mk = self.mkern[xname][:, lk]
                elif spec in ["te", "tb"]:
                    k = self.xkern[xname][:, lk]
                elif spec == "eb":
                    k = self.pkern[xname][:, lk] - self.mkern[xname][:, lk]

                # store final product
                mll[spec][xname] = k * fb2
                if spec in ["ee", "bb"]:
                    mll[mspec][xname] = mk * fb2

        return mll

    def bin_cl_template(
        self,
        cls_shape=None,
        map_tag=None,
        transfer_run=False,
        beam_error=False,
        use_precalc=True,
        fg_ell_ind=0,
    ):
        """
        Compute the Cbl matrix from the input shape spectrum.

        This method requires beam windows, kernels and transfer functions
        (if ``transfer_run`` is False) to have been precomputed.

        Arguments
        ---------
        cls_shape : array_like
            The shape spectrum to use.  This can be computed using
            ``get_signal_shape`` or otherwise.
        map_tag : str
            If supplied, the Cbl is computed only for the given map tag
            (or cross if map_tag is map_tag1:map_tag2).
            Otherwise, it is computed for all maps and crosses.
        transfer_run : bool
            If True, this assumes a unity transfer function for all bins, and
            the output Cbl is used to compute the transfer functions that are
            then loaded when this method is called with ``transfer_run = False``.
        beam_error : bool
            If True, use beam error envelope instead of beam to get cbls that
            are 1 sigma beam error envelope offset of signal terms.
        use_precalc : bool
            If True, load pre-calculated terms stored from a previous iteration,
            and store for a future iteration.  Otherwise, all calculations are
            repeated.
        fg_ell_ind : float
            If binning foreground shape, offset the ell index from the reference
            by this amount.

        Returns
        -------
        cbl : dict of arrays (num_bins, 2, lmax + 1)
            The Cbl matrix, indexed by component and spectrum, then by map
            cross, e.g. ``cbl['cmb_tt']['map1:map2']``.  E/B mixing terms are
            stored in elements ``cbl['cmb_ee_mix']`` and ``cbl['cmb_bb_mix']``,
            and unmixed terms are stored in elements ``cbl['cmb_ee']`` and
            ``cbl['cmb_bb']``.
        """
        if cls_shape is None:
            cls_shape = self.cls_shape

        map_pairs = None
        if map_tag is not None:
            if map_tag in self.map_pairs:
                map_pairs = {map_tag: self.map_pairs[map_tag]}
                map_tags = list(set(self.map_pairs[map_tag]))
            else:
                map_tags = [map_tag]
        else:
            map_tags = self.map_tags

        if map_pairs is None:
            map_pairs = pt.tag_pairs(map_tags)

        specs = list(self.specs)
        if transfer_run:
            if "eb" in specs:
                specs.remove("eb")
            if "tb" in specs:
                specs.remove("tb")

        lmax = self.lmax
        lmax_kern = lmax  # 2 * self.lmax

        if getattr(self, "mll", None) is None or not use_precalc:
            mll = self.kernel_precalc(map_tag=map_tag, transfer_run=transfer_run)
            if use_precalc:
                self.mll = mll
        else:
            mll = self.mll

        if beam_error:
            beam_error = self.get_beam_errors()
            beam_keys = ["b1", "b2", "b3"]

        ls = slice(2, lmax + 1)
        lk = slice(0, lmax_kern + 1)
        cbl = OrderedDict()

        comps = []
        if "cmb_tt" in cls_shape or "cmb_ee" in cls_shape:
            comps += ["cmb"]
        if "fg" in cls_shape and not transfer_run:
            comps += ["fg"]
        if self.nbins_res > 0 and not transfer_run:
            comps += ["res"]
            cls_noise = self.cls_noise_null if self.null_run else self.cls_noise
            cls_res = self.cls_res_null if self.null_run else self.cls_res

        ell = np.arange(lmax_kern + 1)

        def binup(d, left, right, weights):
            return (d[..., left:right] * weights).sum(axis=-1)

        def bin_things(comp, d, md):
            if "res" in comp:
                return
            for si, spec in enumerate(specs):
                stag = "{}_{}".format(comp, spec)
                cbl.setdefault(stag, OrderedDict())
                mstag = None
                if spec in ["ee", "bb"]:
                    mstag = stag + "_mix"
                    cbl.setdefault(mstag, OrderedDict())
                bd = self.bin_def[stag]
                bw = self.bin_weights[stag]
                for xi, (xname, (tag1, tag2)) in enumerate(map_pairs.items()):
                    if beam_error:
                        cbl[stag][xname] = OrderedDict(
                            [(k, np.zeros((len(bd), lmax + 1))) for k in beam_keys]
                        )
                    else:
                        cbl[stag][xname] = np.zeros((len(bd), lmax + 1))
                    if spec in ["ee", "bb"]:
                        if beam_error:
                            cbl[mstag][xname] = OrderedDict(
                                [(k, np.zeros((len(bd), lmax + 1))) for k in beam_keys]
                            )
                        else:
                            cbl[mstag][xname] = np.zeros((len(bd), lmax + 1))

                    # integrate per bin
                    for idx, ((left, right), weights) in enumerate(zip(bd, bw)):
                        if beam_error:
                            for k in beam_keys:
                                cbl[stag][xname][k][idx, ls] = binup(
                                    d[k][si, xi], left, right, weights
                                )
                        else:
                            cbl[stag][xname][idx, ls] = binup(
                                d[si, xi], left, right, weights
                            )
                        if spec in ["ee", "bb"]:
                            if beam_error:
                                for k in beam_keys:
                                    cbl[mstag][xname][k][idx, ls] = binup(
                                        md[k][si - 1, xi], left, right, weights
                                    )
                            else:
                                cbl[mstag][xname][idx, ls] = binup(
                                    md[si - 1, xi], left, right, weights
                                )

        for comp in comps:
            # convert to matrices to do multiplication to speed things up,
            # except for res is weird so don't do it for that.
            # need n_xname x n_spec x ell
            nspec = len(specs)
            nxmap = len(map_pairs)
            if comp == "fg" and fg_ell_ind != 0:
                s_arr = (ell / 80.0) ** fg_ell_ind
                s_arr[0] = 0
                if not beam_error:
                    # don't create a new object in memory each time
                    # use last one's space to save runtime
                    self.d = np.multiply(self.d_fg, s_arr, out=getattr(self, "d", None))
                    self.md = np.multiply(
                        self.md_fg, s_arr, out=getattr(self, "md", None)
                    )
                else:
                    for k in beam_keys:
                        if not hasattr(self, "d"):
                            self.d = OrderedDict([(k, None) for k in beam_keys])
                            self.md = OrderedDict([(k, None) for k in beam_keys])
                        self.d[k] = np.multiply(self.d_fg[k], s_arr, out=self.d[k])
                        self.md[k] = np.multiply(self.md_fg[k], s_arr, out=self.md[k])
                bin_things(comp, self.d, self.md)

            else:
                kshape = [nspec, nxmap, self.lmax - 1, lmax_kern + 1]
                mkshape = [2] + kshape[1:]
                k_arr = np.zeros(kshape)
                mk_arr = np.zeros(mkshape)

                shape = [nspec, nxmap, 1, lmax_kern + 1]
                s_arr = np.zeros(shape)
                if beam_error:
                    b_arr = {k: np.zeros(shape) for k in beam_keys}

                for si, spec in enumerate(specs):
                    stag = "{}_{}".format(comp, spec)
                    mstag = None
                    if comp != "res" and spec in ["ee", "bb"]:
                        mstag = stag + "_mix"

                    for xi, (xname, (tag1, tag2)) in enumerate(map_pairs.items()):
                        if "res" in comp:
                            s0, s1 = spec
                            bd = [[0, lmax + 1]]
                            # if any component of XY spec is in residual bin
                            # def, use that bin def
                            for v in [
                                "res_{}_{}".format(s0 * 2, tag1),
                                "res_{}_{}".format(s0 * 2, tag2),
                                "res_{}_{}".format(s1 * 2, tag1),
                                "res_{}_{}".format(s1 * 2, tag2),
                            ]:
                                if v in self.bin_def:
                                    bd = self.bin_def[v]
                                    break
                                spec0 = v.split("_")[1]
                                if spec0 in ["ee", "bb"]:
                                    v = v.replace(spec0, "eebb")
                                    if v in self.bin_def:
                                        bd = self.bin_def[v]
                                        break

                            comp_list = [("res", cls_noise)] + [
                                ("res_{}".format(k), cls_res[k]) for k in cls_res
                            ]

                            for res_comp, cls in comp_list:
                                stag = "{}_{}".format(res_comp, spec)
                                cbl.setdefault(stag, OrderedDict())
                                cbl[stag][xname] = np.zeros((len(bd), lmax + 1))
                                cl1 = cls[spec][xname]
                                for idx, (left, right) in enumerate(bd):
                                    lls = slice(left, right)
                                    cbl[stag][xname][idx, lls] = np.copy(cl1[lls])

                            continue

                        if beam_error:
                            b_arr["b1"][si, xi] = beam_error[spec][tag1]
                            b_arr["b2"][si, xi] = beam_error[spec][tag2]
                            b_arr["b3"][si, xi] = (
                                b_arr["b1"][si, xi] * b_arr["b2"][si, xi]
                            )

                        # use correct shape spectrum
                        if comp == "fg":
                            # single foreground spectrum
                            s_arr = cls_shape["fg"][lk] * (ell / 80.0) ** fg_ell_ind
                            s_arr[0] = 0
                        else:
                            s_arr[si, xi] = cls_shape["cmb_{}".format(spec)][lk]

                        # get cross spectrum kernel terms
                        k_arr[si, xi] = mll[spec][xname][ls, lk]
                        if spec in ["ee", "bb"]:
                            mspec = spec + "_mix"
                            mk_arr[si - 1, xi] = mll[mspec][xname][ls, lk]

                md = None
                if s_arr.ndim == 1:
                    s_arr_md = s_arr
                else:
                    s_arr_md = s_arr[1:3]
                d = k_arr * s_arr
                if self.pol:
                    md = mk_arr * s_arr_md
                if beam_error:
                    d = OrderedDict([(k, d * b_arr[k]) for k in beam_keys])
                    if self.pol:
                        md = OrderedDict([(k, md * b_arr[k]) for k in beam_keys])
                if comp == "fg":
                    self.d_fg = copy.deepcopy(d)
                    self.md_fg = copy.deepcopy(md)

                bin_things(comp, d, md)

        return cbl

    def get_model_spectra(
        self, qb, cbl, delta=True, res=True, cls_noise=None, cond_noise=None
    ):
        """
        Compute unbinned model spectra from qb amplitudes and a Cbl matrix.
        Requires pre-loaded bin definitions using ``get_bin_def`` or
        ``get_transfer``.

        This method is used internally by ``fisher_calc``.

        Arguments
        ---------
        qb : dict of arrays
            Array of bandpowers for every spectrum bin.
        cbl : dict
            Cbl dict as computed by ``bin_cl_template``.
        delta : bool
            If True, evaluate the foreground model at the spectral
            index offset by qb['delta_beta']
        res : bool
            If True, include the residual noise model terms.
        cls_noise : OrderedDict
            If supplied, the noise spectrum is applied to the model spectrum.
        cond_noise : float
            Conditioning noise amplitude to add to TT, EE and BB autospectra,
            to improve convergence of the fisher iterations.  The noise model
            is constant cond_noise for EE, BB and 10x that for TT.

        Returns
        -------
        cls : dict of arrays
            Model spectra.  Keyed by spectrum type, e.g. 'total_xx' for the
            total model spectrom, 'fg_xx' for the foreground terms, 'res_xx' for
            the residual (noise) terms, where 'xx' is one of the six power
            spectrum components (tt, ee, bb, te, eb, tb).  Each entry in the
            dictionary is itself a dictionary keyed by map cross, e.g.
            'map1:map1' for an autospectrum term, 'map1:map2' for a cross
            spectrum, etc, and the map names are the same as those in the
            ``map_tags`` attribute.  Each individual spectrum is an array of
            length ``lmax + 1``.
        """
        comps = []

        if any([k.startswith("cmb_") for k in qb]):
            comps = ["cmb"]

        delta_beta = 0.0
        if "delta_beta" in qb:
            # Evaluate fg at spectral index pivot for derivative
            # in Fisher matrix, unless delta is True
            if delta:
                delta_beta = qb["delta_beta"][0]
            comps += ["fg"]

        if res and any([k.startswith("res_") for k in qb]):
            comps += ["res"]

        if cls_noise is not None:
            comps += ["noise"]

        if cond_noise is not None:
            cls_cond = np.ones(self.lmax + 1) * cond_noise
            cls_cond[:2] = 0
            comps += ["cond"]

        if not len(comps):
            raise ValueError("Must specify at least one model component")

        cls = OrderedDict()

        specs = []
        for spec in self.specs:
            if "cmb_{}".format(spec) in cbl:
                # Don't add entries that won't be filled in later
                cls["total_{}".format(spec)] = OrderedDict()
            elif "fg_{}".format(spec) in cbl:
                cls["total_{}".format(spec)] = OrderedDict()
            specs.append(spec)

        for comp in comps:
            for spec in specs:
                stag = "{}_{}".format(comp, spec)
                if spec in ["ee", "bb"]:
                    mstag = "{}_{}".format(comp, "bb" if spec == "ee" else "ee")

                if comp == "noise":
                    pairs = list(cls_noise[spec])
                elif comp == "cond":
                    pairs = list(cls["total_{}".format(spec)])
                else:
                    if "res" not in comp and stag not in qb:
                        continue
                    if "res" not in comp and stag not in cbl:
                        continue
                    pairs = list(self.map_pairs)  # list(cbl[stag])

                for xname in pairs:
                    tag1, tag2 = self.map_pairs[xname]

                    # extract qb's for the component spectrum
                    if comp == "cmb":
                        qbs = qb[stag]
                        if spec in ["ee", "bb"]:
                            qbm = qb[mstag]

                    elif comp == "fg":
                        # frequency scaling for foreground model
                        # I don't remember why delta beta was done this way.
                        # For likelihood, it makes sense to just use beta_ref+db
                        freq_scale = st.scale_dust(
                            self.map_freqs[tag1],
                            self.map_freqs[tag2],
                            ref_freq=self.ref_freq,
                            beta=self.beta_ref,
                            delta_beta=delta_beta,
                        )
                        qbs = freq_scale * qb[stag]
                        if spec in ["ee", "bb"]:
                            qbm = freq_scale * qb[mstag]

                    elif comp == "res":
                        # modify model by previously fit res, including
                        # off diagonals and SXN/NXS for nulls
                        s0, s1 = spec  # separate qbs for, eg, TE resTT and resEE
                        res_tags = {
                            "s0m0": "res_{}_{}".format(s0 * 2, tag1),
                            "s0m1": "res_{}_{}".format(s0 * 2, tag2),
                            "s1m0": "res_{}_{}".format(s1 * 2, tag1),
                            "s1m1": "res_{}_{}".format(s1 * 2, tag2),
                        }
                        qbr = {"s0m0": 1, "s0m1": 1, "s1m0": 1, "s1m1": 1}

                        for k, v in res_tags.items():
                            if v in qb:
                                qbr[k] = np.sqrt(1 + qb[v])[:, None]
                            else:
                                spec0 = v.split("_")[1]
                                if spec0 in ["ee", "bb"]:
                                    res_tags[k] = v.replace(spec0, "eebb")
                                    if res_tags[k] in qb:
                                        qbr[k] = np.sqrt(1 + qb[res_tags[k]])[:, None]

                            if np.any(np.isnan(qbr[k])):
                                self.warn(
                                    "Unphysical residuals fit, "
                                    "setting to zero {} bins {}".format(
                                        spec, np.where(np.isnan(qbr[k]))
                                    )
                                )
                                qbr[k][np.isnan(qbr[k])] = 1

                        res_list = [
                            ("s0m0", "s1m1", "nxn0"),  # N_s0m0 x N_s1m1
                            ("s1m0", "s0m1", "nxn1"),  # N_s1m0 x N_s0m1
                        ]
                        if self.null_run:
                            res_list += [
                                (None, "s1m1", "sxn0"),  # S_s0m0 x N_s1m1
                                (None, "s0m1", "sxn1"),  # S_s1m0 x N_s0m1
                                ("s0m0", None, "nxs0"),  # N_s0m0 x S_s1m1
                                ("s1m0", None, "nxs1"),  # N_s1m0 x S_s0m1
                            ]

                        cl1 = np.zeros_like(cl1)
                        for k1, k2, k3 in res_list:
                            q1 = qbr[k1] if k1 else 1.0
                            q2 = qbr[k2] if k2 else 1.0
                            rtag = "res_{}_{}".format(k3, spec)
                            cl1 += ((q1 * q2 - 1.0) * cbl[rtag][xname]).sum(axis=0)

                        # all of these were asymmetric specs, divide by 2 for mean
                        cl1 /= 2.0

                    # compute model spectra
                    if comp in ["cmb", "fg"]:
                        if xname not in cbl[stag]:
                            continue
                        cbl1 = cbl[stag][xname]
                        if isinstance(cbl1, dict):
                            # has beam error terms. deal with them individually
                            cl1 = OrderedDict()
                            for k, v in cbl1.items():
                                cl1[k] = (qbs[:, None] * v).sum(axis=0)
                        else:
                            cl1 = (qbs[:, None] * cbl1).sum(axis=0)
                        if spec in ["ee", "bb"]:
                            # mixing terms, add in-place
                            if qbm is not None and mstag + "_mix" in cbl:
                                cbl1_mix = cbl[mstag + "_mix"][xname]
                                if isinstance(cbl1_mix, dict):
                                    for k, v in cbl1_mix.items():
                                        cl1[k] += (qbm[:, None] * v).sum(axis=0)
                                else:
                                    cl1 += (qbm[:, None] * cbl1_mix).sum(axis=0)

                    elif comp == "noise":
                        cl1 = cls_noise[spec][xname][: self.lmax + 1]

                    elif comp == "cond":
                        # add conditioner along diagonal
                        if tag1 != tag2:
                            continue
                        if spec == "tt":
                            cl1 = 10 * cls_cond
                        elif spec in ["ee", "bb"]:
                            cl1 = cls_cond
                        else:
                            continue

                    # store
                    cls.setdefault(stag, OrderedDict())[xname] = cl1

                    # add to total model
                    if not isinstance(cl1, dict):
                        ttag = "total_{}".format(spec)
                        cls[ttag].setdefault(xname, np.zeros_like(cl1))
                        cls[ttag][xname] += cl1

        return cls

    def get_data_spectra(self, map_tag=None, transfer_run=False, do_noise=True):
        """
        Return data and noise spectra for the given map tag(s).  Data spectra
        and signal/noise sim spectra must have been precomputed or loaded from
        disk.

        Arguments
        ---------
        map_tag : str
            If None, all map-map cross-spectra are included in the outputs.
            Otherwise, only the autospectra of the given map are included.
        transfer_run : bool
            If True, the data cls are the average of the signal simulations, and
            noise cls are ignored.  If False, the data cls are either
            ``cls_data_null`` (for null tests) or ``cls_data``.  See
            ``get_masked_data`` for how these are computed.  The input noise is
            similarly either ``cls_noise_null`` or ``cls_noise``.
        do_noise : bool
            If True, return noise spectra along with data.

        Returns
        -------
        obs : OrderedDict
            Dictionary of data cross spectra
        nell : OrderedDict
            Dictionary of noise cross spectra, or None if transfer_run is True.
        """
        # select map pairs
        if map_tag is not None:
            map_tags = [map_tag]
        else:
            map_tags = self.map_tags
        map_pairs = pt.tag_pairs(map_tags)

        # select spectra
        tbeb = "cmb_tb" in self.bin_def
        if transfer_run or not tbeb:
            specs = self.specs[:4]
        else:
            specs = self.specs

        # obs depends on what you're computing
        if transfer_run:
            obs_quant = self.cls_signal
        elif self.null_run:
            if self.reference_subtracted:
                obs_quant = self.cls_data_sub_null
            else:
                obs_quant = self.cls_data_null
        elif self.template_cleaned:
            obs_quant = self.cls_data_clean
        else:
            obs_quant = self.cls_data

        # in case we're excluding some spectra or maps, repopulate obs dict
        obs = OrderedDict()
        for spec in specs:
            obs[spec] = OrderedDict()
            for xname in map_pairs:
                obs[spec][xname] = obs_quant[spec][xname]

        if not do_noise:
            return obs

        nell = None
        debias = None
        # Nulls are debiased by average of S+N sims
        if self.null_run and not transfer_run and self.cls_sim_null is not None:
            nell = OrderedDict()
            debias = OrderedDict()
            for spec in specs:
                nell[spec] = OrderedDict()
                debias[spec] = OrderedDict()
                for xname in map_pairs:
                    nell[spec][xname] = np.copy(self.cls_sim_null[spec][xname])
                    if self.reference_subtracted:
                        # signal term already subtracted with reference maps
                        debias[spec][xname] = np.copy(self.cls_noise_null[spec][xname])
                    else:
                        debias[spec][xname] = np.copy(self.cls_sim_null[spec][xname])

        # Non-nulls are debiased by average of N sims
        elif not transfer_run and self.cls_noise is not None:
            nell = OrderedDict()
            debias = OrderedDict()
            for spec in specs:
                nell[spec] = OrderedDict()
                debias[spec] = OrderedDict()
                for xname, (m0, m1) in map_pairs.items():
                    if m0 != m1:
                        # set non-auto noise to 0-- don't care to fit cross
                        # spectrum noise
                        nell[spec][xname] = np.zeros_like(self.cls_noise[spec][xname])
                    else:
                        nell[spec][xname] = np.copy(self.cls_noise[spec][xname])
                    debias[spec][xname] = np.copy(nell[spec][xname])

        return obs, nell, debias

    def do_qb2cb(self, qb, inv_fish, wbl):
        """
        Compute binned output spectra and covariances by averaging
        the shape spectrum over each bin, and applying the appropriate
        ``qb`` bandpower amplitude.

        This method is used internally by ``fisher_calc``, and requires
        bin definitions to have been pre-loaded using ``get_bin_def``
        or ``get_transfer``.

        Arguments
        ---------
        qb : dict
            Bandpower amplitudes for each spectrum bin.
        inv_fish : array_like, (nbins, nbins)
            Inverse fisher matrix for computing the bin errors and covariance.
        wbl : dict
            Window functions for each qb

        Returns
        -------
        cb : dict of arrays
            Binned spectrum
        dcb : dict of arrays
            Binned spectrum error
        ellb : dict of arrays
            Average bin center
        cov : array_like, (nbins, nbins)
            Binned spectrum covariance
        qb2cb : dict
            The conversion matrix from ``qb`` to ``cb`` for each spectrum
            component, computed from the qb window functions
        wbl_cb : dict
            Window functions for each cb
        """
        qb2cb = OrderedDict()
        ellb = OrderedDict()
        cb = OrderedDict()
        wbl_cb = OrderedDict()

        # truncate to only bins for which there are window functions
        bin_index = pt.dict_to_index(self.bin_def)
        nbins = max([bin_index[stag][1] for stag in wbl])
        inv_fish = inv_fish[:nbins, :nbins]
        qb2cb_mat = np.zeros_like(inv_fish)

        # window function normalization
        ell = np.arange(self.lmax + 1)
        norm = (2.0 * ell + 1.0) / 4.0 / np.pi

        # normalization shape spectrum
        model = 1
        if not self.return_cls:
            model = np.ones_like(ell, dtype=float)
            model[1:] = 2.0 * np.pi / ell[1:] / (ell[1:] + 1)

        for stag, wbl1 in wbl.items():
            # compute conversion factors
            qb2cb[stag] = np.zeros((len(wbl1), len(wbl1)))
            cls_shape = self.cls_shape["fg" if "fg" in stag else stag][: len(ell)]
            v = norm * wbl1 * cls_shape
            bd = self.bin_def[stag]
            bw = self.bin_weights[stag]
            for idx, ((l, r), w) in enumerate(zip(bd, bw)):
                qb2cb[stag][:, idx] = np.sum(v[..., l:r] * w, axis=-1)

            # normalize for cb's or db's
            qb2cb[stag] /= np.sum(norm * wbl1 * model, axis=-1)

            # construct conversion matrix for covariance
            left, right = bin_index[stag]
            qb2cb_mat[left:right, left:right] = qb2cb[stag]

            # compute wbls for cb's
            wbl_cb[stag] = np.einsum("ij,jl->il", qb2cb[stag], wbl1)

            # check normalization
            self.log(
                "{} cb window function normalization: {}".format(
                    stag, np.sum(norm * wbl_cb[stag] * model, axis=-1)
                ),
                "debug",
            )

            # compute bin centers
            ellb[stag] = np.sum(norm * wbl_cb[stag] * model * ell, axis=-1)

            # compute cb's
            cb[stag] = np.einsum("ij,j->i", qb2cb[stag], qb[stag])

        # compute covariance and errors
        cov = np.einsum("ik,jl,kl->ij", qb2cb_mat, qb2cb_mat, inv_fish)
        dcb = pt.arr_to_dict(np.sqrt(np.diag(cov)), qb2cb)

        return cb, dcb, ellb, cov, qb2cb, wbl_cb

    def fisher_precalc(
        self,
        cbl,
        cls_input,
        cls_debias=None,
        likelihood=False,
        windows=False,
    ):
        """
        Pre-compute the D_ell and signal derivative matrices necessary for
        ``fisher_calc`` from the input data spectra.  This method requires bin
        definitions precomputed by ``get_bin_def`` or ``get_transfer``.

        Arguments
        ---------
        cbl : OrderedDict
            Cbl dict computed by ``bin_cl_template`` for a given
            shape spectrum.
        cls_input : OrderedDict
            Input spectra.  If computing a transfer function, this is the
            average ``cls_signal``.  If computing a null test, this is
            ``cls_data_null``, and otherwise it is ``cls_data``, for a single map or
            several input maps.
        cls_debias : OrderedDict
            If supplied, the debias spectrum is subtracted from the input.
        likelihood : bool
            If True, compute just Dmat_obs_b.  Otherwise, Dmat_obs and
            dSdqb_mat1 are also computed.
        windows : bool
            If True, compute dSdqb and Mll for constructing window functions.

        Returns
        -------
        Dmat_obs : OrderedDict
            De-biased D_ell matrix from ``cls_input``
        Dmat_obs_b : OrderedDict
            Biased D_ell matrix from ``cls_input`` (for likelihood)
        dSdqb_mat1 : OrderedDict
            Signal derivative matrix from Cbl
        Mmat : OrderedDict
            Mode mixing matrix (Kll' * Fl * Bl^2) for constructing
            window functions.
        Mmat_mix : OrderedDict
            EB mixing terms for the mode mixxing matrix

        .. note:: the output arrays are also stored as attributes of the
        parent object to avoid repeating the computation in ``fisher_calc``
        """
        num_maps = self.num_maps
        pol_dim = 3 if self.pol else 1
        dim1 = pol_dim * num_maps

        comps = ["cmb"]
        if "fg_tt" in cbl:
            comps += ["fg"]
        if "res_tt" in cbl or "res_ee" in cbl:
            comps += ["res"]

        specs = list(cls_input)

        if likelihood:
            Dmat_obs_b = OrderedDict()
            Dmat_obs = None
            dSdqb = None
            Mmat = None
            Mmat_mix = None
        else:
            if windows:
                Dmat_obs = None
                Mmat = OrderedDict()
                Mmat_mix = OrderedDict() if self.pol else None
                mll = getattr(self, "mll", None)
                if mll is None:
                    mll = self.kernel_precalc()
            else:
                Dmat_obs = OrderedDict()
                Mmat = None
                Mmat_mix = None
            Dmat_obs_b = None
            dSdqb = OrderedDict()

        for xname, (m0, m1) in self.map_pairs.items():
            # transfer function doesn't have all the crosses
            if xname not in cls_input[specs[0]]:
                continue

            if likelihood:
                Dmat_obs_b[xname] = OrderedDict()
            elif windows:
                Mmat[xname] = OrderedDict()
                if self.pol:
                    Mmat_mix[xname] = OrderedDict()
            else:
                Dmat_obs[xname] = OrderedDict()

            for spec in specs:
                if likelihood:
                    # without bias subtraction for likelihood
                    Dmat_obs_b[xname][spec] = cls_input[spec][xname]
                elif windows:
                    Mmat[xname][spec] = mll[spec][xname]
                    if spec in ["ee", "bb"]:
                        mspec = "bb" if spec == "ee" else "ee"
                        Mmat_mix[xname][spec] = mll["{}_mix".format(mspec)][xname]
                else:
                    if cls_debias is not None:
                        Dmat_obs[xname][spec] = (
                            cls_input[spec][xname] - cls_debias[spec][xname]
                        )
                    else:
                        Dmat_obs[xname][spec] = cls_input[spec][xname]

            if likelihood:
                continue

            for comp in comps:
                for spec in specs:
                    stag = "{}_{}".format(comp, spec)
                    if stag not in cbl:
                        continue
                    if xname not in cbl[stag]:
                        continue

                    dSdqb.setdefault(comp, OrderedDict()).setdefault(
                        xname, OrderedDict()
                    ).setdefault(spec, OrderedDict())
                    dSdqb[comp][xname][spec][spec] = cbl[stag][xname]

                    if spec in ["ee", "bb"]:
                        if stag + "_mix" not in cbl:
                            continue
                        mspec = "bb" if spec == "ee" else "ee"
                        mix_cbl = cbl[stag + "_mix"][xname]
                        dSdqb[comp][xname][spec][mspec] = mix_cbl

                if comp == "fg":
                    # add delta beta bin for spectral index
                    dSdqb.setdefault("delta_beta", OrderedDict()).setdefault(
                        xname, OrderedDict()
                    )
                    for spec in specs:
                        # this will be filled in in fisher_calc
                        dSdqb["delta_beta"][xname][spec] = OrderedDict()

        return Dmat_obs, Dmat_obs_b, dSdqb, Mmat, Mmat_mix

    def clear_precalc(self):
        """
        Clear variables pre-computed with ``fisher_precalc``.
        """
        self.Dmat_obs = None
        self.Dmat_obs_b = None
        self.dSdqb_mat1 = None
        self.Mmat = None
        self.Mmat_mix = None
        self.mll = None

    def fisher_calc(
        self,
        qb,
        cbl,
        cls_input,
        cls_noise=None,
        cls_debias=None,
        cls_model=None,
        cond_noise=None,
        cond_criteria=None,
        likelihood=False,
        like_lmin=2,
        like_lmax=None,
        delta_beta_prior=None,
        null_first_cmb=False,
        use_precalc=True,
        windows=False,
        inv_fish=None,
    ):
        """
        Re-compute the Fisher matrix and qb amplitudes based on
        input data spectra.  This method is called iteratively
        by ``fisher_iterate``, and requires bin definitions precomputed
        by ``get_bin_def`` or ``get_transfer``.

        Arguments
        ---------
        qb : OrderedDict
            Bandpower amplitudes, typically computed in a previous call
            to this method.
        cbl : OrderedDict
            Cbl matrix computed by ``bin_cl_template`` for a given
            shape spectrum.
        cls_input : OrderedDict
            Input spectra.  If computing a transfer function,
            this is the average ``cls_signal``.  If computing a null
            test, this is ``cls_data_null``, and otherwise it is
            ``cls_data``, for a single map or several input maps.
        cls_noise : OrderedDict
            If supplied, the noise spectrum is applied to the model spectrum.
        cls_debias : OrderedDict
            If supplied, the debias spectrum is subtracted from the input.
        cls_model : OrderedDict
            Unbinned model spectrum computed from cbl
        cond_noise : float
            The level of regularizing noise to add to EE and BB diagonals.
        cond_criteria : float
            The maximum condition number allowed for Dmat1 to be acceptable
            for taking its inverse.
        likelihood : bool
            If True, return the likelihood for the given input bandpowers, shapes
            and data spectra.  Otherwise, computes output bandpowers and the fisher
            covariance for a NR iteration.
        like_lmin : int
            The minimum ell value to be included in the likelihood calculation
        like_lmax : int
            The maximum ell value to be included in the likelihood calculation
        delta_beta_prior : float
            The width of the prior on the additive change from beta_ref. If you
            don't want the code to fit for a spectral index different
            from beta_ref, set this to be a very small value (O(1e-10)).
        null_first_cmb : bool
            Keep first CMB bandpowers fixed to input shape (qb=1).
        use_precalc : bool
            If True, load pre-calculated terms stored from a previous iteration,
            and store for a future iteration.  Otherwise, all calculations are
            repeated.
        windows : bool
            If True, return W_bl window functions for each CMB qb.
        inv_fish : array_like
            Inverse Fisher matrix. If provided, don't need to recompute. Useful
            if just getting bandpower window functions.

        Returns
        -------
        qb : OrderedDict
            New bandpower amplitudes
        inv_fish : array_like
            Inverse Fisher correlation matrix over all bins
        -- or --
        likelihood : scalar
            Likelihood of the given input parameters.
        -- or --
        windows : OrderedDict
            qb window functions
        """
        if cond_criteria is None:
            cond_criteria = np.inf
        well_cond = False

        pol_dim = 3 if self.pol else 1
        do_fg = "fg_tt" in cbl

        dkey = "Dmat_obs_b" if likelihood else "Mmat" if windows else "Dmat_obs"

        if getattr(self, dkey, None) is None or not use_precalc:
            Dmat_obs, Dmat_obs_b, dSdqb_mat1, Mmat, Mmat_mix = self.fisher_precalc(
                cbl,
                cls_input,
                cls_debias=cls_debias,
                likelihood=likelihood,
                windows=windows,
            )
            if use_precalc:
                self.Dmat_obs = Dmat_obs
                self.Dmat_obs_b = Dmat_obs_b
                self.dSdqb_mat1 = dSdqb_mat1
                self.Mmat = Mmat
                self.Mmat_mix = Mmat_mix
        else:
            if likelihood:
                Dmat_obs_b = self.Dmat_obs_b
            else:
                if windows:
                    Mmat = self.Mmat
                    Mmat_mix = self.Mmat_mix
                else:
                    Dmat_obs = self.Dmat_obs
                dSdqb_mat1 = self.dSdqb_mat1

        delta_beta = 0.0
        if "delta_beta" in qb:
            delta_beta = qb["delta_beta"][0]

        if not likelihood:
            dSdqb_mat1_freq = copy.deepcopy(dSdqb_mat1)

        if likelihood or windows or not cond_noise:
            well_cond = True
            cond_noise = None

        gmat_ell = OrderedDict()
        Dmat1 = OrderedDict()

        if cls_model is None:
            cls_model = self.get_model_spectra(
                qb, cbl, delta=True, cls_noise=cls_noise, cond_noise=cond_noise
            )

        mkeys = list(cls_model)
        for xname, (m0, m1) in self.map_pairs.items():
            # transfer function does not have crosses
            if xname not in cls_model[mkeys[0]]:
                continue
            gmat_ell[xname] = self.gmat_ell[xname]

            if well_cond:
                for spec in self.specs:
                    # transfer function doesn't have eb/tb
                    if "total_{}".format(spec) not in cls_model:
                        continue
                    Dmat1.setdefault(xname, OrderedDict())
                    Dmat1[xname][spec] = cls_model["total_{}".format(spec)][xname]

        if well_cond:
            Dmat1_mat = pt.dict_to_dmat(Dmat1)

        # Set up dSdqb frequency dependence
        for xname, (m0, m1) in self.map_pairs.items():
            # transfer function does not have crosses
            if xname not in cls_model[mkeys[0]]:
                continue

            if do_fg and not likelihood:
                # Add spectral index dependence
                # dSdqb now depends on qb (spec index) because
                # model is non-linear so cannot be precomputed.

                # get foreground at pivot point spectral index
                # and first derivative
                freq_scale0, freq_scale_deriv = st.scale_dust(
                    self.map_freqs[m0],
                    self.map_freqs[m1],
                    ref_freq=self.ref_freq,
                    beta=self.beta_ref,
                    deriv=True,
                )
                freq_scale = freq_scale0 + delta_beta * freq_scale_deriv
                freq_scale_ratio = freq_scale_deriv / freq_scale

                # scale foreground model by frequency scaling adjusted for beta
                for s1, sdat in dSdqb_mat1_freq["fg"][xname].items():
                    for s2, sdat2 in sdat.items():
                        dSdqb_mat1_freq["fg"][xname][s1][s2] *= freq_scale

                # build delta_beta term from frequency scaled model,
                # divide out frequeny scaling and apply derivative term
                for s1, sdat in dSdqb_mat1_freq["delta_beta"][xname].items():
                    sdat[s1] = cls_model["fg_{}".format(s1)][xname] * freq_scale_ratio

        # Set up Dmat -- if it's not well conditioned, add noise to the
        # diagonal until it is.
        cond_iter = 0
        while not well_cond:
            cls_model = self.get_model_spectra(
                qb, cbl, delta=True, cls_noise=cls_noise, cond_noise=cond_noise
            )

            for xname, (m0, m1) in self.map_pairs.items():
                # transfer function does not have crosses
                if xname not in cls_model[mkeys[0]]:
                    continue

                for spec in self.specs:
                    # transfer function doesn't have eb/tb
                    if "total_{}".format(spec) not in cls_model:
                        continue
                    Dmat1.setdefault(xname, OrderedDict())
                    Dmat1[xname][spec] = cls_model["total_{}".format(spec)][xname]

            Dmat1_mat = pt.dict_to_dmat(Dmat1)

            cond = np.abs(
                np.linalg.cond(Dmat1_mat[:, :, self.lmin :].swapaxes(0, -1))
            ).max()

            if cond > cond_criteria and cond_noise:
                cond_iter += 1
                # cond_noise iteration factor found through trial and error
                cond_noise *= 10 ** (cond_iter / 100.0)
                if cond_iter == 1:
                    self.log(
                        "Condition criteria not met. "
                        "Max Cond={:.0f}, Thresh={:.0f}".format(cond, cond_criteria),
                        "debug",
                    )
            else:
                well_cond = True
                self.log(
                    "Condition criteria met. "
                    "Max Cond={:.0f}, Thresh={:.0f}, Iter={:d}".format(
                        cond, cond_criteria, cond_iter
                    ),
                    "debug",
                )
                if cond_noise is not None:
                    self.log("Cond_noise = {:.3e}".format(cond_noise), "debug")

        # construct arrays from dictionaries
        Dmat1 = Dmat1_mat
        gmat = pt.dict_to_dmat(gmat_ell)
        if likelihood:
            Dmat_obs_b = pt.dict_to_dmat(Dmat_obs_b)
        else:
            if windows:
                Mmat = pt.dict_to_dmat(Mmat)
                if self.pol:
                    Mmat_mix = pt.dict_to_dmat(Mmat_mix)
            else:
                Dmat_obs = pt.dict_to_dmat(Dmat_obs)
            dSdqb_mat1_freq = pt.dict_to_dsdqb_mat(dSdqb_mat1_freq, self.bin_def)

        # apply ell limits
        if likelihood:
            ell = slice(
                self.lmin if like_lmin is None else like_lmin,
                self.lmax + 1 if like_lmax is None else like_lmax + 1,
            )
        else:
            ell = slice(self.lmin, self.lmax + 1)
        Dmat1 = Dmat1[..., ell]
        if likelihood:
            Dmat_obs_b = Dmat_obs_b[..., ell]
        else:
            if windows:
                Mmat = Mmat[..., ell, :]
                if self.pol:
                    Mmat_mix = Mmat_mix[..., ell, :]
            else:
                Dmat_obs = Dmat_obs[..., ell]
            dSdqb_mat1_freq = dSdqb_mat1_freq[..., ell]
        gmat = gmat[..., ell]

        self.Dmat1 = Dmat1

        lam, R = np.linalg.eigh(Dmat1.swapaxes(0, -1))
        bad = (lam <= 0).sum(axis=-1).astype(bool)
        if bad.sum():
            # exclude any ell's with ill-conditioned D matrix
            # this should happen only far from max like
            bad_idx = np.unique(np.where(bad)[0])
            bad_ells = np.arange(ell.start, ell.stop)[bad_idx]
            self.log(
                "Found negative eigenvalues at ells {}".format(bad_ells), "warning"
            )
            gmat[..., bad_idx] = 0
        inv_lam = 1.0 / lam
        Dinv = np.einsum("...ij,...j,...kj->...ik", R, inv_lam, R).swapaxes(0, -1)

        if likelihood:
            # log(det(D)) = tr(log(D)), latter is numerically stable
            # compute log(D) by eigenvalue decomposition per ell
            log_lam = np.log(lam)
            Dlog = np.einsum("...ij,...j,...kj->...ik", R, log_lam, R).swapaxes(0, -1)

        else:
            # compute ell-by-ell inverse
            # Dinv = np.linalg.inv(Dmat1.swapaxes(0, -1)).swapaxes(0, -1)

            # optimized matrix multiplication
            # there is something super weird about this whole matrix operation
            # that causes the computation of mats to take four times as long
            # if mat1 is not computed.
            eye = np.eye(len(gmat))
            mat1 = np.einsum("ij...,jk...->ik...", eye, Dinv)
            mat2 = np.einsum("klm...,ln...->knm...", dSdqb_mat1_freq, Dinv)
            mat = np.einsum("ik...,knm...->inm...", mat1, mat2)

        if likelihood:
            # compute log likelihood as tr(g * (D^-1 * Dobs + log(D)))
            arg = np.einsum("ij...,jk...->ik...", Dinv, Dmat_obs_b) + Dlog
            like = -np.einsum("iij,iij->", gmat, arg) / 2.0

            # include priors in likelihood
            if "delta_beta" in qb and delta_beta_prior is not None:
                chi = (qb["delta_beta"] - self.delta_beta_fix) / delta_beta_prior
                like -= chi ** 2 / 2.0

            if null_first_cmb:
                for spec in self.specs:
                    stag = "cmb_{}".format(spec)
                    if stag not in qb:
                        continue
                    chi = (qb[stag][0] - 1) / np.sqrt(1e-10)
                    like -= chi ** 2 / 2.0

            return like

        # construct matrices for the qb and fisher terms,
        # and take the trace and sum over ell simultaneously
        if not windows:
            qb_vec = np.einsum("iil,ijkl,jil->k", gmat, mat, Dmat_obs) / 2.0
        if not windows or (windows and inv_fish is None):
            fisher = np.einsum("iil,ijkl,jiml->km", gmat, mat, dSdqb_mat1_freq) / 2

        if windows:
            if inv_fish is None:
                inv_fish = np.linalg.solve(fisher, np.eye(len(fisher)))

            # compute prefactors
            ells = np.arange(0, self.lmax + 1)
            norm = (2.0 * ells + 1.0) / 4.0 / np.pi

            # compute binning term
            arg = np.einsum("ij,kljm->klim", inv_fish, mat)
            wbl = OrderedDict()
            bin_index = pt.dict_to_index(qb)
            spec_mask = pt.spec_mask(nmaps=self.num_maps)

            # compute window functions for each spectrum
            for k, (left, right) in bin_index.items():
                comp, spec = k.split("_", 1)
                if comp not in ["cmb", "fg"]:
                    continue

                # select bins for corresponding spectrum
                sarg = arg[:, :, left:right]

                # select the spectrum terms from the kernel matrix
                smat = spec_mask[spec][:, :, None, None] * Mmat
                if spec in ["ee", "bb"]:
                    # add mixing terms
                    mspec = "bb" if spec == "ee" else "ee"
                    smat += spec_mask[mspec][:, :, None, None] * Mmat_mix

                # qb window function
                wbl1 = np.einsum("iil,ijkl,jilm->km", gmat, sarg, smat) / 2.0 / norm

                # bin weighting, allowing for overlapping bin edges
                chi_bl = np.zeros_like(norm)
                for (l, r), w in zip(self.bin_def[k], self.bin_weights[k]):
                    chi_bl[l:r] += w

                # check normalization
                cls_shape = self.cls_shape["fg" if comp == "fg" else k][: len(norm)]
                self.log(
                    "{} qb window function normalization: {}".format(
                        k, np.sum(wbl1 * norm * chi_bl * cls_shape, axis=-1)
                    ),
                    "debug",
                )

                wbl[k] = wbl1

            return wbl

        bin_index = pt.dict_to_index(qb)

        if "delta_beta" in qb and delta_beta_prior is not None:
            # XXX need documentation for what happens here
            # for imposing the delta_beta prior
            sl = slice(*(bin_index["delta_beta"]))
            d = 1.0 / delta_beta_prior ** 2
            qb_vec[sl] += d * self.delta_beta_fix
            fisher[sl, sl] += d

        if null_first_cmb:
            # blow up the fisher matrix for first bin
            # by implementing a tight prior
            for spec in self.specs:
                stag = "cmb_{}".format(spec)
                if stag in bin_index:  # check for transfer function
                    b0 = bin_index[stag][0]
                    fisher[b0, b0] += 1e10
                    qb_vec[b0] += 1e10

        # invert
        qb_vec = np.linalg.solve(fisher, qb_vec)
        inv_fish = np.linalg.solve(fisher, np.eye(len(qb_vec)))
        qb_vec = pt.arr_to_dict(qb_vec, qb)

        return qb_vec, inv_fish

    def fisher_iterate(
        self,
        cbl,
        map_tag=None,
        iter_max=200,
        converge_criteria=0.005,
        qb_start=None,
        transfer_run=False,
        save_iters=False,
        null_first_cmb=False,
        delta_beta_prior=None,
        cond_noise=None,
        cond_criteria=None,
        like_profiles=False,
        like_profile_sigma=3.0,
        like_profile_points=100,
        file_tag=None,
    ):
        """
        Iterate over the Fisher calculation to compute bandpower estimates
        assuming an input shape spectrum.

        Arguments
        ---------
        cbl : OrderedDict
            Cbl matrix computed from an input shape spectrum
        map_tag : str
            If not None, then iteration is performed over the spectra
            corresponding to the given map, rather over all possible
            combinations of map-map cross-spectra. In this case, the first
            dimension of the input cbl must be of size 1 (this is done
            automatically by calling ``bin_cl_template(..., map_tag=<map_tag>)``.
        iter_max : int
            Maximum number of iterations to perform.  if this limit is
            reached, a warning is issued.
        converge_criteria : float
            Maximum fractional change in qb that indicates convergence and
            stops iteration.
        qb_start : OrderedDict
            Initial guess at ``qb`` bandpower amplitudes.  If None, unity is
            assumed for all bins.
        transfer_run : bool
            If True, the input Cls passed to ``fisher_calc`` are the average
            of the signal simulations, and noise cls are ignored.
            If False, the input Cls are either ``cls_data_null``
            (for null tests) or ``cls_data``.  See ``get_masked_data`` for
            how these are computed.  The input noise is similarly either
            ``cls_noise_null`` or ``cls_noise``.
        save_iters : bool
            If True, the output data from each Fisher iteration are stored
            in an individual npz file.
        null_first_cmb : bool
            Keep first CMB bandpowers fixed to input shape (qb=1).
        delta_beta_prior : float
            The width of the prior on the additive change from beta_ref. If you
            don't want the code to fit for a spectral index different
            from beta_ref, set this to be a very small value (O(1e-10)).
        cond_noise : float
            The level of regularizing noise to add to EE and BB diagonals.
        cond_criteria : float
            The maximum condition number allowed for Dmat1 to be acceptable
            for taking its inverse.
        like_profiles : bool
            If True, compute profile likelihoods for each qb, leaving all
            others fixed at their maximum likelihood values.  Profiles are
            computed over a range +/--sigma as estimated from the diagonals
            of the inverse Fisher matrix.
        like_profile_sigma : float
            Range in units of 1sigma over which to compute profile likelihoods
        like_profile_points : int
            Number of points to sample along the likelihood profile
        file_tag : string
            If supplied, appended to the bandpowers filename.

        Returns
        -------
        data : dict
            The results of the Fisher iteration process.  This dictionary
            contains the fields::

                qb : converged bandpower amplitudes
                cb : output binned spectrum
                dcb : binned spectrum errors
                ellb : bin centers
                cov : spectrum covariance
                inv_fish : inverse fisher matrix
                fqb : fractional change in qb for the last iteration
                qb2cb : conversion factor
                cbl : Cbl matrix
                cls_model : unbinned model spectrum computed from Cbl
                bin_def : bin definition array
                cls_obs : observed input spectra
                cls_noise : noise spectra
                cls_shape : shape spectrum
                iters : number of iterations completed

            If ``transfer_run`` is False, this dictionary also contains::

                qb_transfer : transfer function amplitudes
                transfer : ell-by-ell transfer function
                nbins_res : number of residual bins


        Notes
        -----
        This method stores outputs to files with name 'transfer' for transfer
        function runs (if ``transfer_run = True``), otherwise with name
        'bandpowers'.  Outputs from each individual iteration, containing
        only the quantities that change with each step, are stored in
        separate files with the same name (and different index).
        """

        if transfer_run:
            null_first_cmb = False
            save_name = "transfer_wbins" if self.weighted_bins else "transfer"
        else:
            save_name = "bandpowers"

        # previous fqb iterations to monitor convergence and adjust conditioning
        prev_fqb = []
        cond_adjusted = False

        if qb_start is None:
            qb = OrderedDict()
            for k, v in self.bin_def.items():
                if transfer_run:
                    if "cmb" not in k or "eb" in k or "tb" in k:
                        continue
                if k == "delta_beta":
                    # qb_delta beta is a coefficient on the change from beta,
                    # so expect that it should be small if beta_ref is close
                    # (zeroes cause singular matrix problems)
                    qb[k] = [self.delta_beta_fix]
                elif k.startswith("res_") or k.startswith("fg_"):
                    # res qb=0 means noise model is 100% accurate.
                    qb[k] = 1e-5 * np.ones(len(v))
                else:
                    # start by assuming model is 100% accurate
                    qb[k] = np.ones(len(v))
        else:
            qb = qb_start

        obs, nell, debias = self.get_data_spectra(
            map_tag=map_tag, transfer_run=transfer_run
        )

        bin_index = pt.dict_to_index(self.bin_def)

        success = False
        for iter_idx in range(iter_max):
            self.log(
                "Doing Fisher step {}/{}...".format(iter_idx + 1, iter_max), "info"
            )

            qb_new, inv_fish = self.fisher_calc(
                qb,
                cbl,
                obs,
                cls_noise=nell,
                cls_debias=debias,
                cond_noise=cond_noise,
                delta_beta_prior=delta_beta_prior,
                cond_criteria=cond_criteria,
                null_first_cmb=null_first_cmb,
            )

            qb_arr = pt.dict_to_arr(qb, flatten=True)
            qb_new_arr = pt.dict_to_arr(qb_new, flatten=True)
            dqb = qb_new_arr - qb_arr
            fqb = dqb / qb_arr
            max_fqb = np.nanmax(np.abs(fqb))

            prev_fqb.append(max_fqb)

            fnan = np.isnan(fqb)
            if fnan.any():
                (nanidx,) = np.where(fnan)
                self.log(
                    "Iter {}: Ignoring {} bins with fqb=nan: bins={}, qb_new={}, "
                    "qb={}".format(
                        iter_idx,
                        len(nanidx),
                        nanidx,
                        qb_new_arr[nanidx],
                        qb_arr[nanidx],
                    ),
                    "warning",
                )

            self.log("Max fractional change in qb: {}".format(max_fqb), "info")

            # put qb_new in original dict
            qb = copy.deepcopy(qb_new)
            cls_model = self.get_model_spectra(
                qb, cbl, delta=True, cls_noise=nell, cond_noise=None
            )

            if "delta_beta" in qb:
                # get beta fit and beta error
                beta_fit = qb["delta_beta"][0] + self.beta_ref
                db_idx = slice(*bin_index["delta_beta"])
                beta_err = np.sqrt(np.diag(inv_fish[db_idx, db_idx]))[0]
            else:
                beta_fit = None
                beta_err = None

            if save_iters:
                # save only the quantities that change with each iteration
                out = dict(
                    map_tag=map_tag,
                    map_tags=self.map_tags,
                    iter_index=iter_idx,
                    bin_def=self.bin_def,
                    bin_weights=self.bin_weights,
                    cls_shape=self.cls_shape,
                    cls_obs=obs,
                    qb=qb,
                    fqb=fqb,
                    inv_fish=inv_fish,
                    cls_model=cls_model,
                    cbl=cbl,
                    map_freqs=self.map_freqs,
                    cls_signal=self.cls_signal,
                    cls_noise=self.cls_noise,
                    Dmat_obs=self.Dmat_obs,
                    gmat_ell=self.gmat_ell,
                    extra_tag=file_tag,
                )

                if "fg_tt" in self.bin_def:
                    out.update(
                        beta_fit=beta_fit,
                        beta_err=beta_err,
                        ref_freq=self.ref_freq,
                        beta_ref=self.beta_ref,
                    )

                self.save_data(save_name, bp_opts=not transfer_run, **out)

            (nans,) = np.where(np.isnan(qb_new_arr))
            if len(nans):
                msg = "Found NaN values in qb bins {} at iter {}".format(nans, iter_idx)
                break

            if fnan.all():
                msg = (
                    "All bins have fqb=NaN at iter {}, "
                    "something has gone horribly wrong.".format(iter_idx)
                )
                break

            negs = np.where(np.diag(inv_fish) < 0)[0]
            if len(negs):
                self.log(
                    "Iter {}: Found negatives in inv_fish diagonal at locations "
                    "{}".format(iter_idx, negs),
                    "warning",
                )

            if np.nanmax(np.abs(fqb)) < converge_criteria:
                if not transfer_run:
                    # Calculate final fisher matrix without conditioning
                    self.log("Calculating final Fisher matrix.", "info")
                    _, inv_fish = self.fisher_calc(
                        qb,
                        cbl,
                        obs,
                        cls_noise=nell,
                        cls_debias=debias,
                        cond_noise=None,
                        delta_beta_prior=delta_beta_prior,
                        null_first_cmb=null_first_cmb,
                    )

                # If any diagonals of inv_fisher are negative, something went wrong
                negs = np.where(np.diag(inv_fish) < 0)[0]
                if len(negs):
                    self.log(
                        "Found negatives in inv_fish diagonal at locations "
                        "{}".format(negs),
                        "warning",
                    )

                success = True
                break

            else:
                msg = "{} {} did not converge in {} iterations".format(
                    "Multi-map" if map_tag is None else "Map {}".format(map_tag),
                    "transfer function" if transfer_run else "spectrum",
                    iter_max,
                )
                # Check the slope of the last ten fqb_maxpoints.
                # If there's not a downward trend, adjust conditioning
                # criteria to help convergence.
                if len(prev_fqb) <= 10 or transfer_run:
                    continue
                m, b = np.polyfit(np.arange(10), prev_fqb[-10:], 1)
                if m > 0:  # Not converging
                    # First, start from very little conditioning
                    if not cond_adjusted:
                        cond_criteria = 5e3
                        cond_adjusted = True
                        self.log(
                            "Iter {}: Not converging. Setting cond_criteria={}".format(
                                iter_idx, cond_criteria
                            ),
                            "warning",
                        )

                    elif cond_criteria > 100:
                        cond_criteria /= 2.0
                        self.log(
                            "Iter {}: Tightening condition criteria to help convergence. "
                            "cond_criteria={}".format(iter_idx, cond_criteria),
                            "warning",
                        )
                    else:
                        self.log(
                            "Iter {}: Can't reduce cond_criteria any more.".format(
                                iter_idx
                            ),
                            "warning",
                        )
                    # give it ten tries to start converging
                    prev_fqb = []

        # save and return
        out = dict(
            qb=qb,
            inv_fish=inv_fish,
            fqb=fqb,
            bin_def=self.bin_def,
            bin_weights=self.bin_weights,
            iters=iter_idx,
            success=success,
            map_tags=self.map_tags,
            map_freqs=self.map_freqs,
            converge_criteria=converge_criteria,
            cond_noise=cond_noise,
            cond_criteria=cond_criteria,
            null_first_cmb=null_first_cmb,
            apply_gcorr=self.apply_gcorr,
            weighted_bins=self.weighted_bins,
        )

        if "fg_tt" in self.bin_def:
            out.update(
                delta_beta_prior=delta_beta_prior,
                beta_fit=beta_fit,
                beta_err=beta_err,
                ref_freq=self.ref_freq,
                beta_ref=self.beta_ref,
            )

        if self.debug:
            out.update(
                cbl=cbl,
                cls_obs=obs,
                cls_signal=self.cls_signal,
                cls_noise=self.cls_noise,
                cls_model=cls_model,
                cls_shape=self.cls_shape,
                cond_noise=cond_noise,
                Dmat_obs=self.Dmat_obs,
            )

        if not transfer_run:
            out.update(
                qb_transfer=self.qb_transfer,
                transfer=self.transfer,
            )
            if self.template_cleaned:
                out.update(template_alpha=self.template_alpha)

        if success and not transfer_run:
            # do one more fisher calc that doesn't include sample variance
            # set qb=very close to 0. 0 causes singular matrix problems.
            # don't do this for noise residual bins
            self.log("Calculating final Fisher matrix without sample variance.", "info")
            qb_zeroed = copy.deepcopy(qb)
            qb_new_ns = copy.deepcopy(qb)
            for comp in ["cmb", "fg"]:
                for spec in self.specs:
                    stag = "{}_{}".format(comp, spec)
                    if stag not in qb_zeroed:
                        continue
                    qb_zeroed[stag][:] = 1e-20
                    qb_new_ns[stag][:] = 1.0
            if "delta_beta" in qb:
                qb_zeroed["delta_beta"][:] = 1e-20
                qb_new_ns["delta_beta"][:] = 0

            _, inv_fish_ns = self.fisher_calc(
                qb_zeroed,
                cbl,
                obs,
                cls_noise=nell,
                cls_debias=debias,
                cond_noise=None,
                delta_beta_prior=None,
                null_first_cmb=null_first_cmb,
            )

            out.update(
                invfish_nosampvar=inv_fish_ns,
            )

            # compute window functions for CMB bins
            self.log("Calculating window functions for CMB bins", "info")
            wbl_qb = self.fisher_calc(
                qb,
                cbl,
                obs,
                cls_noise=nell,
                cls_debias=None,
                cond_noise=None,
                delta_beta_prior=delta_beta_prior,
                null_first_cmb=null_first_cmb,
                windows=True,
                inv_fish=inv_fish,
            )
            out.update(wbl_qb=wbl_qb)

            # compute bandpowers and covariances
            cb, dcb, ellb, cov, qb2cb, wbl_cb = self.do_qb2cb(qb, inv_fish, wbl_qb)
            _, dcb_ns, _, cov_ns, _, _ = self.do_qb2cb(qb, inv_fish_ns, wbl_qb)

            out.update(
                cb=cb,
                dcb=dcb,
                ellb=ellb,
                cov=cov,
                qb2cb=qb2cb,
                wbl_cb=wbl_cb,
                dcb_nosampvar=dcb_ns,
                cov_nosampvar=cov_ns,
            )

            if like_profiles:
                # compute bandpower likelihoods
                self.log("Calculating bandpower profile likelihoods", "info")
                max_like = self.fisher_calc(
                    qb,
                    cbl,
                    obs,
                    cls_noise=nell,
                    cond_noise=None,
                    delta_beta_prior=delta_beta_prior,
                    null_first_cmb=null_first_cmb,
                    likelihood=True,
                )

                dqb = pt.arr_to_dict(np.sqrt(np.abs(np.diag(inv_fish))), qb)
                qb_like = OrderedDict()
                cb_like = OrderedDict()

                for stag, qbs in qb.items():
                    qb_like[stag] = np.zeros(
                        (len(qbs), 2, like_profile_points), dtype=float
                    )
                    if "res" not in stag:
                        cb_like[stag] = np.zeros(
                            (len(qbs), 2, like_profile_points), dtype=float
                        )

                    for ibin, q in enumerate(qbs):
                        qb1 = copy.deepcopy(qb)
                        dq = dqb[stag][ibin] * like_profile_sigma
                        q_arr = np.linspace(q - dq, q + dq, like_profile_points)
                        like_arr = np.zeros_like(q_arr)
                        cb_arr = np.zeros_like(q_arr)

                        for iq, q1 in enumerate(q_arr):
                            qb1[stag][ibin] = q1
                            # use max likelihood qb's qb2cb to convert qb->cb
                            if "res" not in stag:
                                cb_arr[iq] = np.einsum(
                                    "ij,j->i", qb2cb[stag], qb1[stag]
                                )[ibin]
                            try:
                                like = self.fisher_calc(
                                    qb1,
                                    cbl,
                                    obs,
                                    cls_noise=nell,
                                    cond_noise=None,
                                    delta_beta_prior=delta_beta_prior,
                                    null_first_cmb=null_first_cmb,
                                    likelihood=True,
                                )
                            except np.linalg.LinAlgError:
                                like = np.nan

                            like_arr[iq] = like

                            self.log(
                                "{} bin {} delta qb {} delta like: {}".format(
                                    stag, ibin, q1 - q, like - max_like
                                ),
                                "debug",
                            )

                        qb_like[stag][ibin] = np.vstack([q_arr, like_arr])
                        if "res" not in stag:
                            cb_like[stag][ibin] = np.vstack([cb_arr, like_arr])

                out.update(max_like=max_like, qb_like=qb_like, cb_like=cb_like)

        if not success:
            save_name = "ERROR_{}".format(save_name)
            self.log(msg, "error")
            self.warn(msg)

        return self.save_data(
            save_name,
            map_tag=map_tag,
            bp_opts=not transfer_run,
            extra_tag=file_tag,
            **out
        )

    def get_transfer(
        self,
        converge_criteria=0.005,
        iter_max=200,
        save_iters=False,
        fix_bb_transfer=False,
    ):
        """
        Compute the transfer function from signal simulations created using
        the same spectrum as the input shape.

        This raises a ValueError if a negative transfer function amplitude
        is found.

        Arguments
        ---------
        converge_criteria : float
            Maximum fractional change in qb that indicates convergence and
            stops iteration.
        iter_max : int
            Maximum number of iterations to perform.  if this limit is
            reached, a warning is issued.
        save_iters : bool
            If True, the output data from each Fisher iteration are stored
            in an individual npz file.
        fix_bb_transfer : bool
            If True, after transfer functions have been calculated, impose
            the BB xfer is exactly equal to the EE transfer.

        Returns
        -------
        transfer : OrderedDict
            Ell-by-ell transfer function for each map

        Notes
        -----
        This method is called at the 'transfer' checkpoint, and loads or saves
        a data dictionary named 'transfer_all' with the following entries:

            nbins:
                number of bins
            bin_def:
                bin definition dictionary (see ``get_bin_def``)
            qb_transfer:
                binned transfer function for each map and spectrum component
            transfer:
                ell-by-ell transfer function for each map and spectrum component

        Additionally the final output of ``fisher_iterate`` is stored
        in a dictionary called ``transfer_map<idx>`` for each map.
        """
        transfer_shape = (
            self.num_maps * len(self.specs),
            self.nbins_cmb / len(self.specs),
        )

        opts = dict(
            converge_criteria=converge_criteria,
            fix_bb_transfer=fix_bb_transfer,
            apply_gcorr=self.apply_gcorr,
            weighted_bins=self.weighted_bins,
        )

        save_name = "transfer_all"
        if self.weighted_bins:
            save_name = "{}_wbins".format(save_name)

        ret = self.load_data(
            save_name,
            "transfer",
            to_attrs=False,
            shape_ref="qb_transfer",
            shape=transfer_shape,
            value_ref=opts,
        )

        # function for converting qb's to ell-by-ell transfer function
        def expand_transfer(qb_transfer, bin_def):
            transfer = OrderedDict()
            for spec in self.specs:
                stag = "cmb_{}".format(spec)
                transfer[spec] = OrderedDict()
                bd = bin_def[stag]
                for m0 in self.map_tags:
                    if spec == "bb" and fix_bb_transfer:
                        transfer[spec][m0] = transfer["ee"][m0]
                    elif spec in ["tb", "eb"]:
                        speca, specb = [s + s for s in spec]
                        transfer[spec][m0] = np.sqrt(
                            np.abs(transfer[speca][m0] * transfer[specb][m0])
                        )
                    else:
                        transfer[spec][m0] = pt.expand_qb(
                            qb_transfer[stag][m0], bd, self.lmax + 1
                        )
            return transfer

        if ret is not None:
            self.qb_transfer = ret["qb_transfer"]
            if "transfer" in ret:
                self.transfer = ret["transfer"]
            else:
                self.transfer = expand_transfer(ret["qb_transfer"], ret["bin_def"])
            return self.transfer

        self.qb_transfer = OrderedDict()
        for spec in self.specs:
            self.qb_transfer["cmb_" + spec] = OrderedDict()

        success = True
        msg = ""

        for im0, m0 in enumerate(self.map_tags):
            if not self.fit_transfer[self.map_tags_orig[im0]]:
                for spec in self.specs:
                    self.qb_transfer["cmb_{}".format(spec)][m0] = np.ones(
                        self.nbins_cmb // len(self.specs)
                    )
                self.log("Setting map {} transfer to unity".format(m0), "info")
                continue

            self.log(
                "Computing transfer function for map {}/{}".format(
                    im0 + 1, self.num_maps
                ),
                "info",
            )
            self.clear_precalc()
            cbl = self.bin_cl_template(map_tag=m0, transfer_run=True)
            ret = self.fisher_iterate(
                cbl,
                m0,
                transfer_run=True,
                iter_max=iter_max,
                converge_criteria=converge_criteria,
                save_iters=save_iters,
            )
            qb = ret["qb"]

            success = success and ret["success"]
            if not ret["success"]:
                msg = "Error in fisher_iterate for map {}".format(m0)

            # fix negative amplitude bins
            for k, v in qb.items():
                if np.any(v < 0):
                    (negbin,) = np.where(v < 0)
                    self.warn(
                        "Transfer function amplitude {} < 0"
                        "for {} bin {} of map {}".format(v, k, negbin, m0)
                    )
                    # XXX cludge
                    # This happens in first bin
                    # try linear interp between zero and next value
                    try:
                        qb[k][negbin] = qb[k][negbin + 1] / 2.0
                        self.warn(
                            "Setting Transfer function in negative bin to small "
                            "positive. This is probably due to choice of bins or "
                            "insufficient number of signal sims"
                        )
                    except Exception as e:
                        msg = "Unable to adjust negative bins for map {}: {}".format(
                            m0, str(e)
                        )
                        success = False

            # Set EB/TB qb transfers to geometric means of components
            if len(self.specs) > 4:
                qb["cmb_eb"] = np.sqrt(np.abs(qb["cmb_ee"] * qb["cmb_bb"]))
                qb["cmb_tb"] = np.sqrt(np.abs(qb["cmb_tt"] * qb["cmb_bb"]))

            for stag, qbdat in qb.items():
                self.qb_transfer[stag][m0] = qbdat

        if success:
            self.transfer = expand_transfer(self.qb_transfer, self.bin_def)
        else:
            self.transfer = None

        self.save_data(
            "{}{}".format("" if success else "ERROR_", save_name),
            from_attrs=["nbins", "bin_def", "qb_transfer", "map_tags", "transfer"],
            cls_shape=self.cls_shape,
            success=success,
            **opts
        )

        if not success:
            raise RuntimeError("Error computing transfer function: {}".format(msg))

        return self.transfer

    def get_bandpowers(
        self,
        map_tag=None,
        converge_criteria=0.005,
        iter_max=200,
        return_qb=False,
        save_iters=False,
        ref_freq=359.7,
        beta_ref=1.54,
        delta_beta_prior=None,
        cond_noise=None,
        cond_criteria=None,
        null_first_cmb=False,
        return_cls=False,
        like_profiles=False,
        like_profile_sigma=3.0,
        like_profile_points=100,
        file_tag=None,
    ):
        """
        Compute the maximum likelihood bandpowers of the data, assuming
        a given input spectrum shape.  Requires the transfer function to
        have been computed and loaded using ``get_transfer``.

        Arguments
        ---------
        map_tag : string
            If not None, then iteration is performed over the spectra
            corresponding to the given map, rather over all possible
            combinations of map-map cross-spectra. In this case, the first
            dimension of the input cbl must be of size 1 (this is done
            automatically by calling ``bin_cl_template(..., map_tag=<map_tag>)``.
        converge_criteria : float
            Maximum fractional change in qb that indicates convergence and
            stops iteration.
        iter_max : int
            Maximum number of iterations to perform.  if this limit is
            reached, a warning is issued.
        return_qb : bool
            If True, only the maximum likelihood ``qb`` values are returned.
            Otherwise, the complete output dictionary is returned.
        save_iters : bool
            If True, the output data from each Fisher iteration are stored
            in an individual npz file.
        ref_freq : float
            In GHz, reference frequency for dust model. Dust bandpowers output
            will be at this reference frequency.
        beta_ref : float
            The spectral index of the dust model. This is a fixed value, with
            an additive deviation from this value fit for in foreground fitting
            mode.
        delta_beta_prior : float
            The width of the prior on the additive change from beta_ref. If you
            don't want the code to fit for a spectral index different
            from beta_ref, set this to be a very small value (O(1e-10)).
        cond_noise : float
            The level of regularizing noise to add to EE and BB diagonals.
        cond_criteria : float
            The maximum condition number allowed for Dmat1 to be acceptable
            for taking its inverse.
        null_first_cmb : bool
            Keep first CMB bandpowers fixed to input shape (qb=1).
        return_cls : bool
            If True, return C_ls rather than D_ls
        like_profiles : bool
            If True, compute profile likelihoods for each qb, leaving all
            others fixed at their maximum likelihood values.  Profiles are
            computed over a range +/--sigma as estimated from the diagonals
            of the inverse Fisher matrix.
        like_profile_sigma : float
            Range in units of 1sigma over which to compute profile likelihoods
        like_profile_points : int
            Number of points to sample along the likelihood profile
        file_tag : string
            If supplied, appended to the bandpowers filename.

        Returns
        -------
        data : dict
            Dictionary of maximum likelihood quantities, as output by
            ``fisher_iterate``.
        -- or --
        qb, inv_fish : array_like
            Maximum likelihood bandpower amplitudes and fisher covariance.

        Notes
        -----
        This method is called at the 'bandpowers' checkpoint, and loads or
        saves a data dictionary named 'bandpowers' with the quantities
        returned by ``fisher_iterate``.
        """

        save_name = "bandpowers"

        fish_shape = (len(pt.dict_to_arr(self.bin_def)),) * 2

        # check all options that require rerunning fisher iterations
        opts = dict(
            converge_criteria=converge_criteria,
            cond_noise=cond_noise,
            cond_criteria=cond_criteria,
            null_first_cmb=null_first_cmb,
            apply_gcorr=self.apply_gcorr,
            weighted_bins=self.weighted_bins,
        )

        # foreground fitting
        if "fg_tt" in self.bin_def:
            # reference frequency and spectral index
            self.ref_freq = ref_freq
            self.beta_ref = beta_ref
            # priors on frequency spectral index
            self.delta_beta_fix = 1.0e-8
            opts.update(
                delta_beta_prior=delta_beta_prior,
                ref_freq=self.ref_freq,
                beta_ref=self.beta_ref,
            )

        if self.template_cleaned:
            opts.update(template_alpha=self.template_alpha)
        self.return_cls = return_cls

        ret = self.load_data(
            save_name,
            "bandpowers",
            bp_opts=True,
            to_attrs=False,
            shape=fish_shape,
            shape_ref="inv_fish",
            map_tag=map_tag,
            value_ref=opts,
            extra_tag=file_tag,
        )
        if ret is not None:
            if return_qb:
                return ret["qb"], ret["inv_fish"]
            return ret

        self.clear_precalc()

        cbl = self.bin_cl_template(map_tag=map_tag, transfer_run=False)

        ret = self.fisher_iterate(
            cbl,
            map_tag,
            transfer_run=False,
            iter_max=iter_max,
            converge_criteria=converge_criteria,
            save_iters=save_iters,
            cond_noise=cond_noise,
            cond_criteria=cond_criteria,
            null_first_cmb=null_first_cmb,
            delta_beta_prior=delta_beta_prior,
            like_profiles=like_profiles,
            like_profile_sigma=like_profile_sigma,
            like_profile_points=like_profile_points,
            file_tag=file_tag,
        )

        if not ret["success"]:
            raise RuntimeError("Error computing bandpowers")

        # return
        if return_qb:
            return ret["qb"], ret["inv_fish"]
        return ret

    def get_likelihood(
        self,
        qb,
        inv_fish,
        map_tag=None,
        null_first_cmb=False,
        lmin=33,
        lmax=250,
        mcmc=True,
        alpha_tags="all",
        beam_tags="all",
        r_prior=[0, np.inf],
        alpha_prior=[0, np.inf],
        res_prior=None,
        beam_prior=[0, 1],
        betad_prior=[0, 1],
        dust_amp_prior=[0, np.inf],
        dust_ellind_prior=[0, 1],
        num_walkers=50,
        num_steps=20000,
        converge_criteria=0.01,
        reset_backend=None,
        file_tag=None,
        subtract_template_noise=False,
        r_specs=["ee", "bb"],
        template_specs=["ee", "bb", "eb"],
    ):
        """
        Explore the likelihood, optionally with an MCMC sampler.

        Arguments
        ---------
        qb : OrderedDict
            Bandpower parameters previously computed by Fisher iteration.
        inv_fish : array_like
            Inverse Fisher matrix computed with the input qb's.
        map_tag : string
            If not None, then the likelihood is sampled using the spectra
            corresponding to the given map, rather over all possible
            combinations of map-map cross-spectra.  The input qb's and inv_fish
            must have been computed with the same option.
        null_first_cmb : bool
            Keep first CMB bandpowers fixed to input shape (qb=1).
        lmin : int
            The minimum ell value to be included in the likelihood calculation
        lmax : int
            The maximum ell value to be included in the likelihood calculation
        mcmc : bool
            If True, sample the likelihood using an MCMC sampler.  Remaining
            options determine parameter space and sampler configuration.
        alpha_tags : list of strings
            List of map tags from which foreground template maps should be
            subtracted.  These should be the original map tags, not
            those generated for chunk sets.  If "all", then all map tags
            used in the template subtraction are included, as determined by
            the keys in the `template_alpha` attribute.  If None, then the alpha
            parameters are not included in the likelihood.
        beam_tags : list of strings
            List of map tags from which beam error envelopes should be
            marginalized over. These should be the original map tags, not
            those generated for chunk sets.  If "all", then all available map
            tags in the dataset are included.  If None, then the beam error
            parameters are not included in the likelihood.
        r_prior : 2-list or None
            Prior upper and lower bound on tensor to scalar ratio.  If None, the
            fiducial shape spectrum is assumed, and the r parameter space is not
            varied.
        alpha_prior : 2-list or None
            Prior upper and lower bound on template coefficients.  If None, the
            alpha parameter space is not varied.
        res_prior : 2-list or none
            Prior upper and lower bound on residual qbs.  If None, the
            res parameter space is not varied.
        beam_prior : 2-list or none
            Prior mean and width of gaussian width on beam error (when
            multiplied by beam error envelope).  If None, the
            beam parameter space is not varied.
        betad_prior : 2-list or none
            Prior mean and width of gaussian width on dust spectral index.
            If None, the dust index parameter space is not varied.
        dust_amp_prior : 2-list or none
            Prior upper and lower bound on dust amplitude.
            If None, the dust amp parameter space is not varied.
        dust_ellind_prior : 2-list or none
            Prior mean and width of Gaussian prior on difference in dust ell
            power law index. If None, don't vary from reference if fitting dust
            power spectrum model.
        num_walkers : int
            Number of unique walkers with which to sample the parameter space.
        num_steps : int
            Maximum number of steps each walker can take in sampling the
            parameter space.
        converge_criteria : float
            Convergence criteria for likelihood MCMC chains
        reset_backend : bool
            If True, clear the backend buffer before sampling.  If False,
            samples are appended to the existing buffer.  If not supplied,
            set to True if the checkpoint has been forced to be rerun.
        file_tag : string
            If supplied, appended to the likelihood filename.
        subtract_template_noise : bool
            If True, subtract average of cross spectra of an ensemble of noise
            realizations corresponding to each template map, to debias
            template-cleaned spectra.  Typically, this would be a noise model
            based on the Planck FFP10 ensemble for each half-mission foreground
            template.
        r_specs : list
            Which spectra to use in the r likelihood.
        template_specs : list
            Which spectra to use for alpha in the likelihood.
        """

        # Check if OMP threads > 1. It actually slows the code way down to have
        # more threads. Break instead.
        assert os.getenv("OMP_NUM_THREADS") in [
            None,
            "1",
        ], "OMP threads must be set to 1 for likelihood"

        for x in [
            r_prior,
            alpha_prior,
            res_prior,
            beam_prior,
            betad_prior,
            dust_amp_prior,
            dust_ellind_prior,
        ]:
            if x is not None:
                x[:] = [float(x[0]), float(x[1])]

        r_specs = [x.lower() for x in r_specs]
        template_specs = [x.lower() for x in template_specs]

        save_name = "like_mcmc"
        if not mcmc:
            alpha_prior = None
            res_prior = None
            beam_prior = None
            betad_prior = None
            dust_amp_prior = None
            dust_ellind_prior = None

        # no template cleaning if there aren't any templates specified
        if not getattr(self, "template_cleaned", False):
            alpha_prior = None

        # null out unused priors
        self.template_alpha = getattr(self, "template_alpha", None)
        if self.template_alpha is None or all(
            [x is None for x in self.template_alpha.values()]
        ):
            alpha_prior = None

        # count alpha parameters to fit
        if alpha_tags is None:
            alpha_tags = []
        elif alpha_tags == "all":
            alpha_tags = list(self.template_alpha) if self.template_alpha else []
        alpha_tags = [x for x in alpha_tags if x in self.map_tags_orig]
        if not len(alpha_tags):
            alpha_prior = None

        num_alpha = 0
        if alpha_prior is not None:
            num_alpha = len(alpha_tags)

        # count beam parameters to fit
        if beam_tags is None:
            beam_tags = []
        elif beam_tags == "all":
            beam_tags = list(self.map_tags_orig)
        beam_tags = [x for x in beam_tags if x in self.map_tags_orig]
        if not len(beam_tags):
            beam_prior = None

        num_beam = 0
        if beam_prior is not None:
            num_beam = len(beam_tags)

        if not any([k.startswith("res_") for k in qb]):
            res_prior = None

        if np.any(
            [
                betad_prior is not None,
                dust_amp_prior is not None,
                dust_ellind_prior is not None,
            ]
        ):
            dust_ell_fit = True
        else:
            dust_ell_fit = False

        # bookkeeping: ordered priors
        priors = {
            "r_prior": r_prior,
            "alpha_prior": alpha_prior,
            "res_prior": res_prior,
            "beam_prior": beam_prior,
            "betad_prior": betad_prior,
            "dust_amp_prior": dust_amp_prior,
            "dust_ellind_prior": dust_ellind_prior,
        }
        # priors on quantities that affect Dmat_obs or gmat (precalculated)
        obs_priors = [alpha_prior]

        # check parameter space
        if all([x is None for x in priors.values()]):
            raise RuntimeError("Empty parameter space")

        out = dict(
            r_prior=r_prior,
            alpha_prior=alpha_prior,
            res_prior=res_prior,
            beam_prior=beam_prior,
            betad_prior=betad_prior,
            dust_amp_prior=dust_amp_prior,
            dust_ellind_prior=dust_ellind_prior,
            alpha_tags=alpha_tags,
            num_walkers=num_walkers,
            null_first_cmb=null_first_cmb,
            apply_gcorr=self.apply_gcorr,
            weighted_bins=self.weighted_bins,
            lmin=lmin,
            lmax=lmax,
        )

        if mcmc and reset_backend is None:
            ret = self.load_data(
                save_name,
                "likelihood",
                bp_opts=True,
                to_attrs=False,
                map_tag=map_tag,
                value_ref=out,
                extra_tag=file_tag,
            )
            if ret is not None and ret.get("converged", False):
                if converge_criteria >= ret.get("converge_criteria", 0.01):
                    return ret
            if ret is not None:
                for pname, pval in priors.items():
                    if np.all(pval != ret.get(pname, None)):
                        ret = None
            # clear chain cache if rerunning, otherwise append to chain by default
            reset_backend = ret is None

        out.update(converge_criteria=converge_criteria)

        # save state
        if mcmc and reset_backend:
            self.save_data(
                save_name, map_tag=map_tag, extra_tag=file_tag, bp_opts=True, **out
            )

        # clear pre-computed quantities
        self.clear_precalc()
        use_precalc = all([x is None for x in obs_priors])

        cls_input, cls_noise, cls_debias = self.get_data_spectra()

        # extract residual bins, ignoring bins outside of lmin/lmax
        if res_prior is not None:
            bin_def_orig = copy.deepcopy(self.bin_def)
            nbins_res_orig = self.nbins_res
            qb_res = OrderedDict()
            num_res = 0
            for k in list(qb):
                if k.startswith("res_"):
                    bd = self.bin_def[k]
                    good = np.where((bd[:, 1] > lmin) & (bd[:, 0] < lmax))[0]
                    # use all qb res in range lmin, lmax
                    self.bin_def[k] = bd[good]
                    v = qb.pop(k)[good]
                    num_res += len(v)

                    # use average qb res in good range per map
                    # self.bin_def[k] = np.array([[lmin, lmax + 1]])
                    # v = np.array([(qb.pop(k)[good]).mean()])
                    # num_res += 1
                    qb_res[k] = v
            self.nbins_res = num_res

        # set CMB model bandpowers to unity, since we are computing
        # the likelihood of this model given the data
        if r_prior is None:
            self.log("Computing model spectrum", "debug")
            self.warn("Beam variation not implemented for case of no r fit")
            cbl = self.bin_cl_template(map_tag=map_tag)
            cls_model = self.get_model_spectra(qb, cbl, delta=True, cls_noise=cls_noise)
        else:
            qb = copy.deepcopy(qb)
            for spec in self.specs:
                stags = ["cmb_{}".format(spec), "fg_{}".format(spec)]
                for stag in stags:
                    if stag not in qb:
                        continue
                    qb[stag] = np.ones_like(qb[stag])

            self.log("Computing r model spectrum", "debug")
            cls_shape_scalar = self.get_signal_shape(
                r=1.0, save=False, component="scalar"
            )

            cls_shape_tensor = self.get_signal_shape(
                r=1.0, save=False, component="tensor"
            )

            # load tensor and scalar terms separately
            cbl_scalar = self.bin_cl_template(cls_shape_scalar, map_tag)
            cls_model_scalar = self.get_model_spectra(
                qb, cbl_scalar, delta=True, cls_noise=cls_noise
            )
            cbl_tensor = self.bin_cl_template(cls_shape_tensor, map_tag)
            cls_model_tensor = self.get_model_spectra(
                qb, cbl_tensor, delta=False, res=False
            )
            if beam_prior is not None:
                # load beam error term for tensor and scalar
                cbl_scalar_beam = self.bin_cl_template(
                    cls_shape_scalar, map_tag, beam_error=True
                )
                cls_mod_scal_beam = self.get_model_spectra(
                    qb, cbl_scalar_beam, delta=True, res=False
                )
                cbl_tensor_beam = self.bin_cl_template(
                    cls_shape_tensor, map_tag, beam_error=True
                )
                cls_mod_tens_beam = self.get_model_spectra(
                    qb, cbl_tensor_beam, delta=False, res=False
                )

            # load foreground shape
            if dust_ell_fit:
                cls_shape_dust = self.get_signal_shape(save=False, component="fg")
                # if dust_ellind_prior is None:
                #    # can preload shape since not varying ell index
                cbl_fg = self.bin_cl_template(cls_shape_dust, map_tag=map_tag)
                if beam_prior is not None:
                    cbl_fg_beam = self.bin_cl_template(
                        cls_shape_dust, map_tag, beam_error=True
                    )

            cbl = copy.deepcopy(cbl_scalar)
            cls_model = copy.deepcopy(cls_model_scalar)

        # XXX TODO
        # how to marginalize over the garbage bin?

        def parse_params(theta):
            """
            Parse array of parameters into a dict
            """
            params = {}
            if r_prior is not None:
                params["r"] = theta[0]
                theta = theta[1:]
            if alpha_prior is not None:
                params["alpha"] = theta[:num_alpha]
                theta = theta[num_alpha:]
            if res_prior is not None:
                params["res"] = theta[:num_res]
                theta = theta[num_res:]
            if beam_prior is not None:
                params["beam"] = theta[:num_beam]
                theta = theta[num_beam:]
            if betad_prior is not None:
                params["betad"] = theta[0]
                theta = theta[1:]
            if dust_amp_prior is not None:
                # param for ee and bb
                params["dust_amp"] = theta[:2]
                theta = theta[2:]
            if dust_ellind_prior is not None:
                params["dust_ellind"] = theta[0]
                theta = theta[1:]
            if len(theta):
                raise ValueError("Too many parameters to parse")
            return params

        def log_prior(
            r=None,
            alpha=None,
            res=None,
            beam=None,
            betad=None,
            dust_amp=None,
            dust_ellind=None,
        ):
            """
            Log prior function constructed from input options
            """
            values = {
                "r_prior": r,
                "alpha_prior": alpha,
                "res_prior": res,
                "dust_amp_prior": dust_amp,
            }
            for v, pval in values.items():
                prior = priors[v]
                if pval is not None and prior is not None:
                    if np.any(pval < prior[0]) or np.any(pval > prior[1]):
                        return -np.inf

            values_gauss = {
                "beam_prior": beam,
                "betad_prior": betad,
                "dust_ellind_prior": dust_ellind,
            }
            # for beam and betad, use gaussian prior
            log_prob = 0.0
            for v, pval in values_gauss.items():
                prior = priors[v]
                if pval is not None and prior is not None:
                    pval = np.atleast_1d(pval)
                    norm = np.log(1.0 / (prior[1] * np.sqrt(2 * np.pi)))
                    chi = (pval - prior[0]) / prior[1]
                    log_prob += np.sum(norm - chi ** 2 / 2.0)

            return log_prob

        def log_like(
            r=None,
            alpha=None,
            res=None,
            beam=None,
            betad=None,
            dust_amp=None,
            dust_ellind=None,
        ):
            """
            Log likelihood function constructed from input options
            """
            cls_model0 = copy.deepcopy(cls_model)

            # compute new template subtracted data spectra
            if alpha is None:
                clsi = cls_input
            else:
                self.get_masked_data(
                    template_alpha=OrderedDict(zip(alpha_tags, alpha)),
                    subtract_template_noise=subtract_template_noise,
                    template_specs=template_specs,
                )
                clsi = self.get_data_spectra(do_noise=False)

            if beam is not None:
                beam = dict(zip(beam_tags, beam))
                beam_coeffs = dict()
                for xname, (m0, m1) in self.map_pairs_orig.items():
                    d = {}
                    b0, b1 = [beam.get(m, None) for m in (m0, m1)]
                    if b0 is not None:
                        d["b1"] = b0
                    if b1 is not None:
                        d["b2"] = b1
                        if b0 is not None:
                            d["b3"] = b0 * b1
                    beam_coeffs[xname] = d

            # compute new signal shape by scaling tensor component by r
            if r is not None:
                for stag, d in cls_model0.items():
                    comp, spec = stag.split("_", 1)
                    if spec not in r_specs or comp not in ["cmb", "total"]:
                        continue
                    ctag = "cmb_{}".format(spec)
                    for xname, dd in d.items():
                        dd[:] = (
                            cls_model_scalar[stag][xname]
                            + r * cls_model_tensor[ctag][xname]
                        )

                        if beam is None:
                            continue
                        beam_term = 0
                        for bn, bc in beam_coeffs[xname].items():
                            beam_term += bc * (
                                cls_mod_scal_beam[ctag][xname][bn]
                                + r * cls_mod_tens_beam[ctag][xname][bn]
                            )
                        dd[:] += beam_term

            elif beam is not None:
                for stag, d in cls_model0.items():
                    comp, spec = stag.split("_", 1)
                    if spec not in r_specs or comp not in ["cmb", "total"]:
                        continue
                    ctag = "cmb_{}".format(spec)
                    for xname, dd in d.items():
                        beam_term = 0
                        for bn, bc in beam_coeffs[xname].items():
                            beam_term += bc * cls_mod_scal_beam[ctag][xname][bn]
                        dd[:] = cls_model_scalar[stag][xname] + beam_term

            # fg term, including beam modifications. Because mix terms are
            # dependent on dust amp, get model specs here.
            if dust_ell_fit:
                if dust_amp is None:
                    qb["fg_ee"][:] = 1
                    qb["fg_bb"][:] = 1
                else:
                    qb["fg_ee"][:] = dust_amp[0]
                    qb["fg_bb"][:] = dust_amp[1]
                if betad is None:
                    qb["delta_beta"][:] = 0
                else:
                    qb["delta_beta"][:] = betad
                if dust_ellind is not None:
                    cbl_fg0 = self.bin_cl_template(
                        cls_shape_dust, map_tag=map_tag, fg_ell_ind=dust_ellind
                    )
                    if beam is not None:
                        cbl_fg_beam0 = self.bin_cl_template(
                            cls_shape_dust,
                            map_tag,
                            fg_ell_ind=dust_ellind,
                            beam_error=True,
                        )
                else:
                    cbl_fg0 = cbl_fg
                    if beam is not None:
                        cbl_fg_beam0 = cbl_fg_beam

                cls_model_fg = self.get_model_spectra(
                    qb, cbl_fg0, delta=True, res=False
                )
                if beam is not None:
                    cls_mod_fg_beam = self.get_model_spectra(
                        qb, cbl_fg_beam0, delta=True, res=False
                    )
                # add fg field to model, and add fg to total model
                for stag, d in cls_model_fg.items():
                    comp, spec = stag.split("_", 1)
                    if spec not in ["ee", "bb"] or comp not in ["fg", "total"]:
                        continue
                    ftag = "fg_{}".format(spec)
                    if stag not in cls_model0:
                        cls_model0[stag] = OrderedDict()
                    for xname, dd in d.items():
                        if xname not in cls_model0[stag]:
                            cls_model0[stag][xname] = cls_model_fg[ftag][xname]
                        else:
                            cls_model0[stag][xname] += cls_model_fg[ftag][xname]

                        # add beam terms to fg and total fields
                        if beam is not None:
                            beam_term = 0
                            for bn, bc in beam_coeffs[xname].items():
                                beam_term += bc * cls_mod_fg_beam[ftag][xname][bn]
                            cls_model0[stag][xname] += beam_term

            # compute noise model terms
            if res is None:
                clsm = cls_model0
            else:
                res = pt.arr_to_dict(res, qb_res)
                clsm = copy.deepcopy(cls_model0)
                cls_res = self.get_model_spectra(res, cbl)
                for stag, d in cls_res.items():
                    if stag not in clsm:
                        clsm[stag] = OrderedDict()
                    for xname, dd in d.items():
                        if xname not in clsm[stag]:
                            clsm[stag][xname] = dd
                        else:
                            clsm[stag][xname] += dd

            # compute likelihood
            like = self.fisher_calc(
                qb,
                cbl,
                clsi,
                cls_noise=cls_noise,
                cls_debias=cls_debias,
                cls_model=clsm,
                null_first_cmb=null_first_cmb,
                likelihood=True,
                use_precalc=use_precalc,
                like_lmin=lmin,
                like_lmax=lmax,
            )
            return like

        def log_prob(theta):
            """
            Log posterior probability from prior and likelihood

            Returns log_prior with each step
            """
            params = parse_params(theta)
            prior = log_prior(**params)
            if not np.isfinite(prior):
                return -np.inf, -np.inf
            like = log_like(**params)
            if not np.isfinite(like):
                return -np.inf, prior
            return prior + like, prior

        # initial values
        x0 = []
        brute_force = True if not mcmc else False  # only vary r
        if r_prior is not None:
            x0 += [0.01]
        if alpha_prior is not None:
            alphas = [self.template_alpha[tag] for tag in alpha_tags]
            x0 += [0.01 if a == 0 else a for a in alphas]
            brute_force = False
        if res_prior is not None:
            x0 += list(pt.dict_to_arr(qb_res, flatten=True))
            brute_force = False
        if beam_prior is not None:
            # add a beam term for each frequency
            x0 += [0.01] * len(beam_tags)
            brute_force = False
        if betad_prior is not None:
            x0 += [0.01]
            brute_force = False
        if dust_amp_prior is not None:
            x0 += [1, 1]
            brute_force = False
        if dust_ellind_prior is not None:
            x0 += [0.01]
            brute_force = False

        ndim = len(x0)
        if ndim * 2 > num_walkers:
            num_walkers = int(np.round(ndim / float(num_walkers)) * num_walkers * 2)
            self.warn(
                "Found {} parameters, increasing number of MCMC walkers to {}".format(
                    ndim, num_walkers
                )
            )
        x0 = np.array(x0)[None, :] * (1 + 1e-4 * np.random.randn(num_walkers, len(x0)))

        if brute_force or (r_prior is not None and ndim == 1):
            self.log("Computing brute-force r profile likelihood", "info")
            likefile = self.get_filename(
                save_name, ext=".txt", map_tag=map_tag, extra_tag=file_tag, bp_opts=True
            )
            rs = np.linspace(0, 3, 500)
            likes = np.zeros_like(rs)
            for idx, r in enumerate(rs):
                like = log_like(r=r)
                if idx % 20 == 0:
                    self.log("r = {:.3f}, loglike = {:.2f}".format(r, like), "debug")
                likes[idx] = like
            header = "{} r likelihood\nColumns: r, loglike".format(
                "Multi-map" if map_tag is None else "Map {}".format(map_tag)
            )
            np.savetxt(likefile, np.column_stack((rs, likes)), header=header)

        if not mcmc:
            return [rs, likes]

        # run chains!
        import emcee

        # setup sampler output file
        filename = self.get_filename(
            save_name, ext=".h5", map_tag=map_tag, extra_tag=file_tag, bp_opts=True
        )
        backend_exists = os.path.exists(filename)
        backend = emcee.backends.HDFBackend(filename)
        if backend_exists and backend.shape != (num_walkers, ndim):
            self.warn(
                "Expected backend of shape ({}, {}), found {}. Resetting".format(
                    num_walkers, ndim, backend.shape
                )
            )
            reset_backend = True
        if reset_backend:
            backend.reset(num_walkers, ndim)

        # initialize sampler
        self.log("Initializing sampler", "info")
        sampler = emcee.EnsembleSampler(num_walkers, ndim, log_prob, backend=backend)
        if not reset_backend and backend_exists:
            # grab the last sample if appending to an existing run
            x0 = sampler.run_mcmc(None, 1)

        # track autocorrelation time
        old_tau = np.inf
        converged = False

        self.log(
            "Starting {} iterations with {} parameters".format(num_steps, ndim), "info"
        )
        for sample in sampler.sample(x0, iterations=num_steps):
            if not sampler.iteration % 10:
                self.log("MCMC iteration {}".format(sampler.iteration), "debug")
            # check convergence every 100 steps
            if sampler.iteration % 100:
                continue

            # compute autocorrelation time
            tau = sampler.get_autocorr_time(tol=0)

            # check convergence
            converged = np.all(tau / converge_criteria < sampler.iteration)
            converged &= np.all(np.abs(old_tau - tau) / tau < converge_criteria)
            self.log(
                "MCMC iteration {} autocorr time: mean {:.1f} min {:.1f} max {:.1f}".format(
                    sampler.iteration, np.mean(tau), np.min(tau), np.max(tau)
                ),
                "info",
            )
            if converged:
                break
            old_tau = tau

        out.update(converged=converged, num_steps=sampler.iteration)

        # converged posterior distribution
        if converged:
            self.log(
                "MCMC converged in {} iterations".format(sampler.iteration), "info"
            )
            tau = sampler.get_autocorr_time()
            burnin = int(2 * np.max(tau))
            thin = int(0.5 * np.min(tau))
            samples = sampler.get_chain(discard=burnin, thin=thin, flat=True)
            out.update(tau=tau, burnin=burnin, thin=thin, samples=samples)
        else:
            self.warn("MCMC not converged in {} iterations".format(num_steps))

        if res_prior is not None:
            self.bin_def = bin_def_orig
            self.nbins_res = nbins_res_orig

        # save and return
        return self.save_data(
            save_name, map_tag=map_tag, extra_tag=file_tag, bp_opts=True, **out
        )
